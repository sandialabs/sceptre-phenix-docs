{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome \u00b6 This is the documentation for the minimega phenix orchestration tool. phenix development happens in the sandia-minimega/phenix GitHub repository. Getting Started with phenix \u00b6 The first step in using phenix is to get it installed. phenix needs access to the minimega unix socket, so the best place to deploy it is on a minimega cluster's head node. The minimega unix socket will be located at /tmp/minimega/minimega on default cluster deployments and will be owned by root, so unless the socket's group ownership and group write permissions have been updated, phenix will need to be run as root in order to access the socket. Installing and Running via Docker \u00b6 A Docker image can be built by cloning the git repo and running the docker-build.sh script in the root of the repo. To run phenix as a Docker container, it will need to run in privileged mode and have access to the host network and some host directories. An example is below. docker run -d --name phenix \\ --hostname=$(hostname) \\ --network=host \\ --privileged \\ --volume=/dev:/dev \\ --volume=/proc:/proc \\ --volume=/phenix:/phenix \\ --volume=/etc/phenix:/etc/phenix \\ --volume=/tmp:/tmp \\ --volume=/var/log/phenix:/var/log/phenix \\ --volume=/etc/localtime:/etc/localtime:ro \\ phenix phenix ui --hostname=$(hostname) : set the container's hostname to be the same as the host. This is mainly beneficial when gathering cluster node details. --network=host : use the host's network stack in the container. Using --publish=3000:3000 is also a valid option. --privileged , --volume=/dev:/dev , and --volume=/proc:/proc : needed for building images with phenix . These options can be omitted if phenix won't be used to build images. --volume=/phenix:/phenix : /phenix is used as the base directory for phenix by default (see --base-dir.phenix global option), so we share this directory with the host to persist changes across container restarts. --volume=/etc/phenix:/etc/phenix : the phenix config store is written to /etc/phenix/store.bdb by default when phenix is run as root, so we share this directory with the host so config changes persist across container restarts. --volume=/tmp:/tmp : minimega creates its Unix socket in /tmp/minimega by default, so we share this directory with the host so phenix can have access to the minimega socket. phenix also writes some files to /tmp that minimega needs access to (e.g. injecting the miniccc agent into images), which also makes this volume mount necessary. --volume=/var/log/phenix:/var/log/phenix : phenix writes its logs to /var/log/phenix by default when run as root. Sharing this directory with the host makes it easier to debug issues if the container fails. --volume=/etc/localtime:/etc/localtime:ro : set the container's timezone to be the same as the host. Note If you build the Docker image manually, be sure to replace the last line in the command above with the tag used to build the image. With phenix running in a container, it's useful to setup a bash alias for phenix : alias phenix=\"docker exec -it phenix phenix\" Note The Docker image will also include the phenix user apps available in the sandia-minimega/phenix-apps repo. Installing and Running via Apt \u00b6 A minimega Debian package is hosted at https://apt.sceptre.dev that includes all the minimega executables, as well as the phenix executable. The minimega executables in this package will be more up-to-date than the versioned Debian package released by the official minimega development team. When installed via the Debian package, systemd units get installed for minimega , miniweb , and phenix , and a minimega system group is created. Any user part of the minimega group can access minimega without having to run as root. Contrary to the phenix Docker image, the phenix-apps must be installed separately, but there's a Debian package for them too. See https://apt.sceptre.dev for instructions on adding the Apt repo and installing phenix (via the minimega package) and phenix-apps . Building from Source \u00b6 To build locally, you will need Golang v1.14 and Node v14.2 installed. Once those are installed (if not already), simply run make bin/phenix . If you do not want to install Golang and/or Node locally, you can also use Docker to build phenix (assuming you have Docker installed). Simply run ./docker-build.sh from the phenix directory and once built, the phenix binary will be available at bin/phenix . See ./docker-build.sh -h for usage details. Using \u00b6 The following output results from bin/phenix help : A cli application for phenix Usage: phenix [flags] phenix [command] Available Commands: config Configuration file management experiment Experiment management help Help about any command image Virtual disk image management ui Run the phenix UI util Utility commands version print version information vlan Used to manage VLANs vm Virtual machine management Flags: --base-dir.minimega string base minimega directory (default \"/tmp/minimega\") --base-dir.phenix string base phenix directory (default \"/phenix\") -h, --help help for phenix --hostname-suffixes string hostname suffixes to strip --log.error-file string log fatal errors to file (default \"/var/log/phenix/error.log\") --log.error-stderr log fatal errors to STDERR (default true) --store.endpoint string endpoint for storage service (default \"bolt:///etc/phenix/store.bdb\") Use \"phenix [command] --help\" for more information about a command. Thanks to viper , it is possible to specify values for all of the global and ui command flags listed above using a configuration file. Global flags set at the command line will override settings in the configuration file. phenix looks for a configuration in the following locations. When run as root (not including sudo): /etc/phenix/config.[yaml|json|toml] When run as regular user (including sudo): $HOME/.config/phenix/config.[yaml|json|toml] /etc/phenix/config.[yaml|json|toml] An example configuration file might look like the following: base-dir: minimega: /tmp/minimega phenix: /phenix log: error-file: /var/log/phenix/error.log error-stderr: true store: endpoint: bolt:///etc/phenix/store.bdb ui: listen-endpoint: 0.0.0.0:3000 jwt-signing-key: abcde12345 log-level: info log-verbose: true logs: phenix-path: /var/log/phenix/phenix.log minimega-path: /var/log/minimega/minimega.log Environment variables can also be used to set global and ui command flags. The environment variables must be prefixed with PHENIX_ , with the rest of the variable matching the flag name with - and . replaced with _ . For example, --store.endpoint becomes PHENIX_STORE_ENDPOINT . Further documentation on the available commands can be found at: config experiment vm image Store \u00b6 The phenix tool uses a key-value data store as the storage service for all of data needed throughout the various capabilities (as opposed to a database). By default it uses bbolt but also supports etcd . bbolt is used by default because it has no external dependencies, but has a limitation of only being accessible on a single machine. Using etcd , on the other hand, allows for users to run phenix on multiple machines and access the same data, but requires etcd be deployed as a separate service. To use etcd , the --store.endpoint global flag should be configured with the URL of the deployed etcd server. For example, --store.endpoint etcd://localhost:2379 .","title":"Welcome"},{"location":"#welcome","text":"This is the documentation for the minimega phenix orchestration tool. phenix development happens in the sandia-minimega/phenix GitHub repository.","title":"Welcome"},{"location":"#getting-started-with-phenix","text":"The first step in using phenix is to get it installed. phenix needs access to the minimega unix socket, so the best place to deploy it is on a minimega cluster's head node. The minimega unix socket will be located at /tmp/minimega/minimega on default cluster deployments and will be owned by root, so unless the socket's group ownership and group write permissions have been updated, phenix will need to be run as root in order to access the socket.","title":"Getting Started with phenix"},{"location":"#installing-and-running-via-docker","text":"A Docker image can be built by cloning the git repo and running the docker-build.sh script in the root of the repo. To run phenix as a Docker container, it will need to run in privileged mode and have access to the host network and some host directories. An example is below. docker run -d --name phenix \\ --hostname=$(hostname) \\ --network=host \\ --privileged \\ --volume=/dev:/dev \\ --volume=/proc:/proc \\ --volume=/phenix:/phenix \\ --volume=/etc/phenix:/etc/phenix \\ --volume=/tmp:/tmp \\ --volume=/var/log/phenix:/var/log/phenix \\ --volume=/etc/localtime:/etc/localtime:ro \\ phenix phenix ui --hostname=$(hostname) : set the container's hostname to be the same as the host. This is mainly beneficial when gathering cluster node details. --network=host : use the host's network stack in the container. Using --publish=3000:3000 is also a valid option. --privileged , --volume=/dev:/dev , and --volume=/proc:/proc : needed for building images with phenix . These options can be omitted if phenix won't be used to build images. --volume=/phenix:/phenix : /phenix is used as the base directory for phenix by default (see --base-dir.phenix global option), so we share this directory with the host to persist changes across container restarts. --volume=/etc/phenix:/etc/phenix : the phenix config store is written to /etc/phenix/store.bdb by default when phenix is run as root, so we share this directory with the host so config changes persist across container restarts. --volume=/tmp:/tmp : minimega creates its Unix socket in /tmp/minimega by default, so we share this directory with the host so phenix can have access to the minimega socket. phenix also writes some files to /tmp that minimega needs access to (e.g. injecting the miniccc agent into images), which also makes this volume mount necessary. --volume=/var/log/phenix:/var/log/phenix : phenix writes its logs to /var/log/phenix by default when run as root. Sharing this directory with the host makes it easier to debug issues if the container fails. --volume=/etc/localtime:/etc/localtime:ro : set the container's timezone to be the same as the host. Note If you build the Docker image manually, be sure to replace the last line in the command above with the tag used to build the image. With phenix running in a container, it's useful to setup a bash alias for phenix : alias phenix=\"docker exec -it phenix phenix\" Note The Docker image will also include the phenix user apps available in the sandia-minimega/phenix-apps repo.","title":"Installing and Running via Docker"},{"location":"#installing-and-running-via-apt","text":"A minimega Debian package is hosted at https://apt.sceptre.dev that includes all the minimega executables, as well as the phenix executable. The minimega executables in this package will be more up-to-date than the versioned Debian package released by the official minimega development team. When installed via the Debian package, systemd units get installed for minimega , miniweb , and phenix , and a minimega system group is created. Any user part of the minimega group can access minimega without having to run as root. Contrary to the phenix Docker image, the phenix-apps must be installed separately, but there's a Debian package for them too. See https://apt.sceptre.dev for instructions on adding the Apt repo and installing phenix (via the minimega package) and phenix-apps .","title":"Installing and Running via Apt"},{"location":"#building-from-source","text":"To build locally, you will need Golang v1.14 and Node v14.2 installed. Once those are installed (if not already), simply run make bin/phenix . If you do not want to install Golang and/or Node locally, you can also use Docker to build phenix (assuming you have Docker installed). Simply run ./docker-build.sh from the phenix directory and once built, the phenix binary will be available at bin/phenix . See ./docker-build.sh -h for usage details.","title":"Building from Source"},{"location":"#using","text":"The following output results from bin/phenix help : A cli application for phenix Usage: phenix [flags] phenix [command] Available Commands: config Configuration file management experiment Experiment management help Help about any command image Virtual disk image management ui Run the phenix UI util Utility commands version print version information vlan Used to manage VLANs vm Virtual machine management Flags: --base-dir.minimega string base minimega directory (default \"/tmp/minimega\") --base-dir.phenix string base phenix directory (default \"/phenix\") -h, --help help for phenix --hostname-suffixes string hostname suffixes to strip --log.error-file string log fatal errors to file (default \"/var/log/phenix/error.log\") --log.error-stderr log fatal errors to STDERR (default true) --store.endpoint string endpoint for storage service (default \"bolt:///etc/phenix/store.bdb\") Use \"phenix [command] --help\" for more information about a command. Thanks to viper , it is possible to specify values for all of the global and ui command flags listed above using a configuration file. Global flags set at the command line will override settings in the configuration file. phenix looks for a configuration in the following locations. When run as root (not including sudo): /etc/phenix/config.[yaml|json|toml] When run as regular user (including sudo): $HOME/.config/phenix/config.[yaml|json|toml] /etc/phenix/config.[yaml|json|toml] An example configuration file might look like the following: base-dir: minimega: /tmp/minimega phenix: /phenix log: error-file: /var/log/phenix/error.log error-stderr: true store: endpoint: bolt:///etc/phenix/store.bdb ui: listen-endpoint: 0.0.0.0:3000 jwt-signing-key: abcde12345 log-level: info log-verbose: true logs: phenix-path: /var/log/phenix/phenix.log minimega-path: /var/log/minimega/minimega.log Environment variables can also be used to set global and ui command flags. The environment variables must be prefixed with PHENIX_ , with the rest of the variable matching the flag name with - and . replaced with _ . For example, --store.endpoint becomes PHENIX_STORE_ENDPOINT . Further documentation on the available commands can be found at: config experiment vm image","title":"Using"},{"location":"#store","text":"The phenix tool uses a key-value data store as the storage service for all of data needed throughout the various capabilities (as opposed to a database). By default it uses bbolt but also supports etcd . bbolt is used by default because it has no external dependencies, but has a limitation of only being accessible on a single machine. Using etcd , on the other hand, allows for users to run phenix on multiple machines and access the same data, but requires etcd be deployed as a separate service. To use etcd , the --store.endpoint global flag should be configured with the URL of the deployed etcd server. For example, --store.endpoint etcd://localhost:2379 .","title":"Store"},{"location":"apps/","text":"Apps \u00b6 phenix apps provide a means of modifying an experiment topology, cluster networking, hardware-in-the-loop devices, etc. in a layered, scripted, and codified manner. phenix itself includes four (4) default apps that get applied to every experiment by default. In addition to the default apps, it is possible to apply user apps to an experiment using a scenario configuration. Default Apps \u00b6 ntp provides/configures NTP service for experiment serial configures serial interfaces in VM images startup configures minimega startup injections based on OS type vrouter customizes Vyatta/VyOS and minirouter routers, including setting interfaces, ACL rules, etc. vrouter App \u00b6 As of commit e276a5b , the vrouter app also supports the use of minimega's minirouter to include interface configuration, DHCP and DNS configuration, firewall rules, etc. The following is an example of how the vrouter app can be configured via a Scenario configuration, showing all the possible options. spec: apps: - name: vrouter hosts: - hostname: rtr metadata: ipsec: - local: 10.0.10.2 peer: 10.0.40.2 tunnels: - local: 192.168.10.0/24 remote: 192.168.100.0/24 acl: ingress: IF0: in-rules rulesets: - name: in-rules default: drop rules: - action: accept description: Allow Incoming HTTP source: address: 192.168.0.0/24 destination: address: 10.0.0.0/24 port: 80 protocol: tcp dhcp: - listenAddress: 10.0.0.254 ranges: - lowAddress: 10.0.0.10 highAddress: 10.0.0.20 defaultRoute: 10.0.0.254 dnsServers: - 10.0.0.254 staticAssignments: 00:00:00:00:00:AA: 10.0.0.50 - listenAddress: 192.168.0.254 ranges: - lowAddress: 192.168.0.10 highAddress: 192.168.0.20 defaultRoute: 192.168.0.254 dnsServers: - 192.168.0.254 staticAssignments: 00:00:00:00:00:BB: 192.168.0.50 dns: 1.2.3.4: foo.com snat: - interface: IF0 srcAddr: 192.168.0.0/24 translation: masquerade dnat: - interface: IF1 dstAddr: 10.0.0.1 dstPort: 80 protocol: tcp translation: 192.168.0.250:8080 emulators: - ingress: - IF0 egress: - IF1 name: comcast bandwidth: 400kbit burst: 15kb delay: 500ms corruption: 5% loss: 10% reordering: 5% ipsec : if present, point-to-point IPSec tunnels are nailed up between the list of given IP addresses and traffic between the given networks is tunneled. local : IP address on a local interface (e.g. this router) to bind the IPSec tunnel to. It must have a route to the peer IP address. peer : remote IP address to create the IPSec tunnel with. tunnels : list of local and remote networks to tunnel through this point-to-point connection. local : local network to tunnel to the given remote network. remote : remote network to tunnel to the given local network. acl : if present, access control lists (ACLs / firewall rules) are created on the router per the defined rulesets. ingress : for each interface-to-ruleset mapping, apply the given ruleset to the given interface for inbound traffic. Note that the interface name used (in this example, IF0 ) refers to the name given to a network interface in the router's topology configuration. egress : for each interface-to-ruleset mapping, apply the given ruleset to the given interface for outbound traffic. rulesets : list of rulesets to create on the router. name : name of the ruleset; used in the interface-to-ruleset mapping in the ingress/egress sections. default : default action to apply to traffic that doesn't match any rules. rules : list of rules to apply to traffic. action : action to apply to traffic matching rule. source : map describing what source to limit matching traffic to. If not provided, all sources are matched. address : source address to limit matching traffic to. If not provided, all source addresses are matched. port : source port to limit matching traffic to. If not provided, all source ports are matched. destination : map describing what destination to limit matching traffic to. If not provided, all sources are matched. address : destination address to limit matching traffic to. If not provided, all destination addresses are matched. port : destination port to limit matching traffic to. If not provided, all destination ports are matched. protocol : IP protocol to limit matching traffic to. If not provided, all protocols are matched. stateful : if true, enable established and related traffic for this ruleset. dhcp : if present, DHCP is configured on the router per the provided list. listenAddress : IP address on a local interface (e.g. this router) to bind this DHCP configuration to. ranges : list of IP address low/high ranges to use for DHCP assignments. The IP addresses must be within the IP network of the listenAddress . defaultRoute : default gateway to be included in DHCP leases. dnsServers : list of DNS servers to be included in DHCP leases. staticAssignments : map of MAC-to-IP assignments to use for static DHCP addresses. dns : if present, map of IP-to-domain DNS entries to create on the router. Note Currently, the ipsec , emulators , and snat/dnat metadata sections only apply to Vyatta/VyOS routers. Note Currently, the stateful setting for ACL rules only applies to Vyatta/VyOS routers. Note Currently, the dhcp and dns metadata sections only apply to minirouter routers. Additional Core Apps \u00b6 The apps listed below are provided by the core phenix application, but are not considered default apps since they do not get applied to every experiment by default. They're more like a user app , but implemented in the core application instead of as a stand-alone executable. scorch Scorch \u2014 SC enario ORCH estration \u2014 is an automated scenario orchestration framework within phenix soh provide state of health monitoring for an experiment tap manage host taps (typically used for external network access) for an experiment tap App \u00b6 The tap app manages the creation and removal of host taps needed by experiments to access external network resources. This includes creating the tap in a network namespace ( netns ) to avoid interface address collisions with other experiments, connecting the netns with the system network to enable external network access, and modifying iptables to allow external network access from the tapped experiment VLAN. Note Host taps can also be used to access VM network resources directly from the host the VM is running on. This is an advanced topic that will be documented soon. In order for a tap to have access to experiment VMs in the tapped VLAN, it must have an IP address on the same subnet as the rest of the VMs in the VLAN. Attempting to tap multiple experiments could fail if the VLANs being tapped are using the same subnet, so the tap is put into a netns to provide isolation and avoid address collisions. With the tap in a netns, however, it no longer has a path to external networks via the system's default netns. To remedy this, a veth pair is used to connect the tap's netns with the default netns. A very small ( /30 ) IP subnet is used for the veth pair, and phenix manages the selection and tracking of the subnets used for each pair to avoid collisions. With the veth pair in place, packets from the experiment VLAN can now be routed externally with the help of IP masquerading in both the tap's netns and the default netns. Warning This has not been fully tested against all the possible iptables firewall configurations. If you experience problems with external access, it may be due to a more restrictive iptables configuration than we've tested with. The following is an example of how the tap app can be configured via a Scenario configuration, showing all the possible options. Note The externalAccess.firewall portion of the tap configuration has not been implemented yet. spec: apps: - name: tap metadata: # the bridge to add the tap to (will default to 'phenix' if not provided) - bridge: phenix # the experiment VLAN to tap vlan: MGMT # IP address to use for host tap -- VMs on the tapped VLAN would use this # address as their gateway if they need external access (and it's enabled # below) ip: 172.20.5.254/24 externalAccess: # defaults to false enabled: true # this section is planned, but not implemented yet firewall: # default action to take if none of the rules below match a packet default: drop rules: # action to take if a packet matches this rule - action: accept description: Only allow web access source: # can also use `addresses` to specify a list of addresses address: 172.20.5.0/29 destination: address: 10.0.0.0/24 # can also use `port` to specify a single port ports: [80, 443] # can also use `protocols` to specify a list of protocols protocol: tcp User Apps \u00b6 phenix user apps are stand-alone executables that phenix shells out to at different stages of the experiment lifecycle ( configure, pre-start, post-start, running, and cleanup ). When phenix encounters an app in an experiment scenario that isn't a default app, it checks to see if an executable exists in its current PATH in the form of phenix-app-<app name> . If the executable exists, phenix shells out to it, providing the current lifecycle stage as an argument and providing the experiment metadata, spec, and status as a JSON string over STDIN . Note There will be three (3) top-level keys available in the JSON passed to a user app over STDIN : metadata, spec, and status . For the configure and pre-start stages, the status value will be null or otherwise ignored. The spec value will be experiment schema . Tip You can run phenix util app-json <exp name> to see an example of what the JSON that's passed to a user app looks like. The user app can modify the experiment at will, then return the updated JSON over STDOUT and exit with a 0 status. If the user app encounters an error, it can print any error messages to STDERR and exit with a non-zero status to signal to phenix that an error occurred. Note phenix will only process updates to the spec value for the configure and pre-start stages, and will only process updates to the status value for the post-start, running, and cleanup stages. More specifically, it will only process updates to status.apps.<app name> , which can be anything the app wants it to be (e.g. a simple string, an array, or a map/dictionary). Note It is possible for the execution of app stages to be canceled by the caller. In the case of user apps, phenix will send a SIGTERM to the user app process and wait a maximum of 10 seconds for the process to exit gracefully before killin the process with a SIGKILL . Available User Apps \u00b6 The sandia-minimega/phenix-apps repo is home to some user apps that have already been created by the community, including the following: protonuke wireguard mirror In addition, this repo also contains some generic library/utility code for making custom user app development easier. See the README for additional details. Example \u00b6 Below is a very contrived example of a simple user app that changes the disk image used for every node in the experiment topology. Assuming the name of the executable for this app as phenix-app-image-changer , it could be applied to a topology by including a scenario in an experiment that includes an experiment app named image-changer . import json, sys def eprint(*args): print(*args, file=sys.stderr) def main() : if len(sys.argv) != 2: eprint(\"must pass exactly one argument on the command line\") sys.exit(1) raw = sys.stdin.read() if sys.argv[1] != 'pre-start': print(raw) sys.exit(0) exp = json.loads(raw) spec = exp['spec'] for n in spec['topology']['nodes']: for d in n['hardware']['drives']: d['image'] = 'm$.qc2' print(json.dumps(exp))","title":"Apps"},{"location":"apps/#apps","text":"phenix apps provide a means of modifying an experiment topology, cluster networking, hardware-in-the-loop devices, etc. in a layered, scripted, and codified manner. phenix itself includes four (4) default apps that get applied to every experiment by default. In addition to the default apps, it is possible to apply user apps to an experiment using a scenario configuration.","title":"Apps"},{"location":"apps/#default-apps","text":"ntp provides/configures NTP service for experiment serial configures serial interfaces in VM images startup configures minimega startup injections based on OS type vrouter customizes Vyatta/VyOS and minirouter routers, including setting interfaces, ACL rules, etc.","title":"Default Apps"},{"location":"apps/#vrouter-app","text":"As of commit e276a5b , the vrouter app also supports the use of minimega's minirouter to include interface configuration, DHCP and DNS configuration, firewall rules, etc. The following is an example of how the vrouter app can be configured via a Scenario configuration, showing all the possible options. spec: apps: - name: vrouter hosts: - hostname: rtr metadata: ipsec: - local: 10.0.10.2 peer: 10.0.40.2 tunnels: - local: 192.168.10.0/24 remote: 192.168.100.0/24 acl: ingress: IF0: in-rules rulesets: - name: in-rules default: drop rules: - action: accept description: Allow Incoming HTTP source: address: 192.168.0.0/24 destination: address: 10.0.0.0/24 port: 80 protocol: tcp dhcp: - listenAddress: 10.0.0.254 ranges: - lowAddress: 10.0.0.10 highAddress: 10.0.0.20 defaultRoute: 10.0.0.254 dnsServers: - 10.0.0.254 staticAssignments: 00:00:00:00:00:AA: 10.0.0.50 - listenAddress: 192.168.0.254 ranges: - lowAddress: 192.168.0.10 highAddress: 192.168.0.20 defaultRoute: 192.168.0.254 dnsServers: - 192.168.0.254 staticAssignments: 00:00:00:00:00:BB: 192.168.0.50 dns: 1.2.3.4: foo.com snat: - interface: IF0 srcAddr: 192.168.0.0/24 translation: masquerade dnat: - interface: IF1 dstAddr: 10.0.0.1 dstPort: 80 protocol: tcp translation: 192.168.0.250:8080 emulators: - ingress: - IF0 egress: - IF1 name: comcast bandwidth: 400kbit burst: 15kb delay: 500ms corruption: 5% loss: 10% reordering: 5% ipsec : if present, point-to-point IPSec tunnels are nailed up between the list of given IP addresses and traffic between the given networks is tunneled. local : IP address on a local interface (e.g. this router) to bind the IPSec tunnel to. It must have a route to the peer IP address. peer : remote IP address to create the IPSec tunnel with. tunnels : list of local and remote networks to tunnel through this point-to-point connection. local : local network to tunnel to the given remote network. remote : remote network to tunnel to the given local network. acl : if present, access control lists (ACLs / firewall rules) are created on the router per the defined rulesets. ingress : for each interface-to-ruleset mapping, apply the given ruleset to the given interface for inbound traffic. Note that the interface name used (in this example, IF0 ) refers to the name given to a network interface in the router's topology configuration. egress : for each interface-to-ruleset mapping, apply the given ruleset to the given interface for outbound traffic. rulesets : list of rulesets to create on the router. name : name of the ruleset; used in the interface-to-ruleset mapping in the ingress/egress sections. default : default action to apply to traffic that doesn't match any rules. rules : list of rules to apply to traffic. action : action to apply to traffic matching rule. source : map describing what source to limit matching traffic to. If not provided, all sources are matched. address : source address to limit matching traffic to. If not provided, all source addresses are matched. port : source port to limit matching traffic to. If not provided, all source ports are matched. destination : map describing what destination to limit matching traffic to. If not provided, all sources are matched. address : destination address to limit matching traffic to. If not provided, all destination addresses are matched. port : destination port to limit matching traffic to. If not provided, all destination ports are matched. protocol : IP protocol to limit matching traffic to. If not provided, all protocols are matched. stateful : if true, enable established and related traffic for this ruleset. dhcp : if present, DHCP is configured on the router per the provided list. listenAddress : IP address on a local interface (e.g. this router) to bind this DHCP configuration to. ranges : list of IP address low/high ranges to use for DHCP assignments. The IP addresses must be within the IP network of the listenAddress . defaultRoute : default gateway to be included in DHCP leases. dnsServers : list of DNS servers to be included in DHCP leases. staticAssignments : map of MAC-to-IP assignments to use for static DHCP addresses. dns : if present, map of IP-to-domain DNS entries to create on the router. Note Currently, the ipsec , emulators , and snat/dnat metadata sections only apply to Vyatta/VyOS routers. Note Currently, the stateful setting for ACL rules only applies to Vyatta/VyOS routers. Note Currently, the dhcp and dns metadata sections only apply to minirouter routers.","title":"vrouter App"},{"location":"apps/#additional-core-apps","text":"The apps listed below are provided by the core phenix application, but are not considered default apps since they do not get applied to every experiment by default. They're more like a user app , but implemented in the core application instead of as a stand-alone executable. scorch Scorch \u2014 SC enario ORCH estration \u2014 is an automated scenario orchestration framework within phenix soh provide state of health monitoring for an experiment tap manage host taps (typically used for external network access) for an experiment","title":"Additional Core Apps"},{"location":"apps/#tap-app","text":"The tap app manages the creation and removal of host taps needed by experiments to access external network resources. This includes creating the tap in a network namespace ( netns ) to avoid interface address collisions with other experiments, connecting the netns with the system network to enable external network access, and modifying iptables to allow external network access from the tapped experiment VLAN. Note Host taps can also be used to access VM network resources directly from the host the VM is running on. This is an advanced topic that will be documented soon. In order for a tap to have access to experiment VMs in the tapped VLAN, it must have an IP address on the same subnet as the rest of the VMs in the VLAN. Attempting to tap multiple experiments could fail if the VLANs being tapped are using the same subnet, so the tap is put into a netns to provide isolation and avoid address collisions. With the tap in a netns, however, it no longer has a path to external networks via the system's default netns. To remedy this, a veth pair is used to connect the tap's netns with the default netns. A very small ( /30 ) IP subnet is used for the veth pair, and phenix manages the selection and tracking of the subnets used for each pair to avoid collisions. With the veth pair in place, packets from the experiment VLAN can now be routed externally with the help of IP masquerading in both the tap's netns and the default netns. Warning This has not been fully tested against all the possible iptables firewall configurations. If you experience problems with external access, it may be due to a more restrictive iptables configuration than we've tested with. The following is an example of how the tap app can be configured via a Scenario configuration, showing all the possible options. Note The externalAccess.firewall portion of the tap configuration has not been implemented yet. spec: apps: - name: tap metadata: # the bridge to add the tap to (will default to 'phenix' if not provided) - bridge: phenix # the experiment VLAN to tap vlan: MGMT # IP address to use for host tap -- VMs on the tapped VLAN would use this # address as their gateway if they need external access (and it's enabled # below) ip: 172.20.5.254/24 externalAccess: # defaults to false enabled: true # this section is planned, but not implemented yet firewall: # default action to take if none of the rules below match a packet default: drop rules: # action to take if a packet matches this rule - action: accept description: Only allow web access source: # can also use `addresses` to specify a list of addresses address: 172.20.5.0/29 destination: address: 10.0.0.0/24 # can also use `port` to specify a single port ports: [80, 443] # can also use `protocols` to specify a list of protocols protocol: tcp","title":"tap App"},{"location":"apps/#user-apps","text":"phenix user apps are stand-alone executables that phenix shells out to at different stages of the experiment lifecycle ( configure, pre-start, post-start, running, and cleanup ). When phenix encounters an app in an experiment scenario that isn't a default app, it checks to see if an executable exists in its current PATH in the form of phenix-app-<app name> . If the executable exists, phenix shells out to it, providing the current lifecycle stage as an argument and providing the experiment metadata, spec, and status as a JSON string over STDIN . Note There will be three (3) top-level keys available in the JSON passed to a user app over STDIN : metadata, spec, and status . For the configure and pre-start stages, the status value will be null or otherwise ignored. The spec value will be experiment schema . Tip You can run phenix util app-json <exp name> to see an example of what the JSON that's passed to a user app looks like. The user app can modify the experiment at will, then return the updated JSON over STDOUT and exit with a 0 status. If the user app encounters an error, it can print any error messages to STDERR and exit with a non-zero status to signal to phenix that an error occurred. Note phenix will only process updates to the spec value for the configure and pre-start stages, and will only process updates to the status value for the post-start, running, and cleanup stages. More specifically, it will only process updates to status.apps.<app name> , which can be anything the app wants it to be (e.g. a simple string, an array, or a map/dictionary). Note It is possible for the execution of app stages to be canceled by the caller. In the case of user apps, phenix will send a SIGTERM to the user app process and wait a maximum of 10 seconds for the process to exit gracefully before killin the process with a SIGKILL .","title":"User Apps"},{"location":"apps/#available-user-apps","text":"The sandia-minimega/phenix-apps repo is home to some user apps that have already been created by the community, including the following: protonuke wireguard mirror In addition, this repo also contains some generic library/utility code for making custom user app development easier. See the README for additional details.","title":"Available User Apps"},{"location":"apps/#example","text":"Below is a very contrived example of a simple user app that changes the disk image used for every node in the experiment topology. Assuming the name of the executable for this app as phenix-app-image-changer , it could be applied to a topology by including a scenario in an experiment that includes an experiment app named image-changer . import json, sys def eprint(*args): print(*args, file=sys.stderr) def main() : if len(sys.argv) != 2: eprint(\"must pass exactly one argument on the command line\") sys.exit(1) raw = sys.stdin.read() if sys.argv[1] != 'pre-start': print(raw) sys.exit(0) exp = json.loads(raw) spec = exp['spec'] for n in spec['topology']['nodes']: for d in n['hardware']['drives']: d['image'] = 'm$.qc2' print(json.dumps(exp))","title":"Example"},{"location":"configuration/","text":"Configuration Files \u00b6 phenix currently supports six (6) different configuration file types: Topology Scenario Experiment Image User Role Typically, users will create Topology and Scenario configuration files by hand, while the rest will be generated by phenix using available commands. However, it is possible to create all configuration file types by hand if necessary. Note Configuration files can also be created, viewed, and deleted via the Web-UI (see Web-UI section below). Configuration files are versioned using a header section based heavily on what Kubernetes does. Each configuration file will have a header section that looks like the following: apiVersion: phenix.sandia.gov/v1 kind: Topology metadata: name: foobar spec: ... In the example above, the kind field represents the type of configuration file (e.g., Topology, Scenario, Image). The apiVersion field represents the version the spec section conforms to (currently there is only v1 for all configuration types except Scenario , which is v2 ), and the spec section will contain the actual details for the configuration type based on the configuration schema. Note that, at least in the header section, keys are camel-case and begin with a lowercase letter, while values are camel-cased but begin with a capital letter. Note Throughout the documentation, we mention creating configurations manually . When we say this, we mean passing a YAML or JSON configuration file of any type to the phenix config create command. Topology \u00b6 The Topology configuration is one of the core configuration types for phenix , as it describes a network topology to be deployed in minimega that can be used by one or more experiments to be executed. A topology is comprised of one or more nodes , which is a VM, each including system descriptions and configurations, as well as any networking settings required to connect all of the nodes in a topology together. This configuration becomes the basis for most of the minimega commands later created in the relevant minimega startup script. Default Settings \u00b6 If left unmodified or unset, the following are the default settings for each node: memory will be set to 512 MB vcpus will be set to 1 snapshot will be set to true no network settings will be included Required Values \u00b6 Each topology must have a unique name, which should be lowercase and not include spaces. In addition, each node in the topology must: have a specified type \u2014 the available types are defined in the schema have a unique hostname have an OS type of linux or windows have a disk image assigned Optional Values \u00b6 Optional values for a node in the topology configuration can include: static network configurations specific memory values (e.g., 1-16 GB) specific VCPUs values (e.g., 1-4 ) additional disk storage file injections labels, which are typically used by ph\u0113nix apps routing ruleset(s) delay triggered by user , time , or c2 (command and control) Delay Start \u00b6 It is possible to delay the start of a VM with the delay value. There are three options available to set, but on only one option can be set: user is a boolean value and when set to true the VM will require a manual start either through the ph\u0113nix UI or command line (the latter can be by ph\u0113nix or minimega commands). time is a string that is set as a delay in minutes; e.g., 5m . c2 requires minimega command and control. Note in the example below that the hostname AD1 will be delayed starting until the VM host-00 has started and checked in with minimega command and control. It is possible to have multiple hostnames included. useUUID is an additional boolean value setting, per hostname, that will watch for the UUID instead of the hostname to register with minimega command and control. Note If one or more VMs are set with a delayed start, the UI will display a blue tag in the Delay column next to the Screenshot column. The tag will indicate what type of dealy was set. Once all the delayed VMs have started, the Delay column will no longer be visible. Network Address Translation (NAT) \u00b6 For nodes of type Router , basic source NAT can be configured to masquerade packets from one or more networks out a specific interface. For example, assume a router has three interfaces \u2014 eth0, eth1, eth2 . eth0 is connected to the Internet, while eth1, eth2 are connected to networks that use private IP space. To masquerade packets from eth1, eth2 out eth0 the following configuration could be used. network: nat: - out: eth0 in: - eth1 - eth2 Example \u00b6 A contrived, four node example \u2014 three VMs and a router \u2014 is given below, and is driven by the topology schema described here . apiVersion: phenix.sandia.gov/v1 kind: Topology metadata: name: foobar spec: nodes: - type: VirtualMachine general: hostname: host-00 snapshot: true delay: user: true hardware: os_type: linux drives: - image: ubuntu.qc2 injections: - src: foo/bar/sucka.fish dst: /data/sucka.fish - src: /foo/bar/sucka/fish.sh dst: /data/fish.sh network: interfaces: - name: IF0 vlan: EXP-1 address: 192.168.10.1 mask: 24 gateway: 192.168.10.254 proto: static type: ethernet - name: IF1 vlan: MGMT address: 172.16.10.1 mask: 16 proto: static type: ethernet - type: VirtualMachine general: hostname: host-01 snapshot: true do_not_boot: false delay: time: 5m hardware: os_type: linux drives: - image: ubuntu.qc2 network: interfaces: - name: IF0 vlan: EXP-1 address: 192.168.10.2 mask: 24 gateway: 192.168.10.254 proto: static type: ethernet - name: IF1 vlan: MGMT address: 172.16.10.2 mask: 16 proto: static type: ethernet - name: S0 vlan: EXT address: 10.0.0.1 mask: 24 proto: static type: serial udp_port: 8989 baud_rate: 9600 device: /dev/ttyS0 - type: VirtualMachine general: hostname: AD1 snapshot: true delay: c2: - hostname: host-00 useUUID: false hardware: os_type: windows drives: - image: win-svr-2k8.qc2 network: interfaces: - name: IF0 vlan: EXP-1 address: 192.168.10.250 mask: 24 gateway: 192.168.10.254 proto: static type: ethernet - name: IF1 vlan: MGMT address: 172.16.10.3 mask: 16 proto: static type: ethernet - type: Router labels: ntp-server: \"true\" general: hostname: router-00 snapshot: true hardware: os_type: linux drives: - image: vyatta.qc2 network: interfaces: - name: IF0 vlan: EXP-1 address: 192.168.10.254 mask: 24 proto: static type: ethernet ruleset_in: test - name: IF1 vlan: MGMT address: 172.16.10.254 mask: 16 proto: static type: ethernet rulesets: - name: test default: drop rules: - id: 10 action: accept protocol: all source: address: 1.1.1.1 port: 53 Scenario \u00b6 The Scenario configuration is used to define and configure one or more phenix apps ( default or user ) for use with a topology. In this sense, a topology can have one or more scenarios associated with it, but a scenario can only be associated with a single topology. Apps \u00b6 A phenix app can be applied to an experiment topology using a single configuration, a per-host configuration, or both. Examples could include: a phenix app that adds a minimega tap to all hosts in the cluster, one that injects the same file into every node in the experiment topology, or one that configures a VPN between two nodes in an experiment using WireGuard. The first two examples would involve a single configuration, while the last example would use a per-host configuration because (1) only two nodes will be modified, and (2) each of the two nodes will require different configurations (e.g., one will be a WireGuard client and the other a WireGuard server). Each configured app can contain a list of topology nodes to apply the app to, along with custom metadata for the app specific to the topology node. App Configuration Options \u00b6 assetDir : used by apps to generate absolute path to asset files when relative paths are provided in app metadata. The default is an empty string. fromScenario : name of another scenario config to pull this app config from. This allows for defining complex app configurations in a single base scenario and referencing it from scenarios included in experiments. The default is an empty string. hosts : a list of per-host configurations to apply to the experiment topology. The default is nil . hostname : the name of the experiment VM to apply this per-host metadata to. metadata : app metadata to apply to this experiment VM. The default is nil . metadata : app metadata to apply to this experiment. The default is nil . name : the name of the app being configured. There is no default value; one must be provided. runPeriodically : a Golang duration string specifying how often to trigger the app's running stage. The default value is an empty string, which means the app's running stage will not be triggered periodically. Example \u00b6 The following is an example of a configuration for a scenario named foobar , which can only be applied to an accompanying topology named foobar (while these names are the same in this example, the topology and scenario names do not have to match). Included in this scenario are apps named miniccc-injector , startup , protonuke , and wireguard . Each entry in the list of app hosts includes custom app metadata and the hostname of the topology node to apply the metadata. apiVersion: phenix.sandia.gov/v1 kind: Scenario metadata: name: foobar annotations: topology: foobar spec: apps: - name: miniccc-injector metadata: # files to inject into each node in experiment, based on OS type linux: src: /phenix/injects/miniccc dst: /usr/local/bin/miniccc windows: src: /phenix/injects/miniccc.exe dst: phenix/miniccc.exe - name: startup hosts: - hostname: host-00 # hostname of topology node to apply it to metadata: domain_controller: domain: example.com ip: 10.0.0.1 username: admin password: SuperSecretPassword - name: protonuke hosts: - hostname: host-01 # hostname of topology node to apply it to metadata: # protonuke app metadata for this topology node args: -logfile /var/log/protonuke.log -level debug -http -https -smtp -ssh 192.168.100.100 - name: wireguard hosts: - hostname: AD1 # hostname of topology node to apply it to metadata: # wireguard app metadata for this topology node infrastructure: private_key: GLlxWJom8cQViGHojqOUShWIZG7IsSX8 address: 10.255.255.1/24 listen_port: 51820 peers: public_key: +joyya2F9g72qbKBtPDn00mIevG1j1OqeN76ylFLsiE= allowed_ips: 10.255.255.10/32 Note The above example includes an app named startup , which is a ph\u0113nix default app. Meaning, it is possible to configure default ph\u0113nix apps in a scenario configuration, not just user apps. Note See Scorch for additional information on Scenario Orchestration Experiment \u00b6 Experiment configurations represent the combination of topologies and scenarios to form an experiment (it should be note that an experiments do not require a scenario). Typically experiment configurations are created automatically , but it is possible to create them manually using a configuration file similar to the one shown below. In this case, an experiment named foobar would be created based on an existing topology named foobar and an existing scenario named foobar (note that none of the names are required to match). apiVersion: phenix.sandia.gov/v1 kind: Experiment metadata: name: foobar annotations: topology: foobar # this is required scenario: foobar # this is optional Once created, either manually or automatically, the experiment configuration will be expanded to have the topology and scenario configurations embedded in it, as well as additional details like cluster host schedules for VMs, VLAN ranges, Builder XML, etc. The advantage of embedding the topology and scenario into the experiment is that they can be modified in the experiment without modifying the originals. Image \u00b6 The Image configuration is used to generate VM disk images using a custom version of vmdb2 . Representing a disk image in a configuration like this allows for the same disk image to easily be built in different clusters without having to actually move large disk image files. Note The ph\u0113nix image capability will only generation Linux based images. It will not generate Windows disk images. Typically image configurations are created automatically by the phenix image create command, but users can also create them manually using a configuration file similar to the one shown below. This file can then be created in the ph\u0113nix store using the phenix cfg create <path-to-file> command. apiVersion: phenix.sandia.gov/v1 kind: Image metadata: name: foobar spec: format: qcow2 mirror: http://us.archive.ubuntu.com/ubuntu/ overlay: [] packages: - initramfs-tools - net-tools - isc-dhcp-client - openssh-server - init - iputils-ping - vim - less - netbase - curl - ifupdown - dbus - linux-image-generic - linux-headers-generic release: bionic size: 5G variant: minbase scripts: POSTBUILD_APT_CLEANUP: | apt clean || apt-get clean || echo \"unable to clean apt cache\" POSTBUILD_NO_ROOT_PASSWD: | sed -i 's/nullok_secure/nullok/' /etc/pam.d/common-auth sed -i 's/#PermitRootLogin prohibit-password/PermitRootLogin yes/' /etc/ssh/sshd_config sed -i 's/#PermitEmptyPasswords no/PermitEmptyPasswords yes/' /etc/ssh/sshd_config sed -i 's/PermitRootLogin prohibit-password/PermitRootLogin yes/' /etc/ssh/sshd_config sed -i 's/PermitEmptyPasswords no/PermitEmptyPasswords yes/' /etc/ssh/sshd_config passwd -d root Note If no overlay is used in the image configuration, an empty array must be provided User \u00b6 The User configuration tracks ph\u0113nix UI user settings (e.g., username, password, and RBAC permissions). Typically user configurations are created automatically when a UI admin creates a new user via the UI, but they can also be created manually using a configuration file similar to the one shown below. apiVersion: phenix.sandia.gov/v1 kind: User metadata: name: admin@foo.com spec: username: admin@foo.com first_name: Admin last_name: Istrator password: **************** rbac: roleName: Global Admin policies: - resourceNames: - '*' resources: - '*' - '*/*' verbs: - '*' - resourceNames: - admin@foo.com resources: - users verbs: - get Role \u00b6 The Role configuration is used to represent a named set of RBAC permissions that represent a user's role in the UI. When a new user is created, the role that user should have is specified, and using that role name the appropriate RBAC permissions are copied from the role configuration into the user configuration. There are six (6) default role configurations that get created automatically, and are described here . An example role configuration is shown below for completeness. apiVersion: phenix.sandia.gov/v1 kind: Role metadata: name: global-admin spec: roleName: Global Admin policies: - resourceNames: - '*' resources: - '*' - '*/*' verbs: - '*' Web-UI \u00b6 Config Table \u00b6 The Configs component will initially load with a table presenting all the available configuration files. It is possible to filter in two ways: There is a pull-down selector, which will allow filtering on a specific Kind of config; and, A filter field will allow filtering on Name (if you have filtered on Kind , it will be limited to that kind of config). Clicking on the X next to the Find a Config field will reset both filters. View a Config \u00b6 There are two ways to view a specific config: Clicking the name of the config entry will open a modal to view the config. It is possible to select text and copy from this modal. However, it is not possible to edit the config directly in this view. Instead, there is an Edit Config button, which will open an editor. It is also possible to download a copy of the config. Selecting the Edit button for a given config will open the editor window. It is possible to edit the config in this window and save the update to the ph\u0113nix store. The config will be validated against the schema, and if invalid, an error will be presented. Create a Config \u00b6 There are two ways to create a new config: Selecting the Upload button located to the far right of the search field will allow you to upload a file from disk. It is possible to drag and drop in the upload modal or click the upload icon to open a file browser. Note Only files with the extensions .yml , .yaml , and .json will be allowed. Any file uploaded will be validated against the schema for a given kind of config, and if invalid, an error will be presented. Clicking the + button next to the search field will present an editor window where it is possible to create a new file in place. A basic template is provided based on the kind of config selected (see next section for documentation on the editor window features). Once editing a config is completed, clicking the Create button will post it server-side where it will be validated, similar to file upload. If invalid, an error will be presented. Editor Window \u00b6 The editor window allows direct editing of an existing or new config file. It is possible to edit a file as YAML or JSON by selecting the relevant radio button under File Format . It is also possible to set keybindings to the Vim editor by enabling them with the sliding selector under Vim Keybindings . New Config \u00b6 The editor window for creating a new config will allow editing the filename in a text field to the left or within the config editor directly. Similarly, use the pull-down selector to the left to select the kind of config or within the config editor. Selecting the Create button will post the new config server-side for validation and addition to the store. Choosing the Exit button will leave the editor window; nothing will be saved. Existing Config \u00b6 Changing the name in an existing config is only available through the editor window. Selecting the Save button will post any updates server-side for validation and addition to the store. Selecting the Exit button will leave the editor window; nothing will be saved. Delete a Config \u00b6 Selecting the delete icon will delete a config. A confirmation dialog will be presented to confirm the selection. Once the config is deleted, it cannot be restored to the store. If an experiment config is deleted, the corresponding experiment will not longer be presented in the Experiments tab. Builder \u00b6 The Builder app is an external app that allows users to generate either topology or experiment configuration using a graphical interface. It is based on the minibuilder app in minimega . Users can access the Builder app via the Builder tab in the ph\u0113nix UI. Creating or Editing a Topology \u00b6 When Builder opens, it does so in a new configuration. If a user wanted to open an existing configuration, they are available in the File menu through the Import from ph\u0113nix or Import from Disk selections. A user can add VM hosts or networking components by selecting the relevant image on the left side of the Builder canvas. Each time a configuration is saved to ph\u0113nix, it is available to select from Import from ph\u0113nix in the File menu. A configuration created in Builder can only be edited while in the Builder app. There are two options for editing an existing configuration created in Builder. Select Import from ph\u0113nix in Builder \u2014 make any changes and then add to ph\u0113nix with a new name Select Import from ph\u0113nix in Builder \u2014 make any changes and then add to ph\u0113nix with the same name; this will overwrite the configuration that was selected to import Note Any hosts added to a topology will not have a drive image name; this is a requirement and will need to be included in each node added to the Builder canvas. Other values will be auto-generated but can be customized by clicking on a target node and making changes in the dialogue presented in the Builder UI. It is worth noting that a user can set a single node value and then copy and paste that node multiple times; the customized values will be extended to each node that is pasted in the Builder canvas. While scenarios are not a part of the Builder environment, a user can add them to a topology. When a user selects Save to ph\u0113nix, a pulldown will be presented with available scenarios from the ph\u0113nix store. When a scenario is selected, Builder will write the topology configuration to the ph\u0113nix store and an experiment configuration. When a scenario is not specified, the Builder app will save the topology configuration only. The user will then need to create an experiment in the ph\u0113nix UI.","title":"Configuration Files"},{"location":"configuration/#configuration-files","text":"phenix currently supports six (6) different configuration file types: Topology Scenario Experiment Image User Role Typically, users will create Topology and Scenario configuration files by hand, while the rest will be generated by phenix using available commands. However, it is possible to create all configuration file types by hand if necessary. Note Configuration files can also be created, viewed, and deleted via the Web-UI (see Web-UI section below). Configuration files are versioned using a header section based heavily on what Kubernetes does. Each configuration file will have a header section that looks like the following: apiVersion: phenix.sandia.gov/v1 kind: Topology metadata: name: foobar spec: ... In the example above, the kind field represents the type of configuration file (e.g., Topology, Scenario, Image). The apiVersion field represents the version the spec section conforms to (currently there is only v1 for all configuration types except Scenario , which is v2 ), and the spec section will contain the actual details for the configuration type based on the configuration schema. Note that, at least in the header section, keys are camel-case and begin with a lowercase letter, while values are camel-cased but begin with a capital letter. Note Throughout the documentation, we mention creating configurations manually . When we say this, we mean passing a YAML or JSON configuration file of any type to the phenix config create command.","title":"Configuration Files"},{"location":"configuration/#topology","text":"The Topology configuration is one of the core configuration types for phenix , as it describes a network topology to be deployed in minimega that can be used by one or more experiments to be executed. A topology is comprised of one or more nodes , which is a VM, each including system descriptions and configurations, as well as any networking settings required to connect all of the nodes in a topology together. This configuration becomes the basis for most of the minimega commands later created in the relevant minimega startup script.","title":"Topology"},{"location":"configuration/#default-settings","text":"If left unmodified or unset, the following are the default settings for each node: memory will be set to 512 MB vcpus will be set to 1 snapshot will be set to true no network settings will be included","title":"Default Settings"},{"location":"configuration/#required-values","text":"Each topology must have a unique name, which should be lowercase and not include spaces. In addition, each node in the topology must: have a specified type \u2014 the available types are defined in the schema have a unique hostname have an OS type of linux or windows have a disk image assigned","title":"Required Values"},{"location":"configuration/#optional-values","text":"Optional values for a node in the topology configuration can include: static network configurations specific memory values (e.g., 1-16 GB) specific VCPUs values (e.g., 1-4 ) additional disk storage file injections labels, which are typically used by ph\u0113nix apps routing ruleset(s) delay triggered by user , time , or c2 (command and control)","title":"Optional Values"},{"location":"configuration/#delay-start","text":"It is possible to delay the start of a VM with the delay value. There are three options available to set, but on only one option can be set: user is a boolean value and when set to true the VM will require a manual start either through the ph\u0113nix UI or command line (the latter can be by ph\u0113nix or minimega commands). time is a string that is set as a delay in minutes; e.g., 5m . c2 requires minimega command and control. Note in the example below that the hostname AD1 will be delayed starting until the VM host-00 has started and checked in with minimega command and control. It is possible to have multiple hostnames included. useUUID is an additional boolean value setting, per hostname, that will watch for the UUID instead of the hostname to register with minimega command and control. Note If one or more VMs are set with a delayed start, the UI will display a blue tag in the Delay column next to the Screenshot column. The tag will indicate what type of dealy was set. Once all the delayed VMs have started, the Delay column will no longer be visible.","title":"Delay Start"},{"location":"configuration/#network-address-translation-nat","text":"For nodes of type Router , basic source NAT can be configured to masquerade packets from one or more networks out a specific interface. For example, assume a router has three interfaces \u2014 eth0, eth1, eth2 . eth0 is connected to the Internet, while eth1, eth2 are connected to networks that use private IP space. To masquerade packets from eth1, eth2 out eth0 the following configuration could be used. network: nat: - out: eth0 in: - eth1 - eth2","title":"Network Address Translation (NAT)"},{"location":"configuration/#example","text":"A contrived, four node example \u2014 three VMs and a router \u2014 is given below, and is driven by the topology schema described here . apiVersion: phenix.sandia.gov/v1 kind: Topology metadata: name: foobar spec: nodes: - type: VirtualMachine general: hostname: host-00 snapshot: true delay: user: true hardware: os_type: linux drives: - image: ubuntu.qc2 injections: - src: foo/bar/sucka.fish dst: /data/sucka.fish - src: /foo/bar/sucka/fish.sh dst: /data/fish.sh network: interfaces: - name: IF0 vlan: EXP-1 address: 192.168.10.1 mask: 24 gateway: 192.168.10.254 proto: static type: ethernet - name: IF1 vlan: MGMT address: 172.16.10.1 mask: 16 proto: static type: ethernet - type: VirtualMachine general: hostname: host-01 snapshot: true do_not_boot: false delay: time: 5m hardware: os_type: linux drives: - image: ubuntu.qc2 network: interfaces: - name: IF0 vlan: EXP-1 address: 192.168.10.2 mask: 24 gateway: 192.168.10.254 proto: static type: ethernet - name: IF1 vlan: MGMT address: 172.16.10.2 mask: 16 proto: static type: ethernet - name: S0 vlan: EXT address: 10.0.0.1 mask: 24 proto: static type: serial udp_port: 8989 baud_rate: 9600 device: /dev/ttyS0 - type: VirtualMachine general: hostname: AD1 snapshot: true delay: c2: - hostname: host-00 useUUID: false hardware: os_type: windows drives: - image: win-svr-2k8.qc2 network: interfaces: - name: IF0 vlan: EXP-1 address: 192.168.10.250 mask: 24 gateway: 192.168.10.254 proto: static type: ethernet - name: IF1 vlan: MGMT address: 172.16.10.3 mask: 16 proto: static type: ethernet - type: Router labels: ntp-server: \"true\" general: hostname: router-00 snapshot: true hardware: os_type: linux drives: - image: vyatta.qc2 network: interfaces: - name: IF0 vlan: EXP-1 address: 192.168.10.254 mask: 24 proto: static type: ethernet ruleset_in: test - name: IF1 vlan: MGMT address: 172.16.10.254 mask: 16 proto: static type: ethernet rulesets: - name: test default: drop rules: - id: 10 action: accept protocol: all source: address: 1.1.1.1 port: 53","title":"Example"},{"location":"configuration/#scenario","text":"The Scenario configuration is used to define and configure one or more phenix apps ( default or user ) for use with a topology. In this sense, a topology can have one or more scenarios associated with it, but a scenario can only be associated with a single topology.","title":"Scenario"},{"location":"configuration/#apps","text":"A phenix app can be applied to an experiment topology using a single configuration, a per-host configuration, or both. Examples could include: a phenix app that adds a minimega tap to all hosts in the cluster, one that injects the same file into every node in the experiment topology, or one that configures a VPN between two nodes in an experiment using WireGuard. The first two examples would involve a single configuration, while the last example would use a per-host configuration because (1) only two nodes will be modified, and (2) each of the two nodes will require different configurations (e.g., one will be a WireGuard client and the other a WireGuard server). Each configured app can contain a list of topology nodes to apply the app to, along with custom metadata for the app specific to the topology node.","title":"Apps"},{"location":"configuration/#app-configuration-options","text":"assetDir : used by apps to generate absolute path to asset files when relative paths are provided in app metadata. The default is an empty string. fromScenario : name of another scenario config to pull this app config from. This allows for defining complex app configurations in a single base scenario and referencing it from scenarios included in experiments. The default is an empty string. hosts : a list of per-host configurations to apply to the experiment topology. The default is nil . hostname : the name of the experiment VM to apply this per-host metadata to. metadata : app metadata to apply to this experiment VM. The default is nil . metadata : app metadata to apply to this experiment. The default is nil . name : the name of the app being configured. There is no default value; one must be provided. runPeriodically : a Golang duration string specifying how often to trigger the app's running stage. The default value is an empty string, which means the app's running stage will not be triggered periodically.","title":"App Configuration Options"},{"location":"configuration/#example_1","text":"The following is an example of a configuration for a scenario named foobar , which can only be applied to an accompanying topology named foobar (while these names are the same in this example, the topology and scenario names do not have to match). Included in this scenario are apps named miniccc-injector , startup , protonuke , and wireguard . Each entry in the list of app hosts includes custom app metadata and the hostname of the topology node to apply the metadata. apiVersion: phenix.sandia.gov/v1 kind: Scenario metadata: name: foobar annotations: topology: foobar spec: apps: - name: miniccc-injector metadata: # files to inject into each node in experiment, based on OS type linux: src: /phenix/injects/miniccc dst: /usr/local/bin/miniccc windows: src: /phenix/injects/miniccc.exe dst: phenix/miniccc.exe - name: startup hosts: - hostname: host-00 # hostname of topology node to apply it to metadata: domain_controller: domain: example.com ip: 10.0.0.1 username: admin password: SuperSecretPassword - name: protonuke hosts: - hostname: host-01 # hostname of topology node to apply it to metadata: # protonuke app metadata for this topology node args: -logfile /var/log/protonuke.log -level debug -http -https -smtp -ssh 192.168.100.100 - name: wireguard hosts: - hostname: AD1 # hostname of topology node to apply it to metadata: # wireguard app metadata for this topology node infrastructure: private_key: GLlxWJom8cQViGHojqOUShWIZG7IsSX8 address: 10.255.255.1/24 listen_port: 51820 peers: public_key: +joyya2F9g72qbKBtPDn00mIevG1j1OqeN76ylFLsiE= allowed_ips: 10.255.255.10/32 Note The above example includes an app named startup , which is a ph\u0113nix default app. Meaning, it is possible to configure default ph\u0113nix apps in a scenario configuration, not just user apps. Note See Scorch for additional information on Scenario Orchestration","title":"Example"},{"location":"configuration/#experiment","text":"Experiment configurations represent the combination of topologies and scenarios to form an experiment (it should be note that an experiments do not require a scenario). Typically experiment configurations are created automatically , but it is possible to create them manually using a configuration file similar to the one shown below. In this case, an experiment named foobar would be created based on an existing topology named foobar and an existing scenario named foobar (note that none of the names are required to match). apiVersion: phenix.sandia.gov/v1 kind: Experiment metadata: name: foobar annotations: topology: foobar # this is required scenario: foobar # this is optional Once created, either manually or automatically, the experiment configuration will be expanded to have the topology and scenario configurations embedded in it, as well as additional details like cluster host schedules for VMs, VLAN ranges, Builder XML, etc. The advantage of embedding the topology and scenario into the experiment is that they can be modified in the experiment without modifying the originals.","title":"Experiment"},{"location":"configuration/#image","text":"The Image configuration is used to generate VM disk images using a custom version of vmdb2 . Representing a disk image in a configuration like this allows for the same disk image to easily be built in different clusters without having to actually move large disk image files. Note The ph\u0113nix image capability will only generation Linux based images. It will not generate Windows disk images. Typically image configurations are created automatically by the phenix image create command, but users can also create them manually using a configuration file similar to the one shown below. This file can then be created in the ph\u0113nix store using the phenix cfg create <path-to-file> command. apiVersion: phenix.sandia.gov/v1 kind: Image metadata: name: foobar spec: format: qcow2 mirror: http://us.archive.ubuntu.com/ubuntu/ overlay: [] packages: - initramfs-tools - net-tools - isc-dhcp-client - openssh-server - init - iputils-ping - vim - less - netbase - curl - ifupdown - dbus - linux-image-generic - linux-headers-generic release: bionic size: 5G variant: minbase scripts: POSTBUILD_APT_CLEANUP: | apt clean || apt-get clean || echo \"unable to clean apt cache\" POSTBUILD_NO_ROOT_PASSWD: | sed -i 's/nullok_secure/nullok/' /etc/pam.d/common-auth sed -i 's/#PermitRootLogin prohibit-password/PermitRootLogin yes/' /etc/ssh/sshd_config sed -i 's/#PermitEmptyPasswords no/PermitEmptyPasswords yes/' /etc/ssh/sshd_config sed -i 's/PermitRootLogin prohibit-password/PermitRootLogin yes/' /etc/ssh/sshd_config sed -i 's/PermitEmptyPasswords no/PermitEmptyPasswords yes/' /etc/ssh/sshd_config passwd -d root Note If no overlay is used in the image configuration, an empty array must be provided","title":"Image"},{"location":"configuration/#user","text":"The User configuration tracks ph\u0113nix UI user settings (e.g., username, password, and RBAC permissions). Typically user configurations are created automatically when a UI admin creates a new user via the UI, but they can also be created manually using a configuration file similar to the one shown below. apiVersion: phenix.sandia.gov/v1 kind: User metadata: name: admin@foo.com spec: username: admin@foo.com first_name: Admin last_name: Istrator password: **************** rbac: roleName: Global Admin policies: - resourceNames: - '*' resources: - '*' - '*/*' verbs: - '*' - resourceNames: - admin@foo.com resources: - users verbs: - get","title":"User"},{"location":"configuration/#role","text":"The Role configuration is used to represent a named set of RBAC permissions that represent a user's role in the UI. When a new user is created, the role that user should have is specified, and using that role name the appropriate RBAC permissions are copied from the role configuration into the user configuration. There are six (6) default role configurations that get created automatically, and are described here . An example role configuration is shown below for completeness. apiVersion: phenix.sandia.gov/v1 kind: Role metadata: name: global-admin spec: roleName: Global Admin policies: - resourceNames: - '*' resources: - '*' - '*/*' verbs: - '*'","title":"Role"},{"location":"configuration/#web-ui","text":"","title":"Web-UI"},{"location":"configuration/#config-table","text":"The Configs component will initially load with a table presenting all the available configuration files. It is possible to filter in two ways: There is a pull-down selector, which will allow filtering on a specific Kind of config; and, A filter field will allow filtering on Name (if you have filtered on Kind , it will be limited to that kind of config). Clicking on the X next to the Find a Config field will reset both filters.","title":"Config Table"},{"location":"configuration/#view-a-config","text":"There are two ways to view a specific config: Clicking the name of the config entry will open a modal to view the config. It is possible to select text and copy from this modal. However, it is not possible to edit the config directly in this view. Instead, there is an Edit Config button, which will open an editor. It is also possible to download a copy of the config. Selecting the Edit button for a given config will open the editor window. It is possible to edit the config in this window and save the update to the ph\u0113nix store. The config will be validated against the schema, and if invalid, an error will be presented.","title":"View a Config"},{"location":"configuration/#create-a-config","text":"There are two ways to create a new config: Selecting the Upload button located to the far right of the search field will allow you to upload a file from disk. It is possible to drag and drop in the upload modal or click the upload icon to open a file browser. Note Only files with the extensions .yml , .yaml , and .json will be allowed. Any file uploaded will be validated against the schema for a given kind of config, and if invalid, an error will be presented. Clicking the + button next to the search field will present an editor window where it is possible to create a new file in place. A basic template is provided based on the kind of config selected (see next section for documentation on the editor window features). Once editing a config is completed, clicking the Create button will post it server-side where it will be validated, similar to file upload. If invalid, an error will be presented.","title":"Create a Config"},{"location":"configuration/#editor-window","text":"The editor window allows direct editing of an existing or new config file. It is possible to edit a file as YAML or JSON by selecting the relevant radio button under File Format . It is also possible to set keybindings to the Vim editor by enabling them with the sliding selector under Vim Keybindings .","title":"Editor Window"},{"location":"configuration/#new-config","text":"The editor window for creating a new config will allow editing the filename in a text field to the left or within the config editor directly. Similarly, use the pull-down selector to the left to select the kind of config or within the config editor. Selecting the Create button will post the new config server-side for validation and addition to the store. Choosing the Exit button will leave the editor window; nothing will be saved.","title":"New Config"},{"location":"configuration/#existing-config","text":"Changing the name in an existing config is only available through the editor window. Selecting the Save button will post any updates server-side for validation and addition to the store. Selecting the Exit button will leave the editor window; nothing will be saved.","title":"Existing Config"},{"location":"configuration/#delete-a-config","text":"Selecting the delete icon will delete a config. A confirmation dialog will be presented to confirm the selection. Once the config is deleted, it cannot be restored to the store. If an experiment config is deleted, the corresponding experiment will not longer be presented in the Experiments tab.","title":"Delete a Config"},{"location":"configuration/#builder","text":"The Builder app is an external app that allows users to generate either topology or experiment configuration using a graphical interface. It is based on the minibuilder app in minimega . Users can access the Builder app via the Builder tab in the ph\u0113nix UI.","title":"Builder"},{"location":"configuration/#creating-or-editing-a-topology","text":"When Builder opens, it does so in a new configuration. If a user wanted to open an existing configuration, they are available in the File menu through the Import from ph\u0113nix or Import from Disk selections. A user can add VM hosts or networking components by selecting the relevant image on the left side of the Builder canvas. Each time a configuration is saved to ph\u0113nix, it is available to select from Import from ph\u0113nix in the File menu. A configuration created in Builder can only be edited while in the Builder app. There are two options for editing an existing configuration created in Builder. Select Import from ph\u0113nix in Builder \u2014 make any changes and then add to ph\u0113nix with a new name Select Import from ph\u0113nix in Builder \u2014 make any changes and then add to ph\u0113nix with the same name; this will overwrite the configuration that was selected to import Note Any hosts added to a topology will not have a drive image name; this is a requirement and will need to be included in each node added to the Builder canvas. Other values will be auto-generated but can be customized by clicking on a target node and making changes in the dialogue presented in the Builder UI. It is worth noting that a user can set a single node value and then copy and paste that node multiple times; the customized values will be extended to each node that is pasted in the Builder canvas. While scenarios are not a part of the Builder environment, a user can add them to a topology. When a user selects Save to ph\u0113nix, a pulldown will be presented with available scenarios from the ph\u0113nix store. When a scenario is selected, Builder will write the topology configuration to the ph\u0113nix store and an experiment configuration. When a scenario is not specified, the Builder app will save the topology configuration only. The user will then need to create an experiment in the ph\u0113nix UI.","title":"Creating or Editing a Topology"},{"location":"experiments/","text":"Experiments \u00b6 Listing Experiments \u00b6 From the Web-UI \u00b6 Click on the Experiments tab. This will display all available experiments that the user has access to view or edit. From the Command Line Binary \u00b6 This will display a list of all available experiments: it is run as a root user. $> phenix exp list Starting / Stopping Experiments \u00b6 From the Web-UI \u00b6 Clicking the stopped button will start the experiment; similarly the started button will stop the experiment. A progress bar is used to update the progress of starting an experiment. During the update to the experiment -- starting or stopping -- it will not be accessible or available to delete. From the Command Line Binary \u00b6 $> phenix exp start <experiment name> Or ... $> phenix exp stop <experiment name> Or ... $> phenix exp restart <experiment name> Optionally, you can use the --dry-run flag to do everything except call out to minimega. The phenix exp --help command will output: Experiment management Usage: phenix experiment [flags] phenix experiment [command] Aliases: experiment, exp Available Commands: apps List of available apps to assign an experiment create Create an experiment delete Delete an experiment list Display a table of available experiments restart Start an experiment schedule Schedule an experiment schedulers List of available schedulers to assign an experiment start Start an experiment stop Stop an experiment Flags: -h, --help help for experiment Global Flags: --base-dir.minimega string base minimega directory (default \"/tmp/minimega\") --base-dir.phenix string base phenix directory (default \"/phenix\") --hostname-suffixes string hostname suffixes to strip --log.error-file string log fatal errors to file (default \"/root/.phenix.err\") --log.error-stderr log fatal errors to STDERR --store.endpoint string endpoint for storage service (default \"bolt:///root/.phenix.bdb\") Use \"phenix experiment [command] --help\" for more information about a command. Create a New Experiment \u00b6 From the Web-UI \u00b6 Click the + button to the right of the filter field. Enter Experiment Name and Experiment Topology , the remaining selection are optional. In this example, bennu is an example topology and is not included by default. You will need to create your own topology(ies). From the Command Line Binary \u00b6 Three options are available from the command line. The only requirements are for an experiment and topology name; scenario and base directory are optional. $> phenix experiment create <experiment name> -t <topology name> $> phenix experiment create <experiment name> -t <topology name> -s <scenario name> $> phenix experiment create <experiment name> -t <topology name> -s <scenario name> -d </path/to/dir/>` The phenix exp create --help command will output: Create an experiment Used to create an experiment from an existing configuration; can be a topology, or topology and scenario. (Optional are the arguments for scenario or base directory.) Usage: phenix experiment create <experiment name> [flags] Examples: phenix experiment create <experiment name> -t <topology name> phenix experiment create <experiment name> -t <topology name> -s <scenario name> phenix experiment create <experiment name> -t <topology name> -s <scenario name> -d </path/to/dir/> Flags: -d, --base-dir string Base directory to use for experiment (optional) -h, --help help for create -s, --scenario string Name of an existing scenario to use (optional) -t, --topology string Name of an existing topology to use Global Flags: --base-dir.minimega string base minimega directory (default \"/tmp/minimega\") --base-dir.phenix string base phenix directory (default \"/phenix\") --hostname-suffixes string hostname suffixes to strip --log.error-file string log fatal errors to file (default \"/root/.phenix.err\") --log.error-stderr log fatal errors to STDERR --store.endpoint string endpoint for storage service (default \"bolt:///root/.phenix.bdb\") Scheduling an Experiment \u00b6 From Web-UI \u00b6 The experiment must be stopped; click on the experiment name to enter the Stopped Experiment component. Click on the hamburger menu to the right of the filter field and start button to select a desired schedule. From the Command Line Binary \u00b6 The list of available schedules can be found by running the folowing command. $> phenix exp schedulers Then apply the desired schedule with the following command. $> phenix experiment schedule <experiment name> <algorithm>","title":"Experiments"},{"location":"experiments/#experiments","text":"","title":"Experiments"},{"location":"experiments/#listing-experiments","text":"","title":"Listing Experiments"},{"location":"experiments/#from-the-web-ui","text":"Click on the Experiments tab. This will display all available experiments that the user has access to view or edit.","title":"From the Web-UI"},{"location":"experiments/#from-the-command-line-binary","text":"This will display a list of all available experiments: it is run as a root user. $> phenix exp list","title":"From the Command Line Binary"},{"location":"experiments/#starting-stopping-experiments","text":"","title":"Starting / Stopping Experiments"},{"location":"experiments/#from-the-web-ui_1","text":"Clicking the stopped button will start the experiment; similarly the started button will stop the experiment. A progress bar is used to update the progress of starting an experiment. During the update to the experiment -- starting or stopping -- it will not be accessible or available to delete.","title":"From the Web-UI"},{"location":"experiments/#from-the-command-line-binary_1","text":"$> phenix exp start <experiment name> Or ... $> phenix exp stop <experiment name> Or ... $> phenix exp restart <experiment name> Optionally, you can use the --dry-run flag to do everything except call out to minimega. The phenix exp --help command will output: Experiment management Usage: phenix experiment [flags] phenix experiment [command] Aliases: experiment, exp Available Commands: apps List of available apps to assign an experiment create Create an experiment delete Delete an experiment list Display a table of available experiments restart Start an experiment schedule Schedule an experiment schedulers List of available schedulers to assign an experiment start Start an experiment stop Stop an experiment Flags: -h, --help help for experiment Global Flags: --base-dir.minimega string base minimega directory (default \"/tmp/minimega\") --base-dir.phenix string base phenix directory (default \"/phenix\") --hostname-suffixes string hostname suffixes to strip --log.error-file string log fatal errors to file (default \"/root/.phenix.err\") --log.error-stderr log fatal errors to STDERR --store.endpoint string endpoint for storage service (default \"bolt:///root/.phenix.bdb\") Use \"phenix experiment [command] --help\" for more information about a command.","title":"From the Command Line Binary"},{"location":"experiments/#create-a-new-experiment","text":"","title":"Create a New Experiment"},{"location":"experiments/#from-the-web-ui_2","text":"Click the + button to the right of the filter field. Enter Experiment Name and Experiment Topology , the remaining selection are optional. In this example, bennu is an example topology and is not included by default. You will need to create your own topology(ies).","title":"From the Web-UI"},{"location":"experiments/#from-the-command-line-binary_2","text":"Three options are available from the command line. The only requirements are for an experiment and topology name; scenario and base directory are optional. $> phenix experiment create <experiment name> -t <topology name> $> phenix experiment create <experiment name> -t <topology name> -s <scenario name> $> phenix experiment create <experiment name> -t <topology name> -s <scenario name> -d </path/to/dir/>` The phenix exp create --help command will output: Create an experiment Used to create an experiment from an existing configuration; can be a topology, or topology and scenario. (Optional are the arguments for scenario or base directory.) Usage: phenix experiment create <experiment name> [flags] Examples: phenix experiment create <experiment name> -t <topology name> phenix experiment create <experiment name> -t <topology name> -s <scenario name> phenix experiment create <experiment name> -t <topology name> -s <scenario name> -d </path/to/dir/> Flags: -d, --base-dir string Base directory to use for experiment (optional) -h, --help help for create -s, --scenario string Name of an existing scenario to use (optional) -t, --topology string Name of an existing topology to use Global Flags: --base-dir.minimega string base minimega directory (default \"/tmp/minimega\") --base-dir.phenix string base phenix directory (default \"/phenix\") --hostname-suffixes string hostname suffixes to strip --log.error-file string log fatal errors to file (default \"/root/.phenix.err\") --log.error-stderr log fatal errors to STDERR --store.endpoint string endpoint for storage service (default \"bolt:///root/.phenix.bdb\")","title":"From the Command Line Binary"},{"location":"experiments/#scheduling-an-experiment","text":"","title":"Scheduling an Experiment"},{"location":"experiments/#from-web-ui","text":"The experiment must be stopped; click on the experiment name to enter the Stopped Experiment component. Click on the hamburger menu to the right of the filter field and start button to select a desired schedule.","title":"From Web-UI"},{"location":"experiments/#from-the-command-line-binary_3","text":"The list of available schedules can be found by running the folowing command. $> phenix exp schedulers Then apply the desired schedule with the following command. $> phenix experiment schedule <experiment name> <algorithm>","title":"From the Command Line Binary"},{"location":"image/","text":"Virtual Disk Images Management \u00b6 This is only available from the command line binary at this time. Listing disk images \u00b6 $> phenix image list Creating a disk image \u00b6 The vmdb2 utility is required -- in path -- to create the disk images. $> phenix image create <image name> The phenix image create --help will output: Create a disk image configuration Used to create a virtual disk image configuration from which to build an image Usage: phenix image create <image name> [flags] Examples: phenix image create <image name> phenix image create --size 2G --variant mingui --release xenial --format qcow2 --compress --overlays foobar --packages foo --scripts bar <image name> Flags: -c, --compress Compress image after creation (does not apply to raw image) -d, --debootstrap-append string Additional arguments to debootstrap \"(default: --components=main,restricted,universe,multiverse)\" -f, --format string Format of disk image (default \"raw\") -h, --help help for create -m, --mirror string Debootstrap mirror (must match release) (default \"http://us.archive.ubuntu.com/ubuntu/\") -O, --overlays string List of overlay names (include full path; separated by comma) -P, --packages string List of packages to include in addition to those provided by variant (separated by comma) -R, --ramdisk Create a kernel/initrd pair in addition to a disk image -r, --release string OS release codename (default \"bionic\") -T, --scripts string List of scripts to include in addition to the defaults (include full path; separated by comma) -s, --size string Image size to use (default \"5G\") -v, --variant string Image variant to use (default \"minbase\") Global Flags: --base-dir.minimega string base minimega directory (default \"/tmp/minimega\") --base-dir.phenix string base phenix directory (default \"/phenix\") --hostname-suffixes string hostname suffixes to strip --log.error-file string log fatal errors to file (default \"/var/log/phenix/error.log\") --log.error-stderr log fatal errors to STDERR --store.endpoint string endpoint for storage service (default \"bolt:///etc/phenix/store.bdb\") The vmdb2 configuration file can be read by running the following command: $> phenix cfg get image/<image name> Building a disk image \u00b6 Building a disk image requires an existing configuration in the store (i.e., the create command should be run first to create a configuration); the phenix image build --help will output: Build a virtual disk image Used to build a new virtual disk using an exisitng configuration; vmdb2 must be in path. Usage: phenix image build <configuration name> [flags] Examples: phenix image build <configuration name> phenix image build --very-very-verbose --output </path/to/dir/> Flags: -c, --cache Cache rootfs as tar archive --dry-run Do everything but actually call out to vmdb2 -h, --help help for build -o, --output string Specify the output directory for the disk image to be saved to -v, --verbose Enable verbose output -w, --very-verbose Enable very verbose output -x, --very-very-verbose Enable very verbose output plus additional verbose output from debootstrap Global Flags: --base-dir.minimega string base minimega directory (default \"/tmp/minimega\") --base-dir.phenix string base phenix directory (default \"/phenix\") --hostname-suffixes string hostname suffixes to strip --log.error-file string log fatal errors to file (default \"/var/log/phenix/error.log\") --log.error-stderr log fatal errors to STDERR --store.endpoint string endpoint for storage service (default \"bolt:///etc/phenix/store.bdb\") Miscellaneous Commands \u00b6 Append \u00b6 The disk image management tool will allow you to add packages, overlays, and scripts to exisitng configurations using the append command. Command usage is: $> phenix image append <configuration name> [flags] Flags are for the overlays, packages, and scripts that you want to append. Create From an Existing Configuration \u00b6 Run this command if you have an existing configuration that you would like to use as the base to create a new configuration from. The usage involves referencing the existing configuration, the new configuration name, and then additional packages, overlays, and scripts. $> phenix image create-from <existing configuration> <new configuration> [flags] Flags are for the overlays, packages, and scripts that you want to add to the new configuration. Delete \u00b6 $> phenix image delete <image name> An alternative could be to use the configuration management tool. $> phenix cfg delete image/<image name> Remove \u00b6 The remove command will allow you to remove any packages, overlays, and scripts from an existing image configuration. $> phenix image remove <configuration name> [flags] Flags are for the overlays, packages, and scripts that you want to remove. Update \u00b6 This update command is used to update the script on an existing image configuration. The path to a script is tracked in the code. The image configuration gets updated with the script in the path; if no changes were made no harm. If the path no longer exists, phenix will leave the configuration alone. $> phenix image update <configuration name> Kali Image \u00b6 The Docker image for phenix includes everything needed to build a Kali image. If phenix is installed locally, the following will be needed to create and build a Kali image. To build a Kali release on a non-Kali (but still Debian-based) operating system, the following steps must be taken to prepare the host (Debian-based) OS first. These steps are based on the official Kali documentation located at: https://www.kali.org/tutorials/build-kali-with-live-build-on-debian-based-systems/. Download and install the latest version of the Kali archive keyring package. At time of writing, the latest version was 2020.2. $> wget http://http.kali.org/kali/pool/main/k/kali-archive-keyring/kali-archive-keyring_2020.2_all.deb $> sudo dpkg -i kali-archive-keyring_2020.2_all.deb Next, create the debootstrap build script for Kali, based entirely off the existing Debian Sid build script. Note that the following commands will likely need to be run as root. $> cd /usr/share/debootstrap/scripts $> sed -e \"s/debian-archive-keyring.gpg/kali-archive-keyring.gpg/g\" sid > kali $> ln -s kali kali-rolling At this point, you should be able to build a Kali release with phenix image .","title":"Virtual Disk Images Management"},{"location":"image/#virtual-disk-images-management","text":"This is only available from the command line binary at this time.","title":"Virtual Disk Images Management"},{"location":"image/#listing-disk-images","text":"$> phenix image list","title":"Listing disk images"},{"location":"image/#creating-a-disk-image","text":"The vmdb2 utility is required -- in path -- to create the disk images. $> phenix image create <image name> The phenix image create --help will output: Create a disk image configuration Used to create a virtual disk image configuration from which to build an image Usage: phenix image create <image name> [flags] Examples: phenix image create <image name> phenix image create --size 2G --variant mingui --release xenial --format qcow2 --compress --overlays foobar --packages foo --scripts bar <image name> Flags: -c, --compress Compress image after creation (does not apply to raw image) -d, --debootstrap-append string Additional arguments to debootstrap \"(default: --components=main,restricted,universe,multiverse)\" -f, --format string Format of disk image (default \"raw\") -h, --help help for create -m, --mirror string Debootstrap mirror (must match release) (default \"http://us.archive.ubuntu.com/ubuntu/\") -O, --overlays string List of overlay names (include full path; separated by comma) -P, --packages string List of packages to include in addition to those provided by variant (separated by comma) -R, --ramdisk Create a kernel/initrd pair in addition to a disk image -r, --release string OS release codename (default \"bionic\") -T, --scripts string List of scripts to include in addition to the defaults (include full path; separated by comma) -s, --size string Image size to use (default \"5G\") -v, --variant string Image variant to use (default \"minbase\") Global Flags: --base-dir.minimega string base minimega directory (default \"/tmp/minimega\") --base-dir.phenix string base phenix directory (default \"/phenix\") --hostname-suffixes string hostname suffixes to strip --log.error-file string log fatal errors to file (default \"/var/log/phenix/error.log\") --log.error-stderr log fatal errors to STDERR --store.endpoint string endpoint for storage service (default \"bolt:///etc/phenix/store.bdb\") The vmdb2 configuration file can be read by running the following command: $> phenix cfg get image/<image name>","title":"Creating a disk image"},{"location":"image/#building-a-disk-image","text":"Building a disk image requires an existing configuration in the store (i.e., the create command should be run first to create a configuration); the phenix image build --help will output: Build a virtual disk image Used to build a new virtual disk using an exisitng configuration; vmdb2 must be in path. Usage: phenix image build <configuration name> [flags] Examples: phenix image build <configuration name> phenix image build --very-very-verbose --output </path/to/dir/> Flags: -c, --cache Cache rootfs as tar archive --dry-run Do everything but actually call out to vmdb2 -h, --help help for build -o, --output string Specify the output directory for the disk image to be saved to -v, --verbose Enable verbose output -w, --very-verbose Enable very verbose output -x, --very-very-verbose Enable very verbose output plus additional verbose output from debootstrap Global Flags: --base-dir.minimega string base minimega directory (default \"/tmp/minimega\") --base-dir.phenix string base phenix directory (default \"/phenix\") --hostname-suffixes string hostname suffixes to strip --log.error-file string log fatal errors to file (default \"/var/log/phenix/error.log\") --log.error-stderr log fatal errors to STDERR --store.endpoint string endpoint for storage service (default \"bolt:///etc/phenix/store.bdb\")","title":"Building a disk image"},{"location":"image/#miscellaneous-commands","text":"","title":"Miscellaneous Commands"},{"location":"image/#append","text":"The disk image management tool will allow you to add packages, overlays, and scripts to exisitng configurations using the append command. Command usage is: $> phenix image append <configuration name> [flags] Flags are for the overlays, packages, and scripts that you want to append.","title":"Append"},{"location":"image/#create-from-an-existing-configuration","text":"Run this command if you have an existing configuration that you would like to use as the base to create a new configuration from. The usage involves referencing the existing configuration, the new configuration name, and then additional packages, overlays, and scripts. $> phenix image create-from <existing configuration> <new configuration> [flags] Flags are for the overlays, packages, and scripts that you want to add to the new configuration.","title":"Create From an Existing Configuration"},{"location":"image/#delete","text":"$> phenix image delete <image name> An alternative could be to use the configuration management tool. $> phenix cfg delete image/<image name>","title":"Delete"},{"location":"image/#remove","text":"The remove command will allow you to remove any packages, overlays, and scripts from an existing image configuration. $> phenix image remove <configuration name> [flags] Flags are for the overlays, packages, and scripts that you want to remove.","title":"Remove"},{"location":"image/#update","text":"This update command is used to update the script on an existing image configuration. The path to a script is tracked in the code. The image configuration gets updated with the script in the path; if no changes were made no harm. If the path no longer exists, phenix will leave the configuration alone. $> phenix image update <configuration name>","title":"Update"},{"location":"image/#kali-image","text":"The Docker image for phenix includes everything needed to build a Kali image. If phenix is installed locally, the following will be needed to create and build a Kali image. To build a Kali release on a non-Kali (but still Debian-based) operating system, the following steps must be taken to prepare the host (Debian-based) OS first. These steps are based on the official Kali documentation located at: https://www.kali.org/tutorials/build-kali-with-live-build-on-debian-based-systems/. Download and install the latest version of the Kali archive keyring package. At time of writing, the latest version was 2020.2. $> wget http://http.kali.org/kali/pool/main/k/kali-archive-keyring/kali-archive-keyring_2020.2_all.deb $> sudo dpkg -i kali-archive-keyring_2020.2_all.deb Next, create the debootstrap build script for Kali, based entirely off the existing Debian Sid build script. Note that the following commands will likely need to be run as root. $> cd /usr/share/debootstrap/scripts $> sed -e \"s/debian-archive-keyring.gpg/kali-archive-keyring.gpg/g\" sid > kali $> ln -s kali kali-rolling At this point, you should be able to build a Kali release with phenix image .","title":"Kali Image"},{"location":"kb/","text":"Knowledge Base \u00b6 Article EX-SC-UPG-01 \u00b6 The phenix Scenario configuration has been upgraded to v2 to get rid of the distinction between experiment apps and host apps. While the phenix app takes care of upgrading existing v1 scenarios to v2 when viewing, embedding into an experiment, etc., it does not automatically upgrade embedded scenarios in existing experiment configs. Users might encounter the following error message when trying to do anything with experiments that were created with a v1 scenario config: 'scenario.apps': source data must be an array or slice, got map There are two ways to overcome this error: Delete the experiment using the config delete subcommand and recreate it with the experiment create command; or Edit the experiment using the config edit subcommand. If the experiment is running, use the --force flag with config edit . If you choose to edit an existing experiment rather than deleting and recreating, all you need to do is delete the experiment and host keys in the scenario.apps section of the experiment spec and make scenario.apps a list of apps instead of a map. For example, say you have an experiment whose scenario.apps section of the config looks like the following when you go to edit it: scenario: apps: experiment: - name: test-user-app metadata: {} host: - name: protonuke hosts: - hostname: host-00 metadata: args: -logfile /var/log/protonuke.log -level debug -http -https -smtp -ssh 192.168.100.100 After editing, the scenario section should look like the following: scenario: apps: - name: test-user-app metadata: {} - name: protonuke hosts: - hostname: host-00 metadata: args: -logfile /var/log/protonuke.log -level debug -http -https -smtp -ssh 192.168.100.100","title":"Knowledge Base"},{"location":"kb/#knowledge-base","text":"","title":"Knowledge Base"},{"location":"kb/#article-ex-sc-upg-01","text":"The phenix Scenario configuration has been upgraded to v2 to get rid of the distinction between experiment apps and host apps. While the phenix app takes care of upgrading existing v1 scenarios to v2 when viewing, embedding into an experiment, etc., it does not automatically upgrade embedded scenarios in existing experiment configs. Users might encounter the following error message when trying to do anything with experiments that were created with a v1 scenario config: 'scenario.apps': source data must be an array or slice, got map There are two ways to overcome this error: Delete the experiment using the config delete subcommand and recreate it with the experiment create command; or Edit the experiment using the config edit subcommand. If the experiment is running, use the --force flag with config edit . If you choose to edit an existing experiment rather than deleting and recreating, all you need to do is delete the experiment and host keys in the scenario.apps section of the experiment spec and make scenario.apps a list of apps instead of a map. For example, say you have an experiment whose scenario.apps section of the config looks like the following when you go to edit it: scenario: apps: experiment: - name: test-user-app metadata: {} host: - name: protonuke hosts: - hostname: host-00 metadata: args: -logfile /var/log/protonuke.log -level debug -http -https -smtp -ssh 192.168.100.100 After editing, the scenario section should look like the following: scenario: apps: - name: test-user-app metadata: {} - name: protonuke hosts: - hostname: host-00 metadata: args: -logfile /var/log/protonuke.log -level debug -http -https -smtp -ssh 192.168.100.100","title":"Article EX-SC-UPG-01"},{"location":"schema/","text":"Configuration File Schemas \u00b6 The following schemas are all represented using the OpenAPIv3 format. The complete schema is described here . Topology Schema \u00b6 The topology schema is largely comprised of nodes, described here . Topology: type: object title: Demo Topology required: - nodes properties: nodes: type: array title: Nodes items: $ref: \"#/components/schemas/Node\" Scenario Schema \u00b6 Scenario: type: object required: - apps properties: apps: type: array items: type: object required: - name properties: name: type: string minLength: 1 assetDir: type: string metadata: type: object additionalProperties: true hosts: type: array items: type: object required: - hostname - metadata properties: hostname: type: string minLength: 1 metadata: type: object additionalProperties: true Experiment Schema \u00b6 INCOMPLETE SCHEMA This schema definition is not complete. Experiment: type: object required: - experimentName properties: experimentName: type: string minLength: 1 baseDir: type: string vlans: type: object title: VLANs properties: aliases: type: object title: Aliases additionalProperties: type: integer example: MGMT: 200 min: type: integer max: type: integer schedule: type: object title: Schedule additionalProperties: type: string example: ADServer: compute1 Image Schema \u00b6 INCOMPLETE SCHEMA This schema definition is not complete. Image: type: object title: ubuntu Image required: - release properties: release: type: string minLength: 1 User Schema \u00b6 MISSING SCHEMA This schema definition does not (yet) exist. Role Schema \u00b6 MISSING SCHEMA This schema definition does not (yet) exist. Node Schema \u00b6 The node schema contains references to the interface schema described here . Node: type: object title: Node required: - type - general - hardware properties: type: type: string title: Node Type enum: - Firewall - Printer - Router - Server - Switch - VirtualMachine default: VirtualMachine example: VirtualMachine general: type: object title: General Node Configuration required: - hostname properties: hostname: type: string title: Hostname minLength: 1 example: ADServer description: type: string title: Description example: Active Directory Server vm_type: type: string title: VM (Emulation) Type enum: - kvm - container - \"\" default: kvm example: kvm snapshot: type: boolean title: Snapshot Mode default: false example: false nullable: true do_not_boot: type: boolean title: Do Not Boot VM default: false example: false nullable: true delay: type: object title: Delayed start default: null properties: user: type: boolean title: User triggered boot default: false example: true timer: type: string title: Timer triggered boot minLength: 2 example: 5m c2: type: array title: Command and control triggered boot items: type: object title: Host required: - hostname properties: hostname: type: string title: Hostname example: VM1 useUUID: type: boolean title: Use UUID instead of hostname default: false example: true hardware: type: object title: Node Hardware Configuration required: - os_type - drives properties: cpu: type: string title: CPU Emulation enum: - Broadwell - Haswell - core2duo - pentium3 - \"\" default: Broadwell example: Broadwell vcpus: type: integer title: VCPU Count default: 1 example: 4 memory: type: integer title: Memory default: 1024 example: 8192 os_type: type: string title: OS Type enum: - windows - linux - rhel - centos default: linux example: windows drives: type: array title: Drives items: type: object title: Drive required: - image properties: image: type: string title: Image File Name minLength: 1 example: ubuntu.qc2 interface: type: string title: Drive Interface enum: - ahci - ide - scsi - sd - mtd - floppy - pflash - virtio - \"\" default: ide example: ide cache_mode: type: string title: Drive Cache Mode enum: - none - writeback - unsafe - directsync - writethrough - \"\" default: writeback example: writeback inject_partition: type: integer title: Disk Image Partition to Inject Files Into default: 1 example: 2 nullable: true network: type: object title: Node Network Configuration required: - interfaces properties: interfaces: type: array title: Network Interfaces items: type: object title: Network Interface oneOf: - $ref: '#/components/schemas/static_iface' - $ref: '#/components/schemas/dhcp_iface' - $ref: '#/components/schemas/serial_iface' routes: type: array items: type: object title: Network Route required: - destination - next - cost properties: destination: type: string title: Routing Destination minLength: 1 example: 192.168.0.0/24 next: type: string title: Next Hop for Routing Destination minLength: 1 example: 192.168.1.254 cost: type: integer title: Routing Cost (weight) default: 1 example: 1 ospf: type: object title: OSPF Routing Configuration required: - router_id - areas properties: router_id: type: string title: Router ID minLength: 1 example: 0.0.0.1 areas: type: array title: Routing Areas items: type: object title: Routing Area required: - area_id - area_networks properties: area_id: type: integer title: Area ID example: 1 default: 1 area_networks: type: array title: Area Networks items: type: object title: Area Network required: - network properties: network: type: string title: Network minLength: 1 example: 10.1.25.0/24 rulesets: type: array title: Firewall Rulesets items: type: object title: Firewall Ruleset required: - name - default - rules properties: name: type: string title: Ruleset Name minLength: 1 example: OutToDMZ description: type: string title: Ruleset Description minLength: 1 example: From Corp to the DMZ network default: type: string title: Default Firewall Action enum: - accept - drop - reject example: drop rules: type: array title: Firewall Rules items: type: object title: Firewall Rule required: - id - action - protocol properties: id: type: integer title: Rule ID example: 10 description: type: string title: Rule Description example: Allow UDP 10.1.26.80 ==> 10.2.25.0/24:123 action: type: string title: Rule Action enum: - accept - drop - reject example: accept protocol: type: string title: Network Protocol enum: - tcp - udp - icmp - esp - ah - all default: tcp example: tcp source: type: object title: Source Address required: - address properties: address: type: string title: IP Address minLength: 1 example: 10.1.24.60 port: type: integer title: Port Number example: 3389 destination: type: object title: Destination Address required: - address properties: address: type: string title: IP Address minLength: 1 example: 10.1.24.60 port: type: integer title: Port Number example: 3389 injections: type: array title: Node File Injections items: type: object title: Node File Injection required: - src - dst properties: src: type: string title: Location of Source File to Inject minLength: 1 example: foo.xml dst: type: string title: Destination Location to Inject File To minLength: 1 example: /etc/phenix/foo.xml description: type: string title: Description of file being injected example: phenix config file permissions: type: string title: Injected file permissions (UNIX style) example: 0664 Interface Schema \u00b6 iface: type: object required: - name - vlan properties: name: type: string title: Name minLength: 1 example: eth0 vlan: type: string title: VLAN minLength: 1 example: EXP-1 autostart: type: boolean title: Auto Start Interface default: true mac: type: string title: Interface MAC Address example: 00:11:22:33:44:55:66 pattern: '^([0-9a-fA-F]{2}[:-]){5}([0-9a-fA-F]){2}$' mtu: type: integer title: Interface MTU default: 1500 example: 1500 bridge: type: string title: OpenVSwitch Bridge default: phenix driver: type: string title: QEMU network device driver (qemu-kvm -device help) example: e1000 iface_address: type: object required: - address - mask properties: address: type: string format: ipv4 title: IP Address minLength: 7 example: 192.168.1.100 mask: type: integer title: IP Address Netmask minimum: 0 maximum: 32 default: 24 example: 24 gateway: type: string format: ipv4 title: Default Gateway minLength: 7 example: 192.168.1.1 iface_rulesets: type: object properties: ruleset_out: type: string title: Outbound Ruleset example: OutToInet pattern: '^[\\w-]+$' ruleset_in: type: string title: Inbound Ruleset example: InFromInet pattern: '^[\\w-]+$' static_iface: allOf: - $ref: '#/components/schemas/iface' - $ref: '#/components/schemas/iface_address' - $ref: '#/components/schemas/iface_rulesets' required: - type - proto properties: type: type: string title: Interface Type enum: - ethernet default: ethernet example: ethernet proto: type: string title: Interface Protocol enum: - static - ospf default: static example: static dhcp_iface: allOf: - $ref: '#/components/schemas/iface' - $ref: '#/components/schemas/iface_rulesets' required: - type - proto properties: type: type: string title: Interface Type enum: - ethernet default: ethernet example: ethernet proto: type: string title: Interface Protocol enum: - dhcp default: dhcp example: dhcp serial_iface: allOf: - $ref: '#/components/schemas/iface' - $ref: '#/components/schemas/iface_address' - $ref: '#/components/schemas/iface_rulesets' required: - type - proto - udp_port - baud_rate - device properties: type: type: string title: Interface Type enum: - serial default: serial example: serial proto: type: string title: Interface Protocol enum: - static default: static example: static udp_port: type: integer title: UDP Port minimum: 0 maximum: 65535 default: 8989 example: 8989 baud_rate: type: integer title: Serial Baud Rate enum: - 110 - 300 - 600 - 1200 - 2400 - 4800 - 9600 - 14400 - 19200 - 38400 - 57600 - 115200 - 128000 - 256000 default: 9600 example: 9600 device: type: string title: Serial Device minLength: 1 default: /dev/ttyS0 example: /dev/ttyS0 pattern: '^[\\w\\/]+\\w+$' Complete Schema \u00b6 openapi: \"3.0.0\" info: title: phenix version: \"2.0\" paths: {} components: schemas: Image: type: object title: miniccc Image required: - release properties: release: type: string minLength: 1 Topology: type: object title: Demo Topology required: - nodes properties: nodes: type: array title: Nodes items: $ref: \"#/components/schemas/Node\" Scenario: type: object required: - apps properties: apps: type: array items: type: object required: - name properties: name: type: string minLength: 1 assetDir: type: string metadata: type: object additionalProperties: true hosts: type: array items: type: object required: - hostname - metadata properties: hostname: type: string minLength: 1 metadata: type: object additionalProperties: true Experiment: type: object required: - experimentName properties: experimentName: type: string minLength: 1 baseDir: type: string vlans: type: object title: VLANs properties: aliases: type: object title: Aliases additionalProperties: type: integer example: MGMT: 200 min: type: integer max: type: integer schedule: type: object title: Schedule additionalProperties: type: string example: ADServer: compute1 Node: type: object title: Node required: - type - general - hardware properties: type: type: string title: Node Type enum: - Firewall - Printer - Router - Server - Switch - VirtualMachine default: VirtualMachine example: VirtualMachine general: type: object title: General Node Configuration required: - hostname properties: hostname: type: string title: Hostname minLength: 1 example: ADServer description: type: string title: Description example: Active Directory Server vm_type: type: string title: VM (Emulation) Type enum: - kvm - container - \"\" default: kvm example: kvm snapshot: type: boolean title: Snapshot Mode default: false example: false nullable: true do_not_boot: type: boolean title: Do Not Boot VM default: false example: false nullable: true hardware: type: object title: Node Hardware Configuration required: - os_type - drives properties: cpu: type: string title: CPU Emulation enum: - Broadwell - Haswell - core2duo - pentium3 - \"\" default: Broadwell example: Broadwell vcpus: type: integer title: VCPU Count default: 1 example: 4 memory: type: integer title: Memory default: 1024 example: 8192 os_type: type: string title: OS Type enum: - windows - linux - rhel - centos default: linux example: windows drives: type: array title: Drives items: type: object title: Drive required: - image properties: image: type: string title: Image File Name minLength: 1 example: ubuntu.qc2 interface: type: string title: Drive Interface enum: - ahci - ide - scsi - sd - mtd - floppy - pflash - virtio - \"\" default: ide example: ide cache_mode: type: string title: Drive Cache Mode enum: - none - writeback - unsafe - directsync - writethrough - \"\" default: writeback example: writeback inject_partition: type: integer title: Disk Image Partition to Inject Files Into default: 1 example: 2 nullable: true network: type: object title: Node Network Configuration required: - interfaces properties: interfaces: type: array title: Network Interfaces items: type: object title: Network Interface oneOf: - $ref: '#/components/schemas/static_iface' - $ref: '#/components/schemas/dhcp_iface' - $ref: '#/components/schemas/serial_iface' routes: type: array items: type: object title: Network Route required: - destination - next - cost properties: destination: type: string title: Routing Destination minLength: 1 example: 192.168.0.0/24 next: type: string title: Next Hop for Routing Destination minLength: 1 example: 192.168.1.254 cost: type: integer title: Routing Cost (weight) default: 1 example: 1 ospf: type: object title: OSPF Routing Configuration required: - router_id - areas properties: router_id: type: string title: Router ID minLength: 1 example: 0.0.0.1 areas: type: array title: Routing Areas items: type: object title: Routing Area required: - area_id - area_networks properties: area_id: type: integer title: Area ID example: 1 default: 1 area_networks: type: array title: Area Networks items: type: object title: Area Network required: - network properties: network: type: string title: Network minLength: 1 example: 10.1.25.0/24 rulesets: type: array title: Firewall Rulesets items: type: object title: Firewall Ruleset required: - name - default - rules properties: name: type: string title: Ruleset Name minLength: 1 example: OutToDMZ description: type: string title: Ruleset Description minLength: 1 example: From Corp to the DMZ network default: type: string title: Default Firewall Action enum: - accept - drop - reject example: drop rules: type: array title: Firewall Rules items: type: object title: Firewall Rule required: - id - action - protocol properties: id: type: integer title: Rule ID example: 10 description: type: string title: Rule Description example: Allow UDP 10.1.26.80 ==> 10.2.25.0/24:123 action: type: string title: Rule Action enum: - accept - drop - reject example: accept protocol: type: string title: Network Protocol enum: - tcp - udp - icmp - esp - ah - all default: tcp example: tcp source: type: object title: Source Address required: - address properties: address: type: string title: IP Address minLength: 1 example: 10.1.24.60 port: type: integer title: Port Number example: 3389 destination: type: object title: Destination Address required: - address properties: address: type: string title: IP Address minLength: 1 example: 10.1.24.60 port: type: integer title: Port Number example: 3389 injections: type: array title: Node File Injections items: type: object title: Node File Injection required: - src - dst properties: src: type: string title: Location of Source File to Inject minLength: 1 example: foo.xml dst: type: string title: Destination Location to Inject File To minLength: 1 example: /etc/phenix/foo.xml description: type: string title: Description of file being injected example: phenix config file permissions: type: string title: Injected file permissions (UNIX style) example: 0664 iface: type: object required: - name - vlan properties: name: type: string title: Name minLength: 1 example: eth0 vlan: type: string title: VLAN minLength: 1 example: EXP-1 autostart: type: boolean title: Auto Start Interface default: true mac: type: string title: Interface MAC Address example: 00:11:22:33:44:55:66 pattern: '^([0-9a-fA-F]{2}[:-]){5}([0-9a-fA-F]){2}$' mtu: type: integer title: Interface MTU default: 1500 example: 1500 bridge: type: string title: OpenVSwitch Bridge default: phenix iface_address: type: object required: - address - mask properties: address: type: string format: ipv4 title: IP Address minLength: 7 example: 192.168.1.100 mask: type: integer title: IP Address Netmask minimum: 0 maximum: 32 default: 24 example: 24 gateway: type: string format: ipv4 title: Default Gateway minLength: 7 example: 192.168.1.1 iface_rulesets: type: object properties: ruleset_out: type: string title: Outbound Ruleset example: OutToInet pattern: '^[\\w-]+$' ruleset_in: type: string title: Inbound Ruleset example: InFromInet pattern: '^[\\w-]+$' static_iface: allOf: - $ref: '#/components/schemas/iface' - $ref: '#/components/schemas/iface_address' - $ref: '#/components/schemas/iface_rulesets' required: - type - proto properties: type: type: string title: Interface Type enum: - ethernet default: ethernet example: ethernet proto: type: string title: Interface Protocol enum: - static - ospf default: static example: static dhcp_iface: allOf: - $ref: '#/components/schemas/iface' - $ref: '#/components/schemas/iface_rulesets' required: - type - proto properties: type: type: string title: Interface Type enum: - ethernet default: ethernet example: ethernet proto: type: string title: Interface Protocol enum: - dhcp default: dhcp example: dhcp serial_iface: allOf: - $ref: '#/components/schemas/iface' - $ref: '#/components/schemas/iface_address' - $ref: '#/components/schemas/iface_rulesets' required: - type - proto - udp_port - baud_rate - device properties: type: type: string title: Interface Type enum: - serial default: serial example: serial proto: type: string title: Interface Protocol enum: - static default: static example: static udp_port: type: integer title: UDP Port minimum: 0 maximum: 65535 default: 8989 example: 8989 baud_rate: type: integer title: Serial Baud Rate enum: - 110 - 300 - 600 - 1200 - 2400 - 4800 - 9600 - 14400 - 19200 - 38400 - 57600 - 115200 - 128000 - 256000 default: 9600 example: 9600 device: type: string title: Serial Device minLength: 1 default: /dev/ttyS0 example: /dev/ttyS0 pattern: '^[\\w\\/]+\\w+$'","title":"Configuration File Schemas"},{"location":"schema/#configuration-file-schemas","text":"The following schemas are all represented using the OpenAPIv3 format. The complete schema is described here .","title":"Configuration File Schemas"},{"location":"schema/#topology-schema","text":"The topology schema is largely comprised of nodes, described here . Topology: type: object title: Demo Topology required: - nodes properties: nodes: type: array title: Nodes items: $ref: \"#/components/schemas/Node\"","title":"Topology Schema"},{"location":"schema/#scenario-schema","text":"Scenario: type: object required: - apps properties: apps: type: array items: type: object required: - name properties: name: type: string minLength: 1 assetDir: type: string metadata: type: object additionalProperties: true hosts: type: array items: type: object required: - hostname - metadata properties: hostname: type: string minLength: 1 metadata: type: object additionalProperties: true","title":"Scenario Schema"},{"location":"schema/#experiment-schema","text":"INCOMPLETE SCHEMA This schema definition is not complete. Experiment: type: object required: - experimentName properties: experimentName: type: string minLength: 1 baseDir: type: string vlans: type: object title: VLANs properties: aliases: type: object title: Aliases additionalProperties: type: integer example: MGMT: 200 min: type: integer max: type: integer schedule: type: object title: Schedule additionalProperties: type: string example: ADServer: compute1","title":"Experiment Schema"},{"location":"schema/#image-schema","text":"INCOMPLETE SCHEMA This schema definition is not complete. Image: type: object title: ubuntu Image required: - release properties: release: type: string minLength: 1","title":"Image Schema"},{"location":"schema/#user-schema","text":"MISSING SCHEMA This schema definition does not (yet) exist.","title":"User Schema"},{"location":"schema/#role-schema","text":"MISSING SCHEMA This schema definition does not (yet) exist.","title":"Role Schema"},{"location":"schema/#node-schema","text":"The node schema contains references to the interface schema described here . Node: type: object title: Node required: - type - general - hardware properties: type: type: string title: Node Type enum: - Firewall - Printer - Router - Server - Switch - VirtualMachine default: VirtualMachine example: VirtualMachine general: type: object title: General Node Configuration required: - hostname properties: hostname: type: string title: Hostname minLength: 1 example: ADServer description: type: string title: Description example: Active Directory Server vm_type: type: string title: VM (Emulation) Type enum: - kvm - container - \"\" default: kvm example: kvm snapshot: type: boolean title: Snapshot Mode default: false example: false nullable: true do_not_boot: type: boolean title: Do Not Boot VM default: false example: false nullable: true delay: type: object title: Delayed start default: null properties: user: type: boolean title: User triggered boot default: false example: true timer: type: string title: Timer triggered boot minLength: 2 example: 5m c2: type: array title: Command and control triggered boot items: type: object title: Host required: - hostname properties: hostname: type: string title: Hostname example: VM1 useUUID: type: boolean title: Use UUID instead of hostname default: false example: true hardware: type: object title: Node Hardware Configuration required: - os_type - drives properties: cpu: type: string title: CPU Emulation enum: - Broadwell - Haswell - core2duo - pentium3 - \"\" default: Broadwell example: Broadwell vcpus: type: integer title: VCPU Count default: 1 example: 4 memory: type: integer title: Memory default: 1024 example: 8192 os_type: type: string title: OS Type enum: - windows - linux - rhel - centos default: linux example: windows drives: type: array title: Drives items: type: object title: Drive required: - image properties: image: type: string title: Image File Name minLength: 1 example: ubuntu.qc2 interface: type: string title: Drive Interface enum: - ahci - ide - scsi - sd - mtd - floppy - pflash - virtio - \"\" default: ide example: ide cache_mode: type: string title: Drive Cache Mode enum: - none - writeback - unsafe - directsync - writethrough - \"\" default: writeback example: writeback inject_partition: type: integer title: Disk Image Partition to Inject Files Into default: 1 example: 2 nullable: true network: type: object title: Node Network Configuration required: - interfaces properties: interfaces: type: array title: Network Interfaces items: type: object title: Network Interface oneOf: - $ref: '#/components/schemas/static_iface' - $ref: '#/components/schemas/dhcp_iface' - $ref: '#/components/schemas/serial_iface' routes: type: array items: type: object title: Network Route required: - destination - next - cost properties: destination: type: string title: Routing Destination minLength: 1 example: 192.168.0.0/24 next: type: string title: Next Hop for Routing Destination minLength: 1 example: 192.168.1.254 cost: type: integer title: Routing Cost (weight) default: 1 example: 1 ospf: type: object title: OSPF Routing Configuration required: - router_id - areas properties: router_id: type: string title: Router ID minLength: 1 example: 0.0.0.1 areas: type: array title: Routing Areas items: type: object title: Routing Area required: - area_id - area_networks properties: area_id: type: integer title: Area ID example: 1 default: 1 area_networks: type: array title: Area Networks items: type: object title: Area Network required: - network properties: network: type: string title: Network minLength: 1 example: 10.1.25.0/24 rulesets: type: array title: Firewall Rulesets items: type: object title: Firewall Ruleset required: - name - default - rules properties: name: type: string title: Ruleset Name minLength: 1 example: OutToDMZ description: type: string title: Ruleset Description minLength: 1 example: From Corp to the DMZ network default: type: string title: Default Firewall Action enum: - accept - drop - reject example: drop rules: type: array title: Firewall Rules items: type: object title: Firewall Rule required: - id - action - protocol properties: id: type: integer title: Rule ID example: 10 description: type: string title: Rule Description example: Allow UDP 10.1.26.80 ==> 10.2.25.0/24:123 action: type: string title: Rule Action enum: - accept - drop - reject example: accept protocol: type: string title: Network Protocol enum: - tcp - udp - icmp - esp - ah - all default: tcp example: tcp source: type: object title: Source Address required: - address properties: address: type: string title: IP Address minLength: 1 example: 10.1.24.60 port: type: integer title: Port Number example: 3389 destination: type: object title: Destination Address required: - address properties: address: type: string title: IP Address minLength: 1 example: 10.1.24.60 port: type: integer title: Port Number example: 3389 injections: type: array title: Node File Injections items: type: object title: Node File Injection required: - src - dst properties: src: type: string title: Location of Source File to Inject minLength: 1 example: foo.xml dst: type: string title: Destination Location to Inject File To minLength: 1 example: /etc/phenix/foo.xml description: type: string title: Description of file being injected example: phenix config file permissions: type: string title: Injected file permissions (UNIX style) example: 0664","title":"Node Schema"},{"location":"schema/#interface-schema","text":"iface: type: object required: - name - vlan properties: name: type: string title: Name minLength: 1 example: eth0 vlan: type: string title: VLAN minLength: 1 example: EXP-1 autostart: type: boolean title: Auto Start Interface default: true mac: type: string title: Interface MAC Address example: 00:11:22:33:44:55:66 pattern: '^([0-9a-fA-F]{2}[:-]){5}([0-9a-fA-F]){2}$' mtu: type: integer title: Interface MTU default: 1500 example: 1500 bridge: type: string title: OpenVSwitch Bridge default: phenix driver: type: string title: QEMU network device driver (qemu-kvm -device help) example: e1000 iface_address: type: object required: - address - mask properties: address: type: string format: ipv4 title: IP Address minLength: 7 example: 192.168.1.100 mask: type: integer title: IP Address Netmask minimum: 0 maximum: 32 default: 24 example: 24 gateway: type: string format: ipv4 title: Default Gateway minLength: 7 example: 192.168.1.1 iface_rulesets: type: object properties: ruleset_out: type: string title: Outbound Ruleset example: OutToInet pattern: '^[\\w-]+$' ruleset_in: type: string title: Inbound Ruleset example: InFromInet pattern: '^[\\w-]+$' static_iface: allOf: - $ref: '#/components/schemas/iface' - $ref: '#/components/schemas/iface_address' - $ref: '#/components/schemas/iface_rulesets' required: - type - proto properties: type: type: string title: Interface Type enum: - ethernet default: ethernet example: ethernet proto: type: string title: Interface Protocol enum: - static - ospf default: static example: static dhcp_iface: allOf: - $ref: '#/components/schemas/iface' - $ref: '#/components/schemas/iface_rulesets' required: - type - proto properties: type: type: string title: Interface Type enum: - ethernet default: ethernet example: ethernet proto: type: string title: Interface Protocol enum: - dhcp default: dhcp example: dhcp serial_iface: allOf: - $ref: '#/components/schemas/iface' - $ref: '#/components/schemas/iface_address' - $ref: '#/components/schemas/iface_rulesets' required: - type - proto - udp_port - baud_rate - device properties: type: type: string title: Interface Type enum: - serial default: serial example: serial proto: type: string title: Interface Protocol enum: - static default: static example: static udp_port: type: integer title: UDP Port minimum: 0 maximum: 65535 default: 8989 example: 8989 baud_rate: type: integer title: Serial Baud Rate enum: - 110 - 300 - 600 - 1200 - 2400 - 4800 - 9600 - 14400 - 19200 - 38400 - 57600 - 115200 - 128000 - 256000 default: 9600 example: 9600 device: type: string title: Serial Device minLength: 1 default: /dev/ttyS0 example: /dev/ttyS0 pattern: '^[\\w\\/]+\\w+$'","title":"Interface Schema"},{"location":"schema/#complete-schema","text":"openapi: \"3.0.0\" info: title: phenix version: \"2.0\" paths: {} components: schemas: Image: type: object title: miniccc Image required: - release properties: release: type: string minLength: 1 Topology: type: object title: Demo Topology required: - nodes properties: nodes: type: array title: Nodes items: $ref: \"#/components/schemas/Node\" Scenario: type: object required: - apps properties: apps: type: array items: type: object required: - name properties: name: type: string minLength: 1 assetDir: type: string metadata: type: object additionalProperties: true hosts: type: array items: type: object required: - hostname - metadata properties: hostname: type: string minLength: 1 metadata: type: object additionalProperties: true Experiment: type: object required: - experimentName properties: experimentName: type: string minLength: 1 baseDir: type: string vlans: type: object title: VLANs properties: aliases: type: object title: Aliases additionalProperties: type: integer example: MGMT: 200 min: type: integer max: type: integer schedule: type: object title: Schedule additionalProperties: type: string example: ADServer: compute1 Node: type: object title: Node required: - type - general - hardware properties: type: type: string title: Node Type enum: - Firewall - Printer - Router - Server - Switch - VirtualMachine default: VirtualMachine example: VirtualMachine general: type: object title: General Node Configuration required: - hostname properties: hostname: type: string title: Hostname minLength: 1 example: ADServer description: type: string title: Description example: Active Directory Server vm_type: type: string title: VM (Emulation) Type enum: - kvm - container - \"\" default: kvm example: kvm snapshot: type: boolean title: Snapshot Mode default: false example: false nullable: true do_not_boot: type: boolean title: Do Not Boot VM default: false example: false nullable: true hardware: type: object title: Node Hardware Configuration required: - os_type - drives properties: cpu: type: string title: CPU Emulation enum: - Broadwell - Haswell - core2duo - pentium3 - \"\" default: Broadwell example: Broadwell vcpus: type: integer title: VCPU Count default: 1 example: 4 memory: type: integer title: Memory default: 1024 example: 8192 os_type: type: string title: OS Type enum: - windows - linux - rhel - centos default: linux example: windows drives: type: array title: Drives items: type: object title: Drive required: - image properties: image: type: string title: Image File Name minLength: 1 example: ubuntu.qc2 interface: type: string title: Drive Interface enum: - ahci - ide - scsi - sd - mtd - floppy - pflash - virtio - \"\" default: ide example: ide cache_mode: type: string title: Drive Cache Mode enum: - none - writeback - unsafe - directsync - writethrough - \"\" default: writeback example: writeback inject_partition: type: integer title: Disk Image Partition to Inject Files Into default: 1 example: 2 nullable: true network: type: object title: Node Network Configuration required: - interfaces properties: interfaces: type: array title: Network Interfaces items: type: object title: Network Interface oneOf: - $ref: '#/components/schemas/static_iface' - $ref: '#/components/schemas/dhcp_iface' - $ref: '#/components/schemas/serial_iface' routes: type: array items: type: object title: Network Route required: - destination - next - cost properties: destination: type: string title: Routing Destination minLength: 1 example: 192.168.0.0/24 next: type: string title: Next Hop for Routing Destination minLength: 1 example: 192.168.1.254 cost: type: integer title: Routing Cost (weight) default: 1 example: 1 ospf: type: object title: OSPF Routing Configuration required: - router_id - areas properties: router_id: type: string title: Router ID minLength: 1 example: 0.0.0.1 areas: type: array title: Routing Areas items: type: object title: Routing Area required: - area_id - area_networks properties: area_id: type: integer title: Area ID example: 1 default: 1 area_networks: type: array title: Area Networks items: type: object title: Area Network required: - network properties: network: type: string title: Network minLength: 1 example: 10.1.25.0/24 rulesets: type: array title: Firewall Rulesets items: type: object title: Firewall Ruleset required: - name - default - rules properties: name: type: string title: Ruleset Name minLength: 1 example: OutToDMZ description: type: string title: Ruleset Description minLength: 1 example: From Corp to the DMZ network default: type: string title: Default Firewall Action enum: - accept - drop - reject example: drop rules: type: array title: Firewall Rules items: type: object title: Firewall Rule required: - id - action - protocol properties: id: type: integer title: Rule ID example: 10 description: type: string title: Rule Description example: Allow UDP 10.1.26.80 ==> 10.2.25.0/24:123 action: type: string title: Rule Action enum: - accept - drop - reject example: accept protocol: type: string title: Network Protocol enum: - tcp - udp - icmp - esp - ah - all default: tcp example: tcp source: type: object title: Source Address required: - address properties: address: type: string title: IP Address minLength: 1 example: 10.1.24.60 port: type: integer title: Port Number example: 3389 destination: type: object title: Destination Address required: - address properties: address: type: string title: IP Address minLength: 1 example: 10.1.24.60 port: type: integer title: Port Number example: 3389 injections: type: array title: Node File Injections items: type: object title: Node File Injection required: - src - dst properties: src: type: string title: Location of Source File to Inject minLength: 1 example: foo.xml dst: type: string title: Destination Location to Inject File To minLength: 1 example: /etc/phenix/foo.xml description: type: string title: Description of file being injected example: phenix config file permissions: type: string title: Injected file permissions (UNIX style) example: 0664 iface: type: object required: - name - vlan properties: name: type: string title: Name minLength: 1 example: eth0 vlan: type: string title: VLAN minLength: 1 example: EXP-1 autostart: type: boolean title: Auto Start Interface default: true mac: type: string title: Interface MAC Address example: 00:11:22:33:44:55:66 pattern: '^([0-9a-fA-F]{2}[:-]){5}([0-9a-fA-F]){2}$' mtu: type: integer title: Interface MTU default: 1500 example: 1500 bridge: type: string title: OpenVSwitch Bridge default: phenix iface_address: type: object required: - address - mask properties: address: type: string format: ipv4 title: IP Address minLength: 7 example: 192.168.1.100 mask: type: integer title: IP Address Netmask minimum: 0 maximum: 32 default: 24 example: 24 gateway: type: string format: ipv4 title: Default Gateway minLength: 7 example: 192.168.1.1 iface_rulesets: type: object properties: ruleset_out: type: string title: Outbound Ruleset example: OutToInet pattern: '^[\\w-]+$' ruleset_in: type: string title: Inbound Ruleset example: InFromInet pattern: '^[\\w-]+$' static_iface: allOf: - $ref: '#/components/schemas/iface' - $ref: '#/components/schemas/iface_address' - $ref: '#/components/schemas/iface_rulesets' required: - type - proto properties: type: type: string title: Interface Type enum: - ethernet default: ethernet example: ethernet proto: type: string title: Interface Protocol enum: - static - ospf default: static example: static dhcp_iface: allOf: - $ref: '#/components/schemas/iface' - $ref: '#/components/schemas/iface_rulesets' required: - type - proto properties: type: type: string title: Interface Type enum: - ethernet default: ethernet example: ethernet proto: type: string title: Interface Protocol enum: - dhcp default: dhcp example: dhcp serial_iface: allOf: - $ref: '#/components/schemas/iface' - $ref: '#/components/schemas/iface_address' - $ref: '#/components/schemas/iface_rulesets' required: - type - proto - udp_port - baud_rate - device properties: type: type: string title: Interface Type enum: - serial default: serial example: serial proto: type: string title: Interface Protocol enum: - static default: static example: static udp_port: type: integer title: UDP Port minimum: 0 maximum: 65535 default: 8989 example: 8989 baud_rate: type: integer title: Serial Baud Rate enum: - 110 - 300 - 600 - 1200 - 2400 - 4800 - 9600 - 14400 - 19200 - 38400 - 57600 - 115200 - 128000 - 256000 default: 9600 example: 9600 device: type: string title: Serial Device minLength: 1 default: /dev/ttyS0 example: /dev/ttyS0 pattern: '^[\\w\\/]+\\w+$'","title":"Complete Schema"},{"location":"scorch/","text":"Scorch \u00b6 Scorch \u2014 SC enario ORCH estration \u2014 is an automated scenario orchestration framework within phenix. It is included in phenix as a core app . The development of the Scorch framework was motivated by the need to facilitate rigorous experimentation. Some advantages of Scorch include the ability to run many repeated scenarios on an experiment with consistency and minimal overhead. Scorch also provides the ability to efficiently capture experimental data for retrieval and analysis. A phenix scenario configuration file is used to define and configure the Scorch app for use on a topology. The Scorch app is meant to allow for the staging of Scorch components in sequence to execute against a running experiment. When applied to a given topology, the Scorch app will be available in the Scorch table to execute and then observe, manipulate in some cases, and review output from available components for a given stage in the Scorch pipeline . The screenshots and configuration file in the rest of this document are from an example Scorch app, scorch-demo . Scorch Components \u00b6 A Scorch component is simply an executable available to be called by the Scorch app within phenix. A component is expected to implement any or all of the various stages in the Scorch pipeline . For an executable to be considered a Scorch component, it must meet the following requirements: Follow the phenix-scorch-component-<type> naming convention, where <type> is the component type used in the Scorch app configuration. An example would be phenix-scorch-component-tcpdump . Be an executable file. Be in the PATH of the user running phenix. When the Scorch app executes a Scorch component, it will pass a number of positional arguments to the component via the command line, as well as the JSON representation of the experiment the component is to be executed against via STDIN. The positional arguments passed are as follows: run stage ( configure , start , stop , or cleanup ) component name (name given to component type in Scorch app configuration) run ID (integer >= 0 representing the array index of the Scorch run in the app configuration being executed) loop (integer >= 0 representing the current run loop being executed) count (integer >= 0 representing the current loop count being executed) During component execution, the Scorch app assumes anything written to STDOUT by a component is intended to be relayed to the user. Thus, when Scorch is run via the web UI, anything written to STDOUT gets streamed to the UI for viewing. Any error messages generated by a component should be written to the log file or to STDERR unless it's to also be relayed to the user directly. The Scorch app expects a component executable to exit with a value of 0 upon completion if the component was successful, and exit with any other value otherwise. An exit value of anything other than 0 will result in Scorch halting execution of the current stage and jumping to the next appropriate stage to complete the Scorch run. Automated Component Data Collection \u00b6 The Scorch app is capable of generating a configuration file for and starting an instance of Filebeat in the background before execution of each Scorch run. As each Scorch component is executed, any data it generates and collects can be configured to be automatically processed by Filebeat for indexing in Elasticsearch . At a minimum, this requires the following. Filebeat to be enabled and configured in the Scorch app configuration. A Filebeat input to be configured for each component generating and collecting data. The filebeat executable installed and in the PATH of the user running phenix. See the example configuration below for examples of how Filebeat and Filebeat inputs are configured in the Scorch app configuration. Built-in Components \u00b6 The following Scorch component types are considered core components, in that they are included in the main phenix repository and are available for use in Scorch app configurations by default. break pause tap break Component \u00b6 The break component is comparable to a source code break point when debugging an application in that it pauses execution of the current Scorch run until a user exits the break. While the break component is running, users have access to a shell on the server running phenix as the user running phenix. The first user to access the shell via the terminal modal in the UI will have read-write access. If other users access the shell, they will have read-only access but will get live updates as the user with read-write access uses the terminal. It's possible to configure the break component in the Scorch app configuration to create a minimega tap when the component is executed. When the component is executed, the tap will be deleted. In addition to the tap, external network access can also be configured (e.g., Internet access). An example of configuring a break component to create a tap and configure external network access during the configure stage is as follows. The break component can be configured to run in any stage. spec: apps: - name: scorch metadata: components: - name: break-tap type: break metadata: tap: bridge: phenix vlan: MGMT ip: 172.16.33.25/16 internetAccess: true runs: - configure: [\"break-tap\"] pause Component \u00b6 The pause component is similar to the break component in that it pauses execution of the current Scorch run, but instead of waiting for user intervention it simply pauses for a predefined duration. A simple example is as follows. The pause component can be configured to run in any stage. The value used for the duration key should be a valid Golang duration string . spec: apps: - name: scorch metadata: components: - name: brief-pause type: pause metadata: duration: 2s runs: - start: [\"brief-pause\"] tap Component \u00b6 The tap component implements the exact same functionality described above in the break component for creating a minimega tap and, optionally, external network access, but allows for the tap (and external network access, if configured) to exist while other components are executed (as opposed to only existing for the duration of the break component). An example of configuring a tap component to create a tap and configure external network access is as follows. The tap component can only be configured to run in the start stage (create the tap) and the stop stage (delete the tap). spec: apps: - name: scorch metadata: components: - name: tap-inet type: tap metadata: bridge: phenix vlan: MGMT ip: 172.16.33.25/16 internetAccess: true runs: - start: [\"tap-inet\"] - stop: [\"tap-inet\"] NOTE: In deployments where minimega is running in a container on the headnode, and Docker networking is in use (e.g., the minimega container is not configured to use host networking), users will need to execute the following commands if access to the minimega tap created by the tap (or break ) component from the Docker host is required. ovs-docker add-port phenix TDN minimega docker exec -it minimega ovs-vsctl add-port phenix TDN ovs-vsctl add-port phenix temp-tap tag=<vlan ID> -- set interface temp-tap type=internal The above commands assume the name of the minimega container is minimega . The name of the local tap created (in this case, temp-tap ) can be whatever, but the value for the VLAN tag must match the numerical ID of the VLAN that's mapped to the VLAN alias used in the tap (or break ) component configuration. Using host networking mode for the minimega container allows for all the above nonsense to be skipped. User-defined Components \u00b6 The following Scorch component types have been developed external to the main phenix repository and are available in the phenix-apps repository, which also includes README-based documentation for each. They are all developed in Python, and leverage common helper classes that ease the development of user components. art cc ettercap hoststats snort tcpdump vmstats Scorch Table \u00b6 The Scorch table, accessible as one of the tab selections within the phenix UI, lists all possible Scorch apps available based on the experiments established in phenix. The following columns or functions are available: Experiment name Experiment status: this reports on the status of the experiment \u2014 an experiment must be running for a Scorch app to start Scorch app status: this will report the running or stopped status of the Scorch app itself Terminal: if the Scorch app has reached a break point, a terminal will be available \u2014 if clicked, a terminal dialog will be opened and is running on the phenix host system Find an Experiment: similar to the search fields in other tables within the phenix UI, it is possible to filter experiment names based on terms entered here Scorch Pipeline \u00b6 Scorch pipelines are available on the Scorch table in the phenix UI. The table is sorted by Experiment name by default. Only those experiments with the Scorch app configured in the scenario configuration will be listed in the table. It is possible to start or stop an experiment, as well as start or stop a Scorch component. Finally, if a terminal is available when a break point is reached in a running Scorch app, it can be accessed from the table. The Scorch pipeline provides a graphical representation of the Scorch app, including the configure, start, stop, and cleanup stages. If the Scorch app provides output for a given step, or component, users can click into the component and receive the output. A user can access the terminal if a break point is reached by clicking on the component. As with the terminal access described above, a dialog will be presented with a terminal running on the phenix host system. The following functions are also available in the Scorch pipeline UI: Return to the table: a button that will return to the Scorch table Scorch app status: a button that will allow a running Scorch app to be stopped or started depending on the current status Stages \u00b6 For a given Scorch pipeline, there are four stages of execution: configure start stop cleanup A Scorch component may implement any or all of the various stages, and Scorch will execute each stage inside the components in order. Each component can be configured in the Scorch app scenario configuration file . * There is additional done stage in the UI obtained when cleanup has been completed. It is meant to report the completion of all stages in the Pipeline UI. It's completely up to the component developer if and how an execution stage is implemented and handled by the component. If a component is configured in the Scorch app to be executed as part of a stage, but the component does not implement said stage, then the Scorch app will happily continue on to the next component in the stage (unless the component errors out if the current stage is not implemented, in which case the Scorch run will fail). The following indicators are presented for each component of the Scorch app: Uninitialized: a component has not yet been reached or initialized \u2014 if the component has not yet been run, all components will be identified as uninitialized Running: the component is currently running and has not yet been completed Success: the component has completed successfully Break Point: a break component has been reached \u2014 a terminal should be accessible by clicking on the component Backgrounded: a component is running in the background Failure: a component has failed for some reason \u2014 reporting on the failure may be accessible by click on the component Loops are also available within a Scorch app and are included in the configuration file. A loop supports looping within a given component through additional components. Once a loop is completed, the next component is executed. There is no limit on the number of loops or depth of them. The Scorch pipeline UI supports access to each loop and will report the depth in the pipeline's title. The first loop in the scorch-demo app is presented in the following example; clicking the return button will return the display one level up in the loop chain (or ending at the parent Scorch app). In addition to loops, multi-run is supported within a Scorch app. Unlike loops, multi-run allows for individual Scorch runs containing four separate stages. They are not nested in each other but are independent runs. A use case for multi-run could be executing multiple independent portions of an experiment against a topology, in any order or executing a run multiple times. There are two runs depicted in the configuration below. As described above, each component will provide a modal for output reporting. The output could either be a fairly straight forward report, nothing at all, or it include logging output for a given component in the stage. The result will be streamed as it is received if the component is currently in a running component. If the component has finished running, the output will be static. A terminal modal is available from the Scorch table when a break point component is reached; it is also available in the Scorch pipeline. There are two types of terminal modals: read-write and read-only. If another user opens a terminal modal for a given component, it will be read-only the next time a terminal is open. The following are examples of each. In the first example, a read-only terminal, the user viewing this modal is only observing what another using is executing. In the second example, a read-write terminal, the user ran two simple commands on the phenix host system. If a component fails, the Scorch app will skip the remaining components involved in the current execution stage and jump to the next appropriate stage to complete the Scorch run. For example, if a component fails in the Configure stage the Scorch app will jump to the Cleanup stage, and if a component fails in the Start stage Scorch will jump to the Stop stage (skipping execution of any configured loops in either case). Example Configuration \u00b6 apiVersion: phenix.sandia.gov/v2 kind: Scenario metadata: name: scorch-demo spec: scenario: apps: - name: mirror metadata: directGRE: enabled: true mirrorBridge: phenix mirrorNet: 172.30.0.0/16 mirrorVLAN: mirror hosts: - hostname: detector metadata: interface: IF0 vlans: - EXP - name: scorch metadata: components: - name: vmstats type: vmstats metadata: filebeat.inputs: - enabled: true type: log json.add_error_key: true paths: - vm_stats.jsonl processors: - copy_fields: fields: - from: json to: scorch.vmstats - drop_fields: fields: - json - timestamp: field: scorch.vmstats.UTC layouts: - '2006-01-02 15:04:05' - name: hoststats type: hoststats background: true metadata: filebeat.inputs: - enabled: true type: log json.add_error_key: true paths: - host_stats.jsonl processors: - copy_fields: fields: - from: json to: scorch.hoststats - drop_fields: fields: - json - timestamp: field: scorch.hoststats.timestamp layouts: - UNIX_MS - name: trafficgen type: trafficgen metadata: scripts: backgroundGen: /phenix/topologies/scorch-demo/scripts/background-gen.py malwareGen: /phenix/topologies/scorch-demo/scripts/malware-gen.py trafficServer: /phenix/topologies/scorch-demo/scripts/traffic-server.py targets: - backgroundClient: hostname: background-gen probability: 0.01 rate: 10000 duration: 30 hostname: traffic-server interface: IF0 malwareClient: hostname: malware-gen probability: 1.25 rate: 20 - name: break type: break metadata: {} - name: tcpdump type: tcpdump metadata: convertToJSON: false filebeat.inputs: - enabled: true type: log paths: - tcpdump.pcap.json processors: - copy_fields: fields: - from: json to: scorch.tcpdump - drop_fields: fields: - json vms: detector: eth0 - name: snort type: snort metadata: configs: - dst: /etc/snort/snort.conf name: snort src: /phenix/topologies/scorch-demo/configs/snort.conf - dst: /etc/snort/rules/emotet.rules name: emotet src: /phenix/topologies/scorch-demo/configs/emotet.rules filebeat.inputs: - enabled: true type: log json.add_error_key: true paths: - snort-stats.jsonl processors: - copy_fields: fields: - from: json to: scorch.snort - drop_fields: fields: - json - timestamp: field: scorch.snort.timestamp layouts: - UNIX hostname: detector scripts: configSnort: executor: bash script: /phenix/topologies/scorch-demo/scripts/configure-snort.sh sniffInterface: eth0 waitDuration: 5 runs: - configure: - trafficgen - snort start: - hoststats - vmstats loop: execute: configure: null start: - tcpdump - snort - trafficgen stop: - trafficgen - snort - tcpdump cleanup: null stop: - vmstats - hoststats - break - start: - tcpdump - trafficgen stop: - trafficgen - tcpdump filebeat: enabled: true config: output.elasticsearch: hosts: - es:9200 setup.dashboards.enabled: true setup.kibana.host: http://kibana:5601","title":"Scorch"},{"location":"scorch/#scorch","text":"Scorch \u2014 SC enario ORCH estration \u2014 is an automated scenario orchestration framework within phenix. It is included in phenix as a core app . The development of the Scorch framework was motivated by the need to facilitate rigorous experimentation. Some advantages of Scorch include the ability to run many repeated scenarios on an experiment with consistency and minimal overhead. Scorch also provides the ability to efficiently capture experimental data for retrieval and analysis. A phenix scenario configuration file is used to define and configure the Scorch app for use on a topology. The Scorch app is meant to allow for the staging of Scorch components in sequence to execute against a running experiment. When applied to a given topology, the Scorch app will be available in the Scorch table to execute and then observe, manipulate in some cases, and review output from available components for a given stage in the Scorch pipeline . The screenshots and configuration file in the rest of this document are from an example Scorch app, scorch-demo .","title":"Scorch"},{"location":"scorch/#scorch-components","text":"A Scorch component is simply an executable available to be called by the Scorch app within phenix. A component is expected to implement any or all of the various stages in the Scorch pipeline . For an executable to be considered a Scorch component, it must meet the following requirements: Follow the phenix-scorch-component-<type> naming convention, where <type> is the component type used in the Scorch app configuration. An example would be phenix-scorch-component-tcpdump . Be an executable file. Be in the PATH of the user running phenix. When the Scorch app executes a Scorch component, it will pass a number of positional arguments to the component via the command line, as well as the JSON representation of the experiment the component is to be executed against via STDIN. The positional arguments passed are as follows: run stage ( configure , start , stop , or cleanup ) component name (name given to component type in Scorch app configuration) run ID (integer >= 0 representing the array index of the Scorch run in the app configuration being executed) loop (integer >= 0 representing the current run loop being executed) count (integer >= 0 representing the current loop count being executed) During component execution, the Scorch app assumes anything written to STDOUT by a component is intended to be relayed to the user. Thus, when Scorch is run via the web UI, anything written to STDOUT gets streamed to the UI for viewing. Any error messages generated by a component should be written to the log file or to STDERR unless it's to also be relayed to the user directly. The Scorch app expects a component executable to exit with a value of 0 upon completion if the component was successful, and exit with any other value otherwise. An exit value of anything other than 0 will result in Scorch halting execution of the current stage and jumping to the next appropriate stage to complete the Scorch run.","title":"Scorch Components"},{"location":"scorch/#automated-component-data-collection","text":"The Scorch app is capable of generating a configuration file for and starting an instance of Filebeat in the background before execution of each Scorch run. As each Scorch component is executed, any data it generates and collects can be configured to be automatically processed by Filebeat for indexing in Elasticsearch . At a minimum, this requires the following. Filebeat to be enabled and configured in the Scorch app configuration. A Filebeat input to be configured for each component generating and collecting data. The filebeat executable installed and in the PATH of the user running phenix. See the example configuration below for examples of how Filebeat and Filebeat inputs are configured in the Scorch app configuration.","title":"Automated Component Data Collection"},{"location":"scorch/#built-in-components","text":"The following Scorch component types are considered core components, in that they are included in the main phenix repository and are available for use in Scorch app configurations by default. break pause tap","title":"Built-in Components"},{"location":"scorch/#break-component","text":"The break component is comparable to a source code break point when debugging an application in that it pauses execution of the current Scorch run until a user exits the break. While the break component is running, users have access to a shell on the server running phenix as the user running phenix. The first user to access the shell via the terminal modal in the UI will have read-write access. If other users access the shell, they will have read-only access but will get live updates as the user with read-write access uses the terminal. It's possible to configure the break component in the Scorch app configuration to create a minimega tap when the component is executed. When the component is executed, the tap will be deleted. In addition to the tap, external network access can also be configured (e.g., Internet access). An example of configuring a break component to create a tap and configure external network access during the configure stage is as follows. The break component can be configured to run in any stage. spec: apps: - name: scorch metadata: components: - name: break-tap type: break metadata: tap: bridge: phenix vlan: MGMT ip: 172.16.33.25/16 internetAccess: true runs: - configure: [\"break-tap\"]","title":"break Component"},{"location":"scorch/#pause-component","text":"The pause component is similar to the break component in that it pauses execution of the current Scorch run, but instead of waiting for user intervention it simply pauses for a predefined duration. A simple example is as follows. The pause component can be configured to run in any stage. The value used for the duration key should be a valid Golang duration string . spec: apps: - name: scorch metadata: components: - name: brief-pause type: pause metadata: duration: 2s runs: - start: [\"brief-pause\"]","title":"pause Component"},{"location":"scorch/#tap-component","text":"The tap component implements the exact same functionality described above in the break component for creating a minimega tap and, optionally, external network access, but allows for the tap (and external network access, if configured) to exist while other components are executed (as opposed to only existing for the duration of the break component). An example of configuring a tap component to create a tap and configure external network access is as follows. The tap component can only be configured to run in the start stage (create the tap) and the stop stage (delete the tap). spec: apps: - name: scorch metadata: components: - name: tap-inet type: tap metadata: bridge: phenix vlan: MGMT ip: 172.16.33.25/16 internetAccess: true runs: - start: [\"tap-inet\"] - stop: [\"tap-inet\"] NOTE: In deployments where minimega is running in a container on the headnode, and Docker networking is in use (e.g., the minimega container is not configured to use host networking), users will need to execute the following commands if access to the minimega tap created by the tap (or break ) component from the Docker host is required. ovs-docker add-port phenix TDN minimega docker exec -it minimega ovs-vsctl add-port phenix TDN ovs-vsctl add-port phenix temp-tap tag=<vlan ID> -- set interface temp-tap type=internal The above commands assume the name of the minimega container is minimega . The name of the local tap created (in this case, temp-tap ) can be whatever, but the value for the VLAN tag must match the numerical ID of the VLAN that's mapped to the VLAN alias used in the tap (or break ) component configuration. Using host networking mode for the minimega container allows for all the above nonsense to be skipped.","title":"tap Component"},{"location":"scorch/#user-defined-components","text":"The following Scorch component types have been developed external to the main phenix repository and are available in the phenix-apps repository, which also includes README-based documentation for each. They are all developed in Python, and leverage common helper classes that ease the development of user components. art cc ettercap hoststats snort tcpdump vmstats","title":"User-defined Components"},{"location":"scorch/#scorch-table","text":"The Scorch table, accessible as one of the tab selections within the phenix UI, lists all possible Scorch apps available based on the experiments established in phenix. The following columns or functions are available: Experiment name Experiment status: this reports on the status of the experiment \u2014 an experiment must be running for a Scorch app to start Scorch app status: this will report the running or stopped status of the Scorch app itself Terminal: if the Scorch app has reached a break point, a terminal will be available \u2014 if clicked, a terminal dialog will be opened and is running on the phenix host system Find an Experiment: similar to the search fields in other tables within the phenix UI, it is possible to filter experiment names based on terms entered here","title":"Scorch Table"},{"location":"scorch/#scorch-pipeline","text":"Scorch pipelines are available on the Scorch table in the phenix UI. The table is sorted by Experiment name by default. Only those experiments with the Scorch app configured in the scenario configuration will be listed in the table. It is possible to start or stop an experiment, as well as start or stop a Scorch component. Finally, if a terminal is available when a break point is reached in a running Scorch app, it can be accessed from the table. The Scorch pipeline provides a graphical representation of the Scorch app, including the configure, start, stop, and cleanup stages. If the Scorch app provides output for a given step, or component, users can click into the component and receive the output. A user can access the terminal if a break point is reached by clicking on the component. As with the terminal access described above, a dialog will be presented with a terminal running on the phenix host system. The following functions are also available in the Scorch pipeline UI: Return to the table: a button that will return to the Scorch table Scorch app status: a button that will allow a running Scorch app to be stopped or started depending on the current status","title":"Scorch Pipeline"},{"location":"scorch/#stages","text":"For a given Scorch pipeline, there are four stages of execution: configure start stop cleanup A Scorch component may implement any or all of the various stages, and Scorch will execute each stage inside the components in order. Each component can be configured in the Scorch app scenario configuration file . * There is additional done stage in the UI obtained when cleanup has been completed. It is meant to report the completion of all stages in the Pipeline UI. It's completely up to the component developer if and how an execution stage is implemented and handled by the component. If a component is configured in the Scorch app to be executed as part of a stage, but the component does not implement said stage, then the Scorch app will happily continue on to the next component in the stage (unless the component errors out if the current stage is not implemented, in which case the Scorch run will fail). The following indicators are presented for each component of the Scorch app: Uninitialized: a component has not yet been reached or initialized \u2014 if the component has not yet been run, all components will be identified as uninitialized Running: the component is currently running and has not yet been completed Success: the component has completed successfully Break Point: a break component has been reached \u2014 a terminal should be accessible by clicking on the component Backgrounded: a component is running in the background Failure: a component has failed for some reason \u2014 reporting on the failure may be accessible by click on the component Loops are also available within a Scorch app and are included in the configuration file. A loop supports looping within a given component through additional components. Once a loop is completed, the next component is executed. There is no limit on the number of loops or depth of them. The Scorch pipeline UI supports access to each loop and will report the depth in the pipeline's title. The first loop in the scorch-demo app is presented in the following example; clicking the return button will return the display one level up in the loop chain (or ending at the parent Scorch app). In addition to loops, multi-run is supported within a Scorch app. Unlike loops, multi-run allows for individual Scorch runs containing four separate stages. They are not nested in each other but are independent runs. A use case for multi-run could be executing multiple independent portions of an experiment against a topology, in any order or executing a run multiple times. There are two runs depicted in the configuration below. As described above, each component will provide a modal for output reporting. The output could either be a fairly straight forward report, nothing at all, or it include logging output for a given component in the stage. The result will be streamed as it is received if the component is currently in a running component. If the component has finished running, the output will be static. A terminal modal is available from the Scorch table when a break point component is reached; it is also available in the Scorch pipeline. There are two types of terminal modals: read-write and read-only. If another user opens a terminal modal for a given component, it will be read-only the next time a terminal is open. The following are examples of each. In the first example, a read-only terminal, the user viewing this modal is only observing what another using is executing. In the second example, a read-write terminal, the user ran two simple commands on the phenix host system. If a component fails, the Scorch app will skip the remaining components involved in the current execution stage and jump to the next appropriate stage to complete the Scorch run. For example, if a component fails in the Configure stage the Scorch app will jump to the Cleanup stage, and if a component fails in the Start stage Scorch will jump to the Stop stage (skipping execution of any configured loops in either case).","title":"Stages"},{"location":"scorch/#example-configuration","text":"apiVersion: phenix.sandia.gov/v2 kind: Scenario metadata: name: scorch-demo spec: scenario: apps: - name: mirror metadata: directGRE: enabled: true mirrorBridge: phenix mirrorNet: 172.30.0.0/16 mirrorVLAN: mirror hosts: - hostname: detector metadata: interface: IF0 vlans: - EXP - name: scorch metadata: components: - name: vmstats type: vmstats metadata: filebeat.inputs: - enabled: true type: log json.add_error_key: true paths: - vm_stats.jsonl processors: - copy_fields: fields: - from: json to: scorch.vmstats - drop_fields: fields: - json - timestamp: field: scorch.vmstats.UTC layouts: - '2006-01-02 15:04:05' - name: hoststats type: hoststats background: true metadata: filebeat.inputs: - enabled: true type: log json.add_error_key: true paths: - host_stats.jsonl processors: - copy_fields: fields: - from: json to: scorch.hoststats - drop_fields: fields: - json - timestamp: field: scorch.hoststats.timestamp layouts: - UNIX_MS - name: trafficgen type: trafficgen metadata: scripts: backgroundGen: /phenix/topologies/scorch-demo/scripts/background-gen.py malwareGen: /phenix/topologies/scorch-demo/scripts/malware-gen.py trafficServer: /phenix/topologies/scorch-demo/scripts/traffic-server.py targets: - backgroundClient: hostname: background-gen probability: 0.01 rate: 10000 duration: 30 hostname: traffic-server interface: IF0 malwareClient: hostname: malware-gen probability: 1.25 rate: 20 - name: break type: break metadata: {} - name: tcpdump type: tcpdump metadata: convertToJSON: false filebeat.inputs: - enabled: true type: log paths: - tcpdump.pcap.json processors: - copy_fields: fields: - from: json to: scorch.tcpdump - drop_fields: fields: - json vms: detector: eth0 - name: snort type: snort metadata: configs: - dst: /etc/snort/snort.conf name: snort src: /phenix/topologies/scorch-demo/configs/snort.conf - dst: /etc/snort/rules/emotet.rules name: emotet src: /phenix/topologies/scorch-demo/configs/emotet.rules filebeat.inputs: - enabled: true type: log json.add_error_key: true paths: - snort-stats.jsonl processors: - copy_fields: fields: - from: json to: scorch.snort - drop_fields: fields: - json - timestamp: field: scorch.snort.timestamp layouts: - UNIX hostname: detector scripts: configSnort: executor: bash script: /phenix/topologies/scorch-demo/scripts/configure-snort.sh sniffInterface: eth0 waitDuration: 5 runs: - configure: - trafficgen - snort start: - hoststats - vmstats loop: execute: configure: null start: - tcpdump - snort - trafficgen stop: - trafficgen - snort - tcpdump cleanup: null stop: - vmstats - hoststats - break - start: - tcpdump - trafficgen stop: - trafficgen - tcpdump filebeat: enabled: true config: output.elasticsearch: hosts: - es:9200 setup.dashboards.enabled: true setup.kibana.host: http://kibana:5601","title":"Example Configuration"},{"location":"state-of-health/","text":"State of Health \u00b6 State of Health (SoH) is a core (but not default) app meant to assist with understanding the state of a running experiment. It has many configuration options , and relies on minimega's command and control infrastructure (ie. the miniccc agent running in experiment VMs) to drive and collect the experiment health state data. See the command and control section for more details. SoH information is presented in the UI in three separate tabs. You can access the information by clicking the SoH button from the Experiments or Running Experiment components. Button available on the experiments table. Button available in running experiment. Topology Graph \u00b6 This tab displays a network graph of the running experiment. The color of the node is based on the condition of the corresponding VM and could be: Running Not running (part of the experiment, but currently paused) Not booted (part of the experiment, but marked as Do Not Boot) Not deployed (part of the experiment, but flushed from minimega) Experiment stopped It is also possible to filter the graph based on nodes that are either: Running , Not running , Not booted , or Not deployed . The Refresh Network button will reset the filter, showing all nodes. The Manual Refresh button will request the latest server-side SoH data and update the three tabs. Hovering over a node in the graph will cause it to expand to show what operating system the node is running. An orange border around a node is indicative of the node currently having health issues (e.g., expected processes not running or not being able to reach other nodes on the network). Other virtual systems will also be represented accordingly (e.g., a printer or firewall). Clicking on the node with an orange border will produce a details modal with the current error reporting. If there are no errors to report and command and control is enabled, the details modal will only show the current CPU load. Note the green button in the lower right corner of the modal; this will provide access to the VNC for any running VM. When the VM is not running, access to the VNC will be disabled. Finally, if there is no SoH information to report on a given VM, it will be noted in the details modal. The following screenshot is an example of no SoH information with the VNC button disabled. Network Volume \u00b6 This tab displays a chord graph that shows network flows between nodes using flows from PacketBeat fed to ElasticSearch; connections represent the volume of traffic between nodes. Tip This information is only available if the packet capture option is enabled for SoH. This is an example chord graph of a Protonuke server with two clients; one of which is requesting content at a delayed interval (therefore less traffic flow). If you hover over traffic flow, a tooltip will appear providing additional details. (In this screenshot, the mouse is hovering over the traffic for IP 192.168.100.1 .) SoH Messages \u00b6 The final tab will display any error messages from the SoH agents. Configuration \u00b6 Just like any other app, the SoH app is configured via the Scenario configuration. SoH will only be enabled for an experiment if it's present in the scenario as soh , and has multiple configuration options available, described in detail below. Sample SoH Scenario Config \u00b6 spec: apps: - name: soh metadata: appMetadataProfileKey: sohProfile # metadata key to look for in other apps c2Timeout: 5m exitOnError: false hostCustomTests: host-00: - name: FooBarTest testScript: | cat /etc/passwd | grep root testStdout: root executor: bash host-01: - name: SuckaTest.ps1 testScript: | Get-Process miniccc -ErrorAction SilentlyContinue testStdout: miniccc executor: powershell -NoProfile -ExecutionPolicy bypass -File hostListeners: client: - :502 server: - :80 - :443 hostProcesses: client: - miniccc server: - miniccc hostsToUseUUIDForC2Active: - host-02 injectICMPAllow: true packetCapture: elasticImage: /phenix/images/elasticsearch.qc2 packetBeatImage: /phenix/images/packetbeat.qc2 elasticServer: hostname: soh-elasticsearch-server vcpus: 4 memory: 4096 ipAddress: 172.16.200.1/16 vlan: MGMT captureHosts: client: - IF0 # interface to monitor on \"client\" node in topology server: - IF0 # interface to monitor on \"server\" node in topology skipInitialNetworkConfigTests: false # if true, testReachability will be off skipHosts: - kali.qc2 # can be an image name, in which case any host using image will be skipped - foobar-host # can be hostname from topology testReachability: full # can be off, sample, or full testCustomReachability: - src: host-00 dst: host-01|IF0 proto: tcp port: 22 wait: 30s - src: host-01 dst: host-00|IF0 proto: tcp port: 22 wait: 30s Configuration Options \u00b6 appMetadataProfileKey : since the listeners and processes one might want to monitor could be highly dependent on other apps that are configured for an experiment, it's possible to specify the hostListeners and hostProcesses to be monitored in the other apps themselves under the key specified here. The default key is sohProfile . c2Timeout : a Golang duration string specifying how long to wait for the miniccc C2 client to become active in a VM before moving on and marking the VM as not to be monitored. The default is 5m . exitOnError : a boolean representing whether the app should cause the entire experiment deployment to fail if it has any errors. The default is false . hostCustomTests : If present, a map of custom tests to run on the given hosts. name : name of test. Used as script name to be sent to the host. testScript : the actual script (can be multiple lines) to be executed using the specified executor . executor : the application to execute the testScript with (e.g. bash , powershell ). testStdout : a string to look for in STDOUT from the executed script. If found, the test passes. If not found, it fails. testStderr : a string to look for in STDERR from the executed script. If found, the test passes. If not found, it fails. hostListeners : a map of VMs, each specifying a list of listening ports to check for within the VM. If the port can be listening on any interface, the form :80 can be used. If the port should be listening on a specific interface, the form 192.168.1.10:80 can be used. The default is nil . hostProcesses : a map of VMs, each specifying a list of process names to check for within the VM. The default is nil . hostsToUseUUIDForC2Active : a list of topology hostnames to use the minimega VM UUID for when determining if their cc agent is active. This is useful for topology nodes that are configured with snapshots disabled, preventing their hostname from getting updated when booted. Note that this configuration option also supports being set to all (e.g., hostsToUseUUIDForC2Active: all ) if a user wishes to use the minimega VM UUID for all topology nodes. injectICMPAllow : a boolean representing whether any existing firewall/router rulesets should have a rule added to allow ICMP between all nodes to facilitate reachability testing. See Injecting ICMP Rules for more details. If reachability tests are disabled, then this will be too, regardless of its setting here. The default is false . packetCapture : if present, a partially hidden packet capture infrastructure based on Elastic, Kibana, and PacketBeat will be deployed for the experiment. See Packet Capture for more details. The default is nil . elasticImage : path to the disk image to use for the Elastic/Kibana VM for packet capture. An image/elasticsearch config comes bundled with phenix and can be used to build an image to use here. There is no default for this setting; if packet capture is to be deployed it must be provided. packetBeatImage : path to the disk image to use for the PacketBeat VM for packet capture. An image/packetbeat config comes bundled with phenix and can be used to build an image to use here. There is no default for this setting; if packet capture is to be deployed it must be provided. elasticServer : hostname : the hostname to use for the Elastic/Kibana server added to the experiment topology. There is no default for this setting; if packet capture is to be deployed it must be provided. vcpus : the number of CPUs to assign to the Elastic/Kibana server VM. The default is 4. memory : the amount of memory to assign to the Elastic/Kibana server VM. The default is 4096. ipAddress : the IP address to use for the Elastic/Kibana server added to the experiment topology. The network interface this IP address is used for will be added to the experiment VLAN specified by vlan . The IP address should be specified in CIDR notation. There should also be sufficient IP addresses after the one specified here to be assigned to each of the PacketBeat monitor VMs that will be deployed, as the IP addresses assigned to them on the VLAN specified by vlan will increment up from this IP. There is no default for this setting; if packet capture is to be deployed it must be provided. vlan : the experiment VLAN to add the Elastic/Kibana and PacketBeat VMs to. There is no default for this setting; if packet capture is to be deployed it must be provided. captureHosts : a map of VMs, each specifying a list of network interface names to monitor. One PacketBeat VM will be deployed for each VM interface specified. The defalt is nil . skipInitialNetworkConfigTests : by default, a set of tests will be run on each VM to ensure the VM was assigned the correct IP address and can reach its default gateway (if specified). Setting this to true will skip these initial tests, but will also disable reachability testing. The default is false . skipHosts : a list of VM hostnames and/or disk image names to skip health monitoring for. For disk image names, any host using the disk image as its primary image will be skipped. The default is nil . testReachability : reachability testing is the process of making sure each VM can reach other VMs within the experiment over the network. See Network Reachability for more details. There are three options for this setting: off, sample, full . off : reachability testing is disabled. This is the default. sample : each VM in the experiment will attempt to ping a random VM in every other experiment VLAN. full : each VM in the experiment will attempt to ping every other VM in every other experiment VLAN. testCustomReachability : if present, a list of custom reachability test settings. src : hostname to conduct test from. dst : hostname and interface name (e.g. host-01|IF0 ) to conduct test to. proto : protocol to use for test. Currently the options are tcp and udp . If udp is used, the udpPacketBase64 setting must be provided. port : destination port to conduct test to. wait : amount of time to wait for a response from the destination. If not provided, the default of 5s is used. udpPacketBase64 : a base64-encoded packet to send when testing using udp . This is required to generate a response over UDP to determine if the remote server is up and reachable. The given packet must be valid enough to generate a response from the server. Network Reachability \u00b6 Testing network reachability for a VM requires that the VM has minimega's command and control (C2) layer active (ie., the miniccc agent is running in the VM), has the correct IP address configured, and can ping its default route if one is configured. If C2 is not active for a VM after the c2Timeout duration has passed, then the VM will be excluded from reachability testing. Once C2 is up for a VM, the VM is queried to confirm its IP address is configured and its gateway is reachable for a maximum of 5 minutes. Once all the VMs with C2 detected have their IP address configured and their gateways are reachable, reachability tests begin. Reachability tests are run in the post-start stage and each time the running stage is triggered. Given this, if for some reason a VM comes up with its C2 agent active, but its network has to be configured manually, it will not be included in reachability tests during the post-start stage. Once the VM's network settings have been configured manually, the running stage can be triggered and the VM will be included in reachability tests this time around. Note Support for custom reachability testing using TCP or UDP currently requires the use of the activeshadow/minimega@tcp-conn-test branch until PR 1457 is merged. Injecting ICMP Rules \u00b6 The injectICMPAllow option can be used to add rules to routers/firewalls in the topology to prevent reachability tests from failing due to ACLs. When enabled, all rulesets present in the experiment (either in the topology or scenario) will have a rule prepended to the list of existing rules allowing the ICMP protocol to/from any address. To ensure this rule is applied before any other rule, the SoH app attempts to inject it with an ID of 1 (since Vyatta/VyOS orders rules by ID). If a rule already exists with an ID of 1 then injecting this rule will fail. A check is done each time an experiment is started to see if injecting ICMP rules is enabled, and if not, any injected rules are removed from the experiment. This means the setting can be changed between runs of an experiment (e.g., using phenix config edit experiment/<name> ) and the change will be reflected accurately when the experiment is started again. Packet Capture \u00b6 The SoH packet capture capability leverages minimega's tap mirroring to monitor traffic on experiment VM interfaces with PacketBeat and feed network flow data to ElasticSearch. When enabled, an Elastic/Kibana VM is added to the experiment's topology so it can be accessed via the phenix UI. PacketBeat VMs are deployed in minimega for each experiment VM interface that's configured to be monitored, but are not added to the experiment topology so they do not clutter the phenix UI. When packet capture is enabled, the phenix UI SoH tab will include a Network Volume tab that uses network flow data queried from ElasticSearch to populate a chord graph in an effort to depict how much traffic is flowing between VMs. Users/Analysts can also access Kibana using VNC via the phenix UI to do additional analysis on the network flow data that's being captured. Command and Control \u00b6 As mentioned above, SoH relies on minimega's command and control infrastructure to drive and collect the experiment health state data. Under the hood, the SoH app uses C2 to execute a test on a VM ( cc exec ), wait for the command to complete ( cc commands ), grab the STDOUT/STDERR of the command ( cc responses ), and compare it to an expected response. Current tests executed on Linux VMs include the following: ip addr ip route ping -c 1 <ip> pgrep -f <process> ss -lntu state all 'sport = <port>' cat /proc/loadavg Corresponding tests for Windows VMs include the following: ipconfig /all route print ping -n 1 <ip> powershell -command \"Get-Process <process> -ErrorAction SilentlyContinue\" powershell -command \"netstat -an | select-string -pattern 'listening' | select-string -pattern '<port>'\" powershell -command \"Get-WmiObject Win32_Processor | Measure-Object -Property LoadPercentage -Average | Select Average\"","title":"State of Health"},{"location":"state-of-health/#state-of-health","text":"State of Health (SoH) is a core (but not default) app meant to assist with understanding the state of a running experiment. It has many configuration options , and relies on minimega's command and control infrastructure (ie. the miniccc agent running in experiment VMs) to drive and collect the experiment health state data. See the command and control section for more details. SoH information is presented in the UI in three separate tabs. You can access the information by clicking the SoH button from the Experiments or Running Experiment components. Button available on the experiments table. Button available in running experiment.","title":"State of Health"},{"location":"state-of-health/#topology-graph","text":"This tab displays a network graph of the running experiment. The color of the node is based on the condition of the corresponding VM and could be: Running Not running (part of the experiment, but currently paused) Not booted (part of the experiment, but marked as Do Not Boot) Not deployed (part of the experiment, but flushed from minimega) Experiment stopped It is also possible to filter the graph based on nodes that are either: Running , Not running , Not booted , or Not deployed . The Refresh Network button will reset the filter, showing all nodes. The Manual Refresh button will request the latest server-side SoH data and update the three tabs. Hovering over a node in the graph will cause it to expand to show what operating system the node is running. An orange border around a node is indicative of the node currently having health issues (e.g., expected processes not running or not being able to reach other nodes on the network). Other virtual systems will also be represented accordingly (e.g., a printer or firewall). Clicking on the node with an orange border will produce a details modal with the current error reporting. If there are no errors to report and command and control is enabled, the details modal will only show the current CPU load. Note the green button in the lower right corner of the modal; this will provide access to the VNC for any running VM. When the VM is not running, access to the VNC will be disabled. Finally, if there is no SoH information to report on a given VM, it will be noted in the details modal. The following screenshot is an example of no SoH information with the VNC button disabled.","title":"Topology Graph"},{"location":"state-of-health/#network-volume","text":"This tab displays a chord graph that shows network flows between nodes using flows from PacketBeat fed to ElasticSearch; connections represent the volume of traffic between nodes. Tip This information is only available if the packet capture option is enabled for SoH. This is an example chord graph of a Protonuke server with two clients; one of which is requesting content at a delayed interval (therefore less traffic flow). If you hover over traffic flow, a tooltip will appear providing additional details. (In this screenshot, the mouse is hovering over the traffic for IP 192.168.100.1 .)","title":"Network Volume"},{"location":"state-of-health/#soh-messages","text":"The final tab will display any error messages from the SoH agents.","title":"SoH Messages"},{"location":"state-of-health/#configuration","text":"Just like any other app, the SoH app is configured via the Scenario configuration. SoH will only be enabled for an experiment if it's present in the scenario as soh , and has multiple configuration options available, described in detail below.","title":"Configuration"},{"location":"state-of-health/#sample-soh-scenario-config","text":"spec: apps: - name: soh metadata: appMetadataProfileKey: sohProfile # metadata key to look for in other apps c2Timeout: 5m exitOnError: false hostCustomTests: host-00: - name: FooBarTest testScript: | cat /etc/passwd | grep root testStdout: root executor: bash host-01: - name: SuckaTest.ps1 testScript: | Get-Process miniccc -ErrorAction SilentlyContinue testStdout: miniccc executor: powershell -NoProfile -ExecutionPolicy bypass -File hostListeners: client: - :502 server: - :80 - :443 hostProcesses: client: - miniccc server: - miniccc hostsToUseUUIDForC2Active: - host-02 injectICMPAllow: true packetCapture: elasticImage: /phenix/images/elasticsearch.qc2 packetBeatImage: /phenix/images/packetbeat.qc2 elasticServer: hostname: soh-elasticsearch-server vcpus: 4 memory: 4096 ipAddress: 172.16.200.1/16 vlan: MGMT captureHosts: client: - IF0 # interface to monitor on \"client\" node in topology server: - IF0 # interface to monitor on \"server\" node in topology skipInitialNetworkConfigTests: false # if true, testReachability will be off skipHosts: - kali.qc2 # can be an image name, in which case any host using image will be skipped - foobar-host # can be hostname from topology testReachability: full # can be off, sample, or full testCustomReachability: - src: host-00 dst: host-01|IF0 proto: tcp port: 22 wait: 30s - src: host-01 dst: host-00|IF0 proto: tcp port: 22 wait: 30s","title":"Sample SoH Scenario Config"},{"location":"state-of-health/#configuration-options","text":"appMetadataProfileKey : since the listeners and processes one might want to monitor could be highly dependent on other apps that are configured for an experiment, it's possible to specify the hostListeners and hostProcesses to be monitored in the other apps themselves under the key specified here. The default key is sohProfile . c2Timeout : a Golang duration string specifying how long to wait for the miniccc C2 client to become active in a VM before moving on and marking the VM as not to be monitored. The default is 5m . exitOnError : a boolean representing whether the app should cause the entire experiment deployment to fail if it has any errors. The default is false . hostCustomTests : If present, a map of custom tests to run on the given hosts. name : name of test. Used as script name to be sent to the host. testScript : the actual script (can be multiple lines) to be executed using the specified executor . executor : the application to execute the testScript with (e.g. bash , powershell ). testStdout : a string to look for in STDOUT from the executed script. If found, the test passes. If not found, it fails. testStderr : a string to look for in STDERR from the executed script. If found, the test passes. If not found, it fails. hostListeners : a map of VMs, each specifying a list of listening ports to check for within the VM. If the port can be listening on any interface, the form :80 can be used. If the port should be listening on a specific interface, the form 192.168.1.10:80 can be used. The default is nil . hostProcesses : a map of VMs, each specifying a list of process names to check for within the VM. The default is nil . hostsToUseUUIDForC2Active : a list of topology hostnames to use the minimega VM UUID for when determining if their cc agent is active. This is useful for topology nodes that are configured with snapshots disabled, preventing their hostname from getting updated when booted. Note that this configuration option also supports being set to all (e.g., hostsToUseUUIDForC2Active: all ) if a user wishes to use the minimega VM UUID for all topology nodes. injectICMPAllow : a boolean representing whether any existing firewall/router rulesets should have a rule added to allow ICMP between all nodes to facilitate reachability testing. See Injecting ICMP Rules for more details. If reachability tests are disabled, then this will be too, regardless of its setting here. The default is false . packetCapture : if present, a partially hidden packet capture infrastructure based on Elastic, Kibana, and PacketBeat will be deployed for the experiment. See Packet Capture for more details. The default is nil . elasticImage : path to the disk image to use for the Elastic/Kibana VM for packet capture. An image/elasticsearch config comes bundled with phenix and can be used to build an image to use here. There is no default for this setting; if packet capture is to be deployed it must be provided. packetBeatImage : path to the disk image to use for the PacketBeat VM for packet capture. An image/packetbeat config comes bundled with phenix and can be used to build an image to use here. There is no default for this setting; if packet capture is to be deployed it must be provided. elasticServer : hostname : the hostname to use for the Elastic/Kibana server added to the experiment topology. There is no default for this setting; if packet capture is to be deployed it must be provided. vcpus : the number of CPUs to assign to the Elastic/Kibana server VM. The default is 4. memory : the amount of memory to assign to the Elastic/Kibana server VM. The default is 4096. ipAddress : the IP address to use for the Elastic/Kibana server added to the experiment topology. The network interface this IP address is used for will be added to the experiment VLAN specified by vlan . The IP address should be specified in CIDR notation. There should also be sufficient IP addresses after the one specified here to be assigned to each of the PacketBeat monitor VMs that will be deployed, as the IP addresses assigned to them on the VLAN specified by vlan will increment up from this IP. There is no default for this setting; if packet capture is to be deployed it must be provided. vlan : the experiment VLAN to add the Elastic/Kibana and PacketBeat VMs to. There is no default for this setting; if packet capture is to be deployed it must be provided. captureHosts : a map of VMs, each specifying a list of network interface names to monitor. One PacketBeat VM will be deployed for each VM interface specified. The defalt is nil . skipInitialNetworkConfigTests : by default, a set of tests will be run on each VM to ensure the VM was assigned the correct IP address and can reach its default gateway (if specified). Setting this to true will skip these initial tests, but will also disable reachability testing. The default is false . skipHosts : a list of VM hostnames and/or disk image names to skip health monitoring for. For disk image names, any host using the disk image as its primary image will be skipped. The default is nil . testReachability : reachability testing is the process of making sure each VM can reach other VMs within the experiment over the network. See Network Reachability for more details. There are three options for this setting: off, sample, full . off : reachability testing is disabled. This is the default. sample : each VM in the experiment will attempt to ping a random VM in every other experiment VLAN. full : each VM in the experiment will attempt to ping every other VM in every other experiment VLAN. testCustomReachability : if present, a list of custom reachability test settings. src : hostname to conduct test from. dst : hostname and interface name (e.g. host-01|IF0 ) to conduct test to. proto : protocol to use for test. Currently the options are tcp and udp . If udp is used, the udpPacketBase64 setting must be provided. port : destination port to conduct test to. wait : amount of time to wait for a response from the destination. If not provided, the default of 5s is used. udpPacketBase64 : a base64-encoded packet to send when testing using udp . This is required to generate a response over UDP to determine if the remote server is up and reachable. The given packet must be valid enough to generate a response from the server.","title":"Configuration Options"},{"location":"state-of-health/#network-reachability","text":"Testing network reachability for a VM requires that the VM has minimega's command and control (C2) layer active (ie., the miniccc agent is running in the VM), has the correct IP address configured, and can ping its default route if one is configured. If C2 is not active for a VM after the c2Timeout duration has passed, then the VM will be excluded from reachability testing. Once C2 is up for a VM, the VM is queried to confirm its IP address is configured and its gateway is reachable for a maximum of 5 minutes. Once all the VMs with C2 detected have their IP address configured and their gateways are reachable, reachability tests begin. Reachability tests are run in the post-start stage and each time the running stage is triggered. Given this, if for some reason a VM comes up with its C2 agent active, but its network has to be configured manually, it will not be included in reachability tests during the post-start stage. Once the VM's network settings have been configured manually, the running stage can be triggered and the VM will be included in reachability tests this time around. Note Support for custom reachability testing using TCP or UDP currently requires the use of the activeshadow/minimega@tcp-conn-test branch until PR 1457 is merged.","title":"Network Reachability"},{"location":"state-of-health/#injecting-icmp-rules","text":"The injectICMPAllow option can be used to add rules to routers/firewalls in the topology to prevent reachability tests from failing due to ACLs. When enabled, all rulesets present in the experiment (either in the topology or scenario) will have a rule prepended to the list of existing rules allowing the ICMP protocol to/from any address. To ensure this rule is applied before any other rule, the SoH app attempts to inject it with an ID of 1 (since Vyatta/VyOS orders rules by ID). If a rule already exists with an ID of 1 then injecting this rule will fail. A check is done each time an experiment is started to see if injecting ICMP rules is enabled, and if not, any injected rules are removed from the experiment. This means the setting can be changed between runs of an experiment (e.g., using phenix config edit experiment/<name> ) and the change will be reflected accurately when the experiment is started again.","title":"Injecting ICMP Rules"},{"location":"state-of-health/#packet-capture","text":"The SoH packet capture capability leverages minimega's tap mirroring to monitor traffic on experiment VM interfaces with PacketBeat and feed network flow data to ElasticSearch. When enabled, an Elastic/Kibana VM is added to the experiment's topology so it can be accessed via the phenix UI. PacketBeat VMs are deployed in minimega for each experiment VM interface that's configured to be monitored, but are not added to the experiment topology so they do not clutter the phenix UI. When packet capture is enabled, the phenix UI SoH tab will include a Network Volume tab that uses network flow data queried from ElasticSearch to populate a chord graph in an effort to depict how much traffic is flowing between VMs. Users/Analysts can also access Kibana using VNC via the phenix UI to do additional analysis on the network flow data that's being captured.","title":"Packet Capture"},{"location":"state-of-health/#command-and-control","text":"As mentioned above, SoH relies on minimega's command and control infrastructure to drive and collect the experiment health state data. Under the hood, the SoH app uses C2 to execute a test on a VM ( cc exec ), wait for the command to complete ( cc commands ), grab the STDOUT/STDERR of the command ( cc responses ), and compare it to an expected response. Current tests executed on Linux VMs include the following: ip addr ip route ping -c 1 <ip> pgrep -f <process> ss -lntu state all 'sport = <port>' cat /proc/loadavg Corresponding tests for Windows VMs include the following: ipconfig /all route print ping -n 1 <ip> powershell -command \"Get-Process <process> -ErrorAction SilentlyContinue\" powershell -command \"netstat -an | select-string -pattern 'listening' | select-string -pattern '<port>'\" powershell -command \"Get-WmiObject Win32_Processor | Measure-Object -Property LoadPercentage -Average | Select Average\"","title":"Command and Control"},{"location":"user-administration/","text":"User Administration in phenix \u00b6 Create a new user \u00b6 There are two primary ways to create new users. Choose the Create Account link off the login page and complete all fields in the Create a New Account dialogue. This will initiate a message to an administrator's account who can then activate the account, setting the role(s) and resource name(s). From the Users tab, click the + button to create a new user. Here the administrator will add the role(s) and resource name(s) . Login \u00b6 The login page is self-descriptive. Using the Remember me checkbox will set a token to local storage so that you can remove the requirement to enter a Username and Password each time the page or site is reloaded. If an administrator starts the UI server with the following command, authentication is enabled: $> phenix ui -k <some_string> Without the -k (or --jwt-signing-key ), authentication is disabled. User Administration \u00b6 Updating Users \u00b6 An administrator is able to click on the username on the table in the Users tab to update a user. They can update First Name or Last Name , Role , Experiment Names , and Resource Name(s) . Roles \u00b6 Global Admin is the administrator level account and has access to all capabilities, to include user management. Global Admins also have access to all resources. The following table provides a high-level overview of all the available roles and their access rights. Role Limits List Get Create Update Patch Delete Global Admin Can see and control absolutely anything/everything. E V U E V U E V U E V U E V U E V U Global Viewer Can see absolutely anything/everything, but cannot make any changes. E V U E V U Experiment Admin Can see and control anything/everything for assigned experiments, including VMs, but cannot create new experiments. E V E V V E V V V Experiment User Can see assigned experiments, and can control VMs within assigned experiments, but cannot modify experiments themselves. E V E V V Experiment Viewer Can see assigned experiments and VMs within assigned experiments, but cannot modify or control experiments or VMs. E V E V VM Viewer Can only see VM screenshots and access VM VNC, nothing else. V Key: E - experiment resource, V - VM resource, U - user resource Resources \u00b6 Resource: experiments \u00b6 Verb list Desc get a list of all experiments Exp. Scoped yes (list is filtered to only include experiments in scope) Res. Scoped no Verb get Desc get a specific experiment Exp. Scoped yes Res. Scoped no Verb create Desc create a new experiment Exp. Scoped no Res. Scoped no Verb delete Desc delete a specific experiment Exp. Scoped yes Res. Scoped no Resource: experiments/start \u00b6 Verb update Desc start an experiment Exp. Scoped yes Res. Scoped no Resource: experiments/stop \u00b6 Verb update Desc stop an experiment Exp. Scoped yes Res. Scoped no Resource: experiments/schedule \u00b6 Verb get Desc get current schedule for an experiment Exp. Scoped yes Res. Scoped no Verb create Desc schedule an experiment using schedule algorithm Exp. Scoped yes Res. Scoped no Resource: experiments/trigger \u00b6 Verb create Desc trigger the running stage of an experiment Exp. Scoped yes Res. Scoped no Resource: experiments/captures \u00b6 Verb list Desc get list of packet captures for an experiment Exp. Scoped yes (list is filtered to only include experiments in scope) Res. Scoped yes (list is filtered to only include VMs in scope) Resource: experiments/files \u00b6 Verb list Desc get list of files for an experiment Exp. Scoped yes (list is filtered to only include experiments in scope) Res. Scoped no Verb get Desc get specific experiment file Exp. Scoped yes Res. Scoped no Resource: vms \u00b6 Verb list Desc get list of VMs for an experiment Exp. Scoped yes (list is filtered to only include experiments in scope) Res. Scoped yes (list is filtered to only include VMs in scope) Verb get Desc get a specific experiment VM Exp. Scoped yes Res. Scoped yes Verb patch Desc update a specific experiment VM Exp. Scoped yes Res. Scoped yes Verb delete Desc delete a specific experiment VM Exp. Scoped yes Res. Scoped yes Resource: vms/start \u00b6 Verb update Desc start a specific experiment VM Exp. Scoped yes Res. Scoped yes Resource: vms/stop \u00b6 Verb update Desc stop a specific experiment VM Exp. Scoped yes Res. Scoped yes Resource: vms/redeploy \u00b6 Verb update Desc redeploy a specific experiment VM Exp. Scoped yes Res. Scoped yes Resource: vms/screenshot \u00b6 Verb get Desc get screenshot for a specific experiment VM Exp. Scoped yes Res. Scoped yes Resource: vms/vnc \u00b6 Verb get Desc get VNC address for a specific experiment VM Exp. Scoped yes Res. Scoped yes Resource: vms/captures \u00b6 Verb list Desc get list of packet captures for a specific experiment VM Exp. Scoped yes Res. Scoped yes Verb create Desc start a packet capture on a specific experiment VM Exp. Scoped yes Res. Scoped yes Verb delete Desc stop all packet captures on a specific experiment VM Exp. Scoped yes Res. Scoped yes Resource: vms/snapshots \u00b6 Verb list Desc get list of snapshots for a specific experiment VM Exp. Scoped yes Res. Scoped yes Verb create Desc create a snapshot of a specific experiment VM Exp. Scoped yes Res. Scoped yes Verb update Desc restore a specific experiment VM to a previous snapshot Exp. Scoped yes Res. Scoped yes Resource: vms/commit \u00b6 Verb create Desc create a new backing image from a specific experiment VM Exp. Scoped yes Res. Scoped yes Resource: applications \u00b6 Verb list Desc get list of user applications Exp. Scoped no Res. Scoped yes (list is filtered to only include applications in scope) Resource: topologies \u00b6 Verb list Desc get list of available topologies Exp. Scoped no Res. Scoped yes (list is filtered to only include topologies in scope) Resource: disks \u00b6 Verb list Desc get list of available backing images Exp. Scoped no Res. Scoped yes (list is filtered to only include backing images in scope) Resource: hosts \u00b6 Verb list Desc get list of minimega cluster hosts Exp. Scoped no Res. Scoped yes (list is filtered to only include hosts in scope) Resource: users \u00b6 Verb list Desc get list of users Exp. Scoped no Res. Scoped yes (list is filtered to only include users in scope) Verb get Desc get a specific user Exp. Scoped no Res. Scoped yes Verb create Desc create a new user Exp. Scoped no Res. Scoped no Verb patch Desc update an existing user Exp. Scoped no Res. Scoped yes Verb delete Desc delete an existing user Exp. Scoped no Res. Scoped yes Built-In Roles \u00b6 See the previous section for policy resource and verb descriptions. case GLOBAL_ADMIN: return Policies([]*Policy{ { Experiments: []string{\"*\"}, Resources: []string{\"*\", \"*/*\"}, ResourceNames: []string{\"*\"}, Verbs: []string{\"*\"}, }, }) case GLOBAL_VIEWER: return Policies([]*Policy{ { Experiments: []string{\"*\"}, Resources: []string{\"*\", \"*/*\"}, ResourceNames: []string{\"*\"}, Verbs: []string{\"list\", \"get\"}, }, }) case EXP_ADMIN: // must supply experiment names and resource names or nothing will authorize return Policies([]*Policy{ { Resources: []string{\"experiments\", \"experiments/*\"}, Verbs: []string{\"list\", \"get\", \"update\"}, }, { Resources: []string{\"vms\", \"vms/*\"}, Verbs: []string{\"list\", \"get\", \"create\", \"update\", \"patch\", \"delete\"}, }, { Resources: []string{\"disks\"}, ResourceNames: []string{\"*\"}, Verbs: []string{\"list\"}, }, { Resources: []string{\"hosts\"}, ResourceNames: []string{\"*\"}, Verbs: []string{\"list\"}, }, }) case EXP_USER: // EXP_VIEWER + VM restart + VM update + VM capture // must supply experiment names and resource names or nothing will authorize return Policies([]*Policy{ { Resources: []string{\"experiments\", \"experiments/*\"}, Verbs: []string{\"list\", \"get\"}, }, { Resources: []string{\"vms\", \"vms/*\"}, Verbs: []string{\"list\", \"get\", \"patch\"}, }, { Resources: []string{\"vms/redeploy\"}, Verbs: []string{\"update\"}, }, { Resources: []string{\"vms/captures\"}, Verbs: []string{\"create\", \"delete\"}, }, { Resources: []string{\"vms/snapshots\"}, Verbs: []string{\"list\", \"create\", \"update\"}, }, { Resources: []string{\"hosts\"}, ResourceNames: []string{\"*\"}, Verbs: []string{\"list\"}, }, }) case EXP_VIEWER: // must supply experiment names and resource names or nothing will authorize return Policies([]*Policy{ { Resources: []string{\"experiments\", \"experiments/*\", \"vms\", \"vms/*\"}, Verbs: []string{\"list\", \"get\"}, }, { Resources: []string{\"hosts\"}, ResourceNames: []string{\"*\"}, Verbs: []string{\"list\"}, }, }) case VM_VIEWER: // must supply experiment names and resource names or nothing will authorize return Policies([]*Policy{ { Resources: []string{\"vms\"}, Verbs: []string{\"list\"}, }, { Resources: []string{\"vms/screenshot\", \"vms/vnc\"}, Verbs: []string{\"get\"}, }, })","title":"User Administration in phenix"},{"location":"user-administration/#user-administration-in-phenix","text":"","title":"User Administration in phenix"},{"location":"user-administration/#create-a-new-user","text":"There are two primary ways to create new users. Choose the Create Account link off the login page and complete all fields in the Create a New Account dialogue. This will initiate a message to an administrator's account who can then activate the account, setting the role(s) and resource name(s). From the Users tab, click the + button to create a new user. Here the administrator will add the role(s) and resource name(s) .","title":"Create a new user"},{"location":"user-administration/#login","text":"The login page is self-descriptive. Using the Remember me checkbox will set a token to local storage so that you can remove the requirement to enter a Username and Password each time the page or site is reloaded. If an administrator starts the UI server with the following command, authentication is enabled: $> phenix ui -k <some_string> Without the -k (or --jwt-signing-key ), authentication is disabled.","title":"Login"},{"location":"user-administration/#user-administration","text":"","title":"User Administration"},{"location":"user-administration/#updating-users","text":"An administrator is able to click on the username on the table in the Users tab to update a user. They can update First Name or Last Name , Role , Experiment Names , and Resource Name(s) .","title":"Updating Users"},{"location":"user-administration/#roles","text":"Global Admin is the administrator level account and has access to all capabilities, to include user management. Global Admins also have access to all resources. The following table provides a high-level overview of all the available roles and their access rights. Role Limits List Get Create Update Patch Delete Global Admin Can see and control absolutely anything/everything. E V U E V U E V U E V U E V U E V U Global Viewer Can see absolutely anything/everything, but cannot make any changes. E V U E V U Experiment Admin Can see and control anything/everything for assigned experiments, including VMs, but cannot create new experiments. E V E V V E V V V Experiment User Can see assigned experiments, and can control VMs within assigned experiments, but cannot modify experiments themselves. E V E V V Experiment Viewer Can see assigned experiments and VMs within assigned experiments, but cannot modify or control experiments or VMs. E V E V VM Viewer Can only see VM screenshots and access VM VNC, nothing else. V Key: E - experiment resource, V - VM resource, U - user resource","title":"Roles"},{"location":"user-administration/#resources","text":"","title":"Resources"},{"location":"user-administration/#resource-experiments","text":"Verb list Desc get a list of all experiments Exp. Scoped yes (list is filtered to only include experiments in scope) Res. Scoped no Verb get Desc get a specific experiment Exp. Scoped yes Res. Scoped no Verb create Desc create a new experiment Exp. Scoped no Res. Scoped no Verb delete Desc delete a specific experiment Exp. Scoped yes Res. Scoped no","title":"Resource: experiments"},{"location":"user-administration/#resource-experimentsstart","text":"Verb update Desc start an experiment Exp. Scoped yes Res. Scoped no","title":"Resource: experiments/start"},{"location":"user-administration/#resource-experimentsstop","text":"Verb update Desc stop an experiment Exp. Scoped yes Res. Scoped no","title":"Resource: experiments/stop"},{"location":"user-administration/#resource-experimentsschedule","text":"Verb get Desc get current schedule for an experiment Exp. Scoped yes Res. Scoped no Verb create Desc schedule an experiment using schedule algorithm Exp. Scoped yes Res. Scoped no","title":"Resource: experiments/schedule"},{"location":"user-administration/#resource-experimentstrigger","text":"Verb create Desc trigger the running stage of an experiment Exp. Scoped yes Res. Scoped no","title":"Resource: experiments/trigger"},{"location":"user-administration/#resource-experimentscaptures","text":"Verb list Desc get list of packet captures for an experiment Exp. Scoped yes (list is filtered to only include experiments in scope) Res. Scoped yes (list is filtered to only include VMs in scope)","title":"Resource: experiments/captures"},{"location":"user-administration/#resource-experimentsfiles","text":"Verb list Desc get list of files for an experiment Exp. Scoped yes (list is filtered to only include experiments in scope) Res. Scoped no Verb get Desc get specific experiment file Exp. Scoped yes Res. Scoped no","title":"Resource: experiments/files"},{"location":"user-administration/#resource-vms","text":"Verb list Desc get list of VMs for an experiment Exp. Scoped yes (list is filtered to only include experiments in scope) Res. Scoped yes (list is filtered to only include VMs in scope) Verb get Desc get a specific experiment VM Exp. Scoped yes Res. Scoped yes Verb patch Desc update a specific experiment VM Exp. Scoped yes Res. Scoped yes Verb delete Desc delete a specific experiment VM Exp. Scoped yes Res. Scoped yes","title":"Resource: vms"},{"location":"user-administration/#resource-vmsstart","text":"Verb update Desc start a specific experiment VM Exp. Scoped yes Res. Scoped yes","title":"Resource: vms/start"},{"location":"user-administration/#resource-vmsstop","text":"Verb update Desc stop a specific experiment VM Exp. Scoped yes Res. Scoped yes","title":"Resource: vms/stop"},{"location":"user-administration/#resource-vmsredeploy","text":"Verb update Desc redeploy a specific experiment VM Exp. Scoped yes Res. Scoped yes","title":"Resource: vms/redeploy"},{"location":"user-administration/#resource-vmsscreenshot","text":"Verb get Desc get screenshot for a specific experiment VM Exp. Scoped yes Res. Scoped yes","title":"Resource: vms/screenshot"},{"location":"user-administration/#resource-vmsvnc","text":"Verb get Desc get VNC address for a specific experiment VM Exp. Scoped yes Res. Scoped yes","title":"Resource: vms/vnc"},{"location":"user-administration/#resource-vmscaptures","text":"Verb list Desc get list of packet captures for a specific experiment VM Exp. Scoped yes Res. Scoped yes Verb create Desc start a packet capture on a specific experiment VM Exp. Scoped yes Res. Scoped yes Verb delete Desc stop all packet captures on a specific experiment VM Exp. Scoped yes Res. Scoped yes","title":"Resource: vms/captures"},{"location":"user-administration/#resource-vmssnapshots","text":"Verb list Desc get list of snapshots for a specific experiment VM Exp. Scoped yes Res. Scoped yes Verb create Desc create a snapshot of a specific experiment VM Exp. Scoped yes Res. Scoped yes Verb update Desc restore a specific experiment VM to a previous snapshot Exp. Scoped yes Res. Scoped yes","title":"Resource: vms/snapshots"},{"location":"user-administration/#resource-vmscommit","text":"Verb create Desc create a new backing image from a specific experiment VM Exp. Scoped yes Res. Scoped yes","title":"Resource: vms/commit"},{"location":"user-administration/#resource-applications","text":"Verb list Desc get list of user applications Exp. Scoped no Res. Scoped yes (list is filtered to only include applications in scope)","title":"Resource: applications"},{"location":"user-administration/#resource-topologies","text":"Verb list Desc get list of available topologies Exp. Scoped no Res. Scoped yes (list is filtered to only include topologies in scope)","title":"Resource: topologies"},{"location":"user-administration/#resource-disks","text":"Verb list Desc get list of available backing images Exp. Scoped no Res. Scoped yes (list is filtered to only include backing images in scope)","title":"Resource: disks"},{"location":"user-administration/#resource-hosts","text":"Verb list Desc get list of minimega cluster hosts Exp. Scoped no Res. Scoped yes (list is filtered to only include hosts in scope)","title":"Resource: hosts"},{"location":"user-administration/#resource-users","text":"Verb list Desc get list of users Exp. Scoped no Res. Scoped yes (list is filtered to only include users in scope) Verb get Desc get a specific user Exp. Scoped no Res. Scoped yes Verb create Desc create a new user Exp. Scoped no Res. Scoped no Verb patch Desc update an existing user Exp. Scoped no Res. Scoped yes Verb delete Desc delete an existing user Exp. Scoped no Res. Scoped yes","title":"Resource: users"},{"location":"user-administration/#built-in-roles","text":"See the previous section for policy resource and verb descriptions. case GLOBAL_ADMIN: return Policies([]*Policy{ { Experiments: []string{\"*\"}, Resources: []string{\"*\", \"*/*\"}, ResourceNames: []string{\"*\"}, Verbs: []string{\"*\"}, }, }) case GLOBAL_VIEWER: return Policies([]*Policy{ { Experiments: []string{\"*\"}, Resources: []string{\"*\", \"*/*\"}, ResourceNames: []string{\"*\"}, Verbs: []string{\"list\", \"get\"}, }, }) case EXP_ADMIN: // must supply experiment names and resource names or nothing will authorize return Policies([]*Policy{ { Resources: []string{\"experiments\", \"experiments/*\"}, Verbs: []string{\"list\", \"get\", \"update\"}, }, { Resources: []string{\"vms\", \"vms/*\"}, Verbs: []string{\"list\", \"get\", \"create\", \"update\", \"patch\", \"delete\"}, }, { Resources: []string{\"disks\"}, ResourceNames: []string{\"*\"}, Verbs: []string{\"list\"}, }, { Resources: []string{\"hosts\"}, ResourceNames: []string{\"*\"}, Verbs: []string{\"list\"}, }, }) case EXP_USER: // EXP_VIEWER + VM restart + VM update + VM capture // must supply experiment names and resource names or nothing will authorize return Policies([]*Policy{ { Resources: []string{\"experiments\", \"experiments/*\"}, Verbs: []string{\"list\", \"get\"}, }, { Resources: []string{\"vms\", \"vms/*\"}, Verbs: []string{\"list\", \"get\", \"patch\"}, }, { Resources: []string{\"vms/redeploy\"}, Verbs: []string{\"update\"}, }, { Resources: []string{\"vms/captures\"}, Verbs: []string{\"create\", \"delete\"}, }, { Resources: []string{\"vms/snapshots\"}, Verbs: []string{\"list\", \"create\", \"update\"}, }, { Resources: []string{\"hosts\"}, ResourceNames: []string{\"*\"}, Verbs: []string{\"list\"}, }, }) case EXP_VIEWER: // must supply experiment names and resource names or nothing will authorize return Policies([]*Policy{ { Resources: []string{\"experiments\", \"experiments/*\", \"vms\", \"vms/*\"}, Verbs: []string{\"list\", \"get\"}, }, { Resources: []string{\"hosts\"}, ResourceNames: []string{\"*\"}, Verbs: []string{\"list\"}, }, }) case VM_VIEWER: // must supply experiment names and resource names or nothing will authorize return Policies([]*Policy{ { Resources: []string{\"vms\"}, Verbs: []string{\"list\"}, }, { Resources: []string{\"vms/screenshot\", \"vms/vnc\"}, Verbs: []string{\"get\"}, }, })","title":"Built-In Roles"},{"location":"vm-multi-action/","text":"VM Multi Action \u00b6 Selecting VMs \u00b6 From the Web-UI \u00b6 The experiment must be started; click on the experiment name to enter the Running Experiment component. Within that component, click on the checkbox adjacent to the VM Name column to select all the VMs. Alternatively, the checkbox adjacent to a specific VM name can be used to select the VM. Once one or more VMs are selected, a toolbar will appear to the left of the search text box. The buttons on the toolbar are essentially the same as described in VMs . From the Command Line Binary \u00b6 Not applicable. Searching for VMs \u00b6 The search text box can be used to filter the list of VMs to only apply actions to the filtered list. From the Web-UI \u00b6 The experiment must be started; click on the experiment name to enter the Running Experiment component. Within that component, use the search textbox to find VMs by: state - The keywords running,shutdown,paused,capturing can be used to find VMs in a specific state. Also, the not keyword can be used to negate search term(s) (i.e. not running ) ipv4 address - VMs in a specific subnet can be found by entering the subnet (i.e 192.168.2.0/30 ) other fields (i.e. name, taps, tags) - All other fields will be searched for a keyword contained within the field combine search terms - Search terms can be combined by using or and keywords. Parenthesis can also be used to group search terms. escape keywords - To find keywords that appear in a VM name use double quotes. For example, to find a VM named free_running , type \"running\" . Example \u00b6 Multiple search terms From the Command Line Binary \u00b6 Not applicable. Starting/stoppping Packet Captures \u00b6 From the Web-UI \u00b6 When a valid ipv4 subnet is entered, the play button adjacent to the IPv4 label will be enabled. To start capturing, press the play button. Once there are valid captures, the stop button adjacent to the play button will be enabled. To stop all the catpures, press the stop button. To stop all packet captures for all subnets, the term capturing can be entered in the search bar to find all the VMs with active packet captures. From the Command Line Binary \u00b6 To start packet captures on running VMs for a specific subnet, use the following command. $> phenix vm capture start-subnet <experiment name> <subnet> To stop all packet captures for a specific subnet, use the following command. $> phenix vm capture stop-subnet <experiment name> <subnet> To stop all packet captures for an experiment, use the following command. $> phenix vm capture stop-all <experiment name> Stopped Experiment Component \u00b6 Similar to the Running Experiment component, multiple VMs can be selected for the Stopped Experiment component. In addition, VMs in the Stopped Experiment component can be searched by: state - The keyword dnb can be used to find all VMs with the do not boot flag set to true . Also, the not keyword can be used to negate search term(s) (i.e. not dnb ) ipv4 address - VMs in a specific subnet can be found by entering the subnet (i.e 192.168.2.0/30 ) other fields (i.e. VM, host, disk) combine search terms - Search terms can be combined by using or and keywords. Parenthesis can also be used to group search terms. escape keywords - To find keywords that appear in a VM name use double quotes. For example, to find a VM named dnb_me , type \"dnb\" .","title":"VM Multi Action"},{"location":"vm-multi-action/#vm-multi-action","text":"","title":"VM Multi Action"},{"location":"vm-multi-action/#selecting-vms","text":"","title":"Selecting VMs"},{"location":"vm-multi-action/#from-the-web-ui","text":"The experiment must be started; click on the experiment name to enter the Running Experiment component. Within that component, click on the checkbox adjacent to the VM Name column to select all the VMs. Alternatively, the checkbox adjacent to a specific VM name can be used to select the VM. Once one or more VMs are selected, a toolbar will appear to the left of the search text box. The buttons on the toolbar are essentially the same as described in VMs .","title":"From the Web-UI"},{"location":"vm-multi-action/#from-the-command-line-binary","text":"Not applicable.","title":"From the Command Line Binary"},{"location":"vm-multi-action/#searching-for-vms","text":"The search text box can be used to filter the list of VMs to only apply actions to the filtered list.","title":"Searching for VMs"},{"location":"vm-multi-action/#from-the-web-ui_1","text":"The experiment must be started; click on the experiment name to enter the Running Experiment component. Within that component, use the search textbox to find VMs by: state - The keywords running,shutdown,paused,capturing can be used to find VMs in a specific state. Also, the not keyword can be used to negate search term(s) (i.e. not running ) ipv4 address - VMs in a specific subnet can be found by entering the subnet (i.e 192.168.2.0/30 ) other fields (i.e. name, taps, tags) - All other fields will be searched for a keyword contained within the field combine search terms - Search terms can be combined by using or and keywords. Parenthesis can also be used to group search terms. escape keywords - To find keywords that appear in a VM name use double quotes. For example, to find a VM named free_running , type \"running\" .","title":"From the Web-UI"},{"location":"vm-multi-action/#example","text":"Multiple search terms","title":"Example"},{"location":"vm-multi-action/#from-the-command-line-binary_1","text":"Not applicable.","title":"From the Command Line Binary"},{"location":"vm-multi-action/#startingstoppping-packet-captures","text":"","title":"Starting/stoppping Packet Captures"},{"location":"vm-multi-action/#from-the-web-ui_2","text":"When a valid ipv4 subnet is entered, the play button adjacent to the IPv4 label will be enabled. To start capturing, press the play button. Once there are valid captures, the stop button adjacent to the play button will be enabled. To stop all the catpures, press the stop button. To stop all packet captures for all subnets, the term capturing can be entered in the search bar to find all the VMs with active packet captures.","title":"From the Web-UI"},{"location":"vm-multi-action/#from-the-command-line-binary_2","text":"To start packet captures on running VMs for a specific subnet, use the following command. $> phenix vm capture start-subnet <experiment name> <subnet> To stop all packet captures for a specific subnet, use the following command. $> phenix vm capture stop-subnet <experiment name> <subnet> To stop all packet captures for an experiment, use the following command. $> phenix vm capture stop-all <experiment name>","title":"From the Command Line Binary"},{"location":"vm-multi-action/#stopped-experiment-component","text":"Similar to the Running Experiment component, multiple VMs can be selected for the Stopped Experiment component. In addition, VMs in the Stopped Experiment component can be searched by: state - The keyword dnb can be used to find all VMs with the do not boot flag set to true . Also, the not keyword can be used to negate search term(s) (i.e. not dnb ) ipv4 address - VMs in a specific subnet can be found by entering the subnet (i.e 192.168.2.0/30 ) other fields (i.e. VM, host, disk) combine search terms - Search terms can be combined by using or and keywords. Parenthesis can also be used to group search terms. escape keywords - To find keywords that appear in a VM name use double quotes. For example, to find a VM named dnb_me , type \"dnb\" .","title":"Stopped Experiment Component"},{"location":"vms/","text":"VMs \u00b6 VM Info \u00b6 From the Web-UI \u00b6 The experiment must be started; click on the experiment name to enter the Running Experiment component. Within that component, click on the VM name and you will be presented with a VM information modal. Available commands restore a snapshot by clicking the play button next to the desired snapshot name. Buttons from left to right: pause a running VM Create a memory snapshot of a running VM Create a backing image of a running VM Create a snapshot of a running VM record screenshot of a running VM modify state opens another toolbar of buttons Modify State Toolbar \u00b6 Buttons from left to right: redeploy a running VM reset disk state of a running VM restart a running VM shutdown a running VM kill a running VM close modify state toolbar From the Command Line Binary \u00b6 There are two options for displaying the information for VMs in an experiment. First run the following command to see information for all VMs in a given experiment. $> phenix vm info <experiment name> Or, run the following to see the information for a specific VM in an experiment. $> phenix vm info <experiment name> <vm name> Create a Backing Image \u00b6 From the Web-UI \u00b6 Click on the name of a running VM in a started experiment to access the VM information modal. Click the create backing image button as shown in the screenshot below. From the Command Line Binary \u00b6 Not applicable. Create a Memory Snapshot \u00b6 From the Web-UI \u00b6 Click on the name of a running VM in a started experiment to access the VM information modal. Click the memory snapshot button as shown in the screenshot below. From the Command Line Binary \u00b6 To create an ELF memory dump, run the following command. $> phenix vm memory-snapshot <experiment name> <vm name> <snapshot file path> Create a VM Snapshot \u00b6 From the Web-UI \u00b6 Click on a running VM in a started experiment to access the VM information modal. Click the vm snapshot button as shown in the screenshot below. From the Command Line Binary \u00b6 Not applicable. VM VNC Access \u00b6 From the Web-UI \u00b6 The experiment must be started; click on the VM screenshot to open a new browser tab that provides VNC access to the VM. From the Command Line Binary \u00b6 Not applicable. Packet Capture \u00b6 From the Web-UI \u00b6 Click on the name of the network tap on a running VM in a started experiment to start a packet capture. The name of the network tap will turn green once a packet capture has started. It is possible to start captures on multiple network taps. However, when you stop packet capture, it will stop captures on all network taps. From the Command Line Binary \u00b6 To start a packet capture, run the following command. $> phenix vm capture start <experiment name> <vm name> <iface index> </path/to/out file> To stop all packet captures on a running VM, use the following command. $> phenix vm capture stop <experiment name> <vm name> Kill a VM \u00b6 From the Web-UI \u00b6 Click on the name of a running VM in a started experiment to access the VM information modal. Click the modify state button on the far right to open the modify state toolbar. Click the kill button as shown in the screenshot below. Note : if you stop and then start the experiment again, that VM will run again per the experiment configuration. From the Command Line Binary \u00b6 To kill a VM, run the following command. $> phenix vm kill <experiment name> <vm name> Modify the Network Connectivity \u00b6 From the Web-UI \u00b6 Click on the network for the desired VM in the Running Component to modify the settings. Select from a pull down what network you want to switch the VM interface you clicked on to. To revert back to previous setting, simply repeat selecting the network interface you wish to change, and select the previous network setting. From the Command Line Binary \u00b6 To connect a VM network interface to a different network, run the following command. $> phenix vm net connect <experiment name> <vm name> <iface index> <vlan id> To disconnect a VM network interface, run the following command. $> phenix vm net disconnect <experiment name> <vm name> <iface index> Pause a VM \u00b6 From the Web-UI \u00b6 Click on the name of a running VM in a started experiment to access the VM information modal. To pause a VM, click on the pause button as shown in the screenshot below. To start a paused VM, that same button will become a green play button; simply click it to start. From the Command Line Binary \u00b6 To pause a VM, run the following command. $> phenix vm pause <experiment name> <vm name> To resume a paused VM, run the following command. $> phenix vm resume <experiment name> <vm name> Redeploy a VM \u00b6 From the Web-UI \u00b6 Click on the name of a running VM in a started experiment to access the VM information modal. Click the modify state button on the far right to open the modify state toolbar. Click the redeploy button as shown in the screenshot below. From the Command Line Binary \u00b6 To redploy a VM, run the following command. $> phenix vm redeploy <experiment name> <vm name> Reset Disk State \u00b6 From the Web-UI \u00b6 Click on the name of a running VM in a started experiment to access the VM information modal. Click the modify state button on the far right to open the modify state toolbar. Click the reset disk state button as shown in the screenshot below. From the Command Line Binary \u00b6 To reset the first disk to the initial pre-boot state, run the following command. $> phenix vm reset-disk <experiment name> <vm name> Restart a VM \u00b6 From the Web-UI \u00b6 Click on the name of a running VM in a started experiment to access the VM information modal. Click the modify state button on the far right to open the modify state toolbar. Click the restart button as shown in the screenshot below. From the Command Line Binary \u00b6 To restart a VM, run the following command. $> phenix vm restart <experiment name> <vm name> Resume a VM \u00b6 From the Web-UI \u00b6 Click on the name of the paused VM in a started experiment to access the VM information modal. Click the green play button (previously the pause button, furthest button to the left). From the Command Line Binary \u00b6 To resume a paused VM, run the following command. $> phenix vm resume <experiment name> <vm name> Shutdown a VM \u00b6 From the Web-UI \u00b6 Click on the name of a running VM in a started experiment to access the VM information modal. Click the modify state button on the far right to open the modify state toolbar. Click the shutdown button as shown in the screenshot below. From the Command Line Binary \u00b6 To shutdown a VM, run the following command. $> phenix vm shutdown <experiment name> <vm name> Modify VM Settings \u00b6 From the Web-UI \u00b6 There are two ways to modify VM settings: Click on a stopped experiment to access the Stopped Component. You are able to edit the following: Host name CPUs Memory Disk Do not boot flag From a running experiment, click on the VM name and then the redeploy button (yellow power button, second from the right on the modal footer). You are able to edit the following: CPU Memory Disk Replication of original injections From the Command Line Binary \u00b6 This command is not yet implemented. For now, you can edit the stopped experiment directly with the following command. $> phenix cfg edit topology/<topology name> This will launch the system editor where you can directly modify the experiment settings. Applying Actions to Multiple VMs \u00b6 Note See VM Multi Action for documentation on applying actions to multiple VMs at once.","title":"VMs"},{"location":"vms/#vms","text":"","title":"VMs"},{"location":"vms/#vm-info","text":"","title":"VM Info"},{"location":"vms/#from-the-web-ui","text":"The experiment must be started; click on the experiment name to enter the Running Experiment component. Within that component, click on the VM name and you will be presented with a VM information modal. Available commands restore a snapshot by clicking the play button next to the desired snapshot name. Buttons from left to right: pause a running VM Create a memory snapshot of a running VM Create a backing image of a running VM Create a snapshot of a running VM record screenshot of a running VM modify state opens another toolbar of buttons","title":"From the Web-UI"},{"location":"vms/#modify-state-toolbar","text":"Buttons from left to right: redeploy a running VM reset disk state of a running VM restart a running VM shutdown a running VM kill a running VM close modify state toolbar","title":"Modify State Toolbar"},{"location":"vms/#from-the-command-line-binary","text":"There are two options for displaying the information for VMs in an experiment. First run the following command to see information for all VMs in a given experiment. $> phenix vm info <experiment name> Or, run the following to see the information for a specific VM in an experiment. $> phenix vm info <experiment name> <vm name>","title":"From the Command Line Binary"},{"location":"vms/#create-a-backing-image","text":"","title":"Create a Backing Image"},{"location":"vms/#from-the-web-ui_1","text":"Click on the name of a running VM in a started experiment to access the VM information modal. Click the create backing image button as shown in the screenshot below.","title":"From the Web-UI"},{"location":"vms/#from-the-command-line-binary_1","text":"Not applicable.","title":"From the Command Line Binary"},{"location":"vms/#create-a-memory-snapshot","text":"","title":"Create a Memory Snapshot"},{"location":"vms/#from-the-web-ui_2","text":"Click on the name of a running VM in a started experiment to access the VM information modal. Click the memory snapshot button as shown in the screenshot below.","title":"From the Web-UI"},{"location":"vms/#from-the-command-line-binary_2","text":"To create an ELF memory dump, run the following command. $> phenix vm memory-snapshot <experiment name> <vm name> <snapshot file path>","title":"From the Command Line Binary"},{"location":"vms/#create-a-vm-snapshot","text":"","title":"Create a VM Snapshot"},{"location":"vms/#from-the-web-ui_3","text":"Click on a running VM in a started experiment to access the VM information modal. Click the vm snapshot button as shown in the screenshot below.","title":"From the Web-UI"},{"location":"vms/#from-the-command-line-binary_3","text":"Not applicable.","title":"From the Command Line Binary"},{"location":"vms/#vm-vnc-access","text":"","title":"VM VNC Access"},{"location":"vms/#from-the-web-ui_4","text":"The experiment must be started; click on the VM screenshot to open a new browser tab that provides VNC access to the VM.","title":"From the Web-UI"},{"location":"vms/#from-the-command-line-binary_4","text":"Not applicable.","title":"From the Command Line Binary"},{"location":"vms/#packet-capture","text":"","title":"Packet Capture"},{"location":"vms/#from-the-web-ui_5","text":"Click on the name of the network tap on a running VM in a started experiment to start a packet capture. The name of the network tap will turn green once a packet capture has started. It is possible to start captures on multiple network taps. However, when you stop packet capture, it will stop captures on all network taps.","title":"From the Web-UI"},{"location":"vms/#from-the-command-line-binary_5","text":"To start a packet capture, run the following command. $> phenix vm capture start <experiment name> <vm name> <iface index> </path/to/out file> To stop all packet captures on a running VM, use the following command. $> phenix vm capture stop <experiment name> <vm name>","title":"From the Command Line Binary"},{"location":"vms/#kill-a-vm","text":"","title":"Kill a VM"},{"location":"vms/#from-the-web-ui_6","text":"Click on the name of a running VM in a started experiment to access the VM information modal. Click the modify state button on the far right to open the modify state toolbar. Click the kill button as shown in the screenshot below. Note : if you stop and then start the experiment again, that VM will run again per the experiment configuration.","title":"From the Web-UI"},{"location":"vms/#from-the-command-line-binary_6","text":"To kill a VM, run the following command. $> phenix vm kill <experiment name> <vm name>","title":"From the Command Line Binary"},{"location":"vms/#modify-the-network-connectivity","text":"","title":"Modify the Network Connectivity"},{"location":"vms/#from-the-web-ui_7","text":"Click on the network for the desired VM in the Running Component to modify the settings. Select from a pull down what network you want to switch the VM interface you clicked on to. To revert back to previous setting, simply repeat selecting the network interface you wish to change, and select the previous network setting.","title":"From the Web-UI"},{"location":"vms/#from-the-command-line-binary_7","text":"To connect a VM network interface to a different network, run the following command. $> phenix vm net connect <experiment name> <vm name> <iface index> <vlan id> To disconnect a VM network interface, run the following command. $> phenix vm net disconnect <experiment name> <vm name> <iface index>","title":"From the Command Line Binary"},{"location":"vms/#pause-a-vm","text":"","title":"Pause a VM"},{"location":"vms/#from-the-web-ui_8","text":"Click on the name of a running VM in a started experiment to access the VM information modal. To pause a VM, click on the pause button as shown in the screenshot below. To start a paused VM, that same button will become a green play button; simply click it to start.","title":"From the Web-UI"},{"location":"vms/#from-the-command-line-binary_8","text":"To pause a VM, run the following command. $> phenix vm pause <experiment name> <vm name> To resume a paused VM, run the following command. $> phenix vm resume <experiment name> <vm name>","title":"From the Command Line Binary"},{"location":"vms/#redeploy-a-vm","text":"","title":"Redeploy a VM"},{"location":"vms/#from-the-web-ui_9","text":"Click on the name of a running VM in a started experiment to access the VM information modal. Click the modify state button on the far right to open the modify state toolbar. Click the redeploy button as shown in the screenshot below.","title":"From the Web-UI"},{"location":"vms/#from-the-command-line-binary_9","text":"To redploy a VM, run the following command. $> phenix vm redeploy <experiment name> <vm name>","title":"From the Command Line Binary"},{"location":"vms/#reset-disk-state","text":"","title":"Reset Disk State"},{"location":"vms/#from-the-web-ui_10","text":"Click on the name of a running VM in a started experiment to access the VM information modal. Click the modify state button on the far right to open the modify state toolbar. Click the reset disk state button as shown in the screenshot below.","title":"From the Web-UI"},{"location":"vms/#from-the-command-line-binary_10","text":"To reset the first disk to the initial pre-boot state, run the following command. $> phenix vm reset-disk <experiment name> <vm name>","title":"From the Command Line Binary"},{"location":"vms/#restart-a-vm","text":"","title":"Restart a VM"},{"location":"vms/#from-the-web-ui_11","text":"Click on the name of a running VM in a started experiment to access the VM information modal. Click the modify state button on the far right to open the modify state toolbar. Click the restart button as shown in the screenshot below.","title":"From the Web-UI"},{"location":"vms/#from-the-command-line-binary_11","text":"To restart a VM, run the following command. $> phenix vm restart <experiment name> <vm name>","title":"From the Command Line Binary"},{"location":"vms/#resume-a-vm","text":"","title":"Resume a VM"},{"location":"vms/#from-the-web-ui_12","text":"Click on the name of the paused VM in a started experiment to access the VM information modal. Click the green play button (previously the pause button, furthest button to the left).","title":"From the Web-UI"},{"location":"vms/#from-the-command-line-binary_12","text":"To resume a paused VM, run the following command. $> phenix vm resume <experiment name> <vm name>","title":"From the Command Line Binary"},{"location":"vms/#shutdown-a-vm","text":"","title":"Shutdown a VM"},{"location":"vms/#from-the-web-ui_13","text":"Click on the name of a running VM in a started experiment to access the VM information modal. Click the modify state button on the far right to open the modify state toolbar. Click the shutdown button as shown in the screenshot below.","title":"From the Web-UI"},{"location":"vms/#from-the-command-line-binary_13","text":"To shutdown a VM, run the following command. $> phenix vm shutdown <experiment name> <vm name>","title":"From the Command Line Binary"},{"location":"vms/#modify-vm-settings","text":"","title":"Modify VM Settings"},{"location":"vms/#from-the-web-ui_14","text":"There are two ways to modify VM settings: Click on a stopped experiment to access the Stopped Component. You are able to edit the following: Host name CPUs Memory Disk Do not boot flag From a running experiment, click on the VM name and then the redeploy button (yellow power button, second from the right on the modal footer). You are able to edit the following: CPU Memory Disk Replication of original injections","title":"From the Web-UI"},{"location":"vms/#from-the-command-line-binary_14","text":"This command is not yet implemented. For now, you can edit the stopped experiment directly with the following command. $> phenix cfg edit topology/<topology name> This will launch the system editor where you can directly modify the experiment settings.","title":"From the Command Line Binary"},{"location":"vms/#applying-actions-to-multiple-vms","text":"Note See VM Multi Action for documentation on applying actions to multiple VMs at once.","title":"Applying Actions to Multiple VMs"}]}